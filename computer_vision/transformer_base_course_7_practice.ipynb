{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAtt(nn.Module):\n",
    "    \"\"\"Basic attention block\n",
    "    \n",
    "    This is a simplified version referenced from: \n",
    "    https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py#L178-L202\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_head = d_model // head\n",
    "        self.head = head\n",
    "        \n",
    "        # We don't want to create *head* instances of Linear class\n",
    "        # so we just group it to single Linear that takes in *d_model* channels and returns *d_head* x *head* channels\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        \n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        nn.init.zeros_(self.W_o.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        queries = self.W_q(x) # B x N x head*d_head\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        queries = queries.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3) # B x head x N x d_head\n",
    "        keys = keys.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        values = values.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (queries @ keys.transpose(-2, -1)) / self.d_head ** 0.5\n",
    "        attn = F.softmax(attn, dim=-1) # B x head x N x N\n",
    "        \n",
    "        x = attn @ values # B x head x N x h_head\n",
    "        x = x.transpose(1, 2) # B x N x head x h_head\n",
    "        x = x.reshape(B, N, C) # B x N x head*h_head - Remind: d_model = C = head*h_head\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "module = MultiHeadAtt(256)\n",
    "input_tensor = torch.ones((1, 16, 256))\n",
    "\n",
    "output_tensor = module(input_tensor)\n",
    "assert output_tensor.shape == (1, 16, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/vit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"MLP or Feed forward network used in attention blocks\"\"\"\n",
    "    def __init__(self, d_in_out, d_hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in_out, d_hidden)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        \n",
    "        self.fc2 = nn.Linear(d_hidden, d_in_out)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Basic building block of ViT\"\"\"\n",
    "    def __init__(self, d_model, head, d_ff_hid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        self.multi_attn = MultiHeadAtt(d_model, head)\n",
    "        \n",
    "        self.ff_norm = nn.LayerNorm(d_model)\n",
    "        self.ff = FFN(d_model, d_ff_hid)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        x_res = x\n",
    "        x = self.input_norm(x)\n",
    "        z = self.multi_attn(x) + x_res\n",
    "        \n",
    "        z_res = z\n",
    "        z = self.ff_norm(z)\n",
    "        z = self.ff(z) + z_res\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"A layer that splits image into patches and using a CNN to compute embedding feature of each patch\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.patch_size = patch_size if isinstance(patch_size, tuple) else (patch_size, patch_size)\n",
    "        \n",
    "        self.grid_size = (self.img_size[0] // self.patch_size[0], self.img_size[1] // self.patch_size[1])\n",
    "        self.num_patches = \"practice\"\n",
    "        \n",
    "        self.conv = nn.Conv2d(img_c, d_model, kernel_size=\"practice\", stride=\"practice\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "        x = self.conv(x) # B x d_model x grid_H x grid_W\n",
    "        x = torch.flatten(x, 2) # B x d_model x N\n",
    "        x = x.transpose(1, 2) # B x N x d_model\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "module = PatchEmbed(img_size=224, patch_size=\"practice\", img_c=3, d_model=256)\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = module(input_tensor)\n",
    "assert output_tensor.shape == (1, 14 * 14, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"A skeleton of a typical ViT model\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model, num_class, encoders):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.patch_embeder = PatchEmbed(img_size, patch_size, img_c, d_model)\n",
    "        \n",
    "        num_patches = self.patch_embeder.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(\"practice\")\n",
    "        \n",
    "        self.encoders = encoders\n",
    "        self.mlp_head = nn.Linear(d_model, num_class)\n",
    "        self.cls_morm = nn.LayerNorm(num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x_embed = self.patch_embeder(x)\n",
    "        x_embed = \"practice\"\n",
    "        x_embed = x_embed + self.pos_embed\n",
    "        \n",
    "        x_transformed = self.encoders(x_embed)\n",
    "        cls_transformed_token = x_transformed[:, 0, :] \n",
    "        cls_logits = self.mlp_head(cls_transformed_token)\n",
    "        cls_logits = self.cls_morm(cls_logits)\n",
    "        return cls_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─PatchEmbed: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       590,592\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─TransformerEncoder: 2-2           --\n",
      "|    |    └─LayerNorm: 3-1               1,536\n",
      "|    |    └─MultiHeadAtt: 3-2            2,360,064\n",
      "|    |    └─LayerNorm: 3-3               1,536\n",
      "|    |    └─FFN: 3-4                     4,722,432\n",
      "|    └─TransformerEncoder: 2-3           --\n",
      "|    |    └─LayerNorm: 3-5               1,536\n",
      "|    |    └─MultiHeadAtt: 3-6            2,360,064\n",
      "|    |    └─LayerNorm: 3-7               1,536\n",
      "|    |    └─FFN: 3-8                     4,722,432\n",
      "|    └─TransformerEncoder: 2-4           --\n",
      "|    |    └─LayerNorm: 3-9               1,536\n",
      "|    |    └─MultiHeadAtt: 3-10           2,360,064\n",
      "|    |    └─LayerNorm: 3-11              1,536\n",
      "|    |    └─FFN: 3-12                    4,722,432\n",
      "|    └─TransformerEncoder: 2-5           --\n",
      "|    |    └─LayerNorm: 3-13              1,536\n",
      "|    |    └─MultiHeadAtt: 3-14           2,360,064\n",
      "|    |    └─LayerNorm: 3-15              1,536\n",
      "|    |    └─FFN: 3-16                    4,722,432\n",
      "|    └─TransformerEncoder: 2-6           --\n",
      "|    |    └─LayerNorm: 3-17              1,536\n",
      "|    |    └─MultiHeadAtt: 3-18           2,360,064\n",
      "|    |    └─LayerNorm: 3-19              1,536\n",
      "|    |    └─FFN: 3-20                    4,722,432\n",
      "|    └─TransformerEncoder: 2-7           --\n",
      "|    |    └─LayerNorm: 3-21              1,536\n",
      "|    |    └─MultiHeadAtt: 3-22           2,360,064\n",
      "|    |    └─LayerNorm: 3-23              1,536\n",
      "|    |    └─FFN: 3-24                    4,722,432\n",
      "|    └─TransformerEncoder: 2-8           --\n",
      "|    |    └─LayerNorm: 3-25              1,536\n",
      "|    |    └─MultiHeadAtt: 3-26           2,360,064\n",
      "|    |    └─LayerNorm: 3-27              1,536\n",
      "|    |    └─FFN: 3-28                    4,722,432\n",
      "|    └─TransformerEncoder: 2-9           --\n",
      "|    |    └─LayerNorm: 3-29              1,536\n",
      "|    |    └─MultiHeadAtt: 3-30           2,360,064\n",
      "|    |    └─LayerNorm: 3-31              1,536\n",
      "|    |    └─FFN: 3-32                    4,722,432\n",
      "|    └─TransformerEncoder: 2-10          --\n",
      "|    |    └─LayerNorm: 3-33              1,536\n",
      "|    |    └─MultiHeadAtt: 3-34           2,360,064\n",
      "|    |    └─LayerNorm: 3-35              1,536\n",
      "|    |    └─FFN: 3-36                    4,722,432\n",
      "|    └─TransformerEncoder: 2-11          --\n",
      "|    |    └─LayerNorm: 3-37              1,536\n",
      "|    |    └─MultiHeadAtt: 3-38           2,360,064\n",
      "|    |    └─LayerNorm: 3-39              1,536\n",
      "|    |    └─FFN: 3-40                    4,722,432\n",
      "|    └─TransformerEncoder: 2-12          --\n",
      "|    |    └─LayerNorm: 3-41              1,536\n",
      "|    |    └─MultiHeadAtt: 3-42           2,360,064\n",
      "|    |    └─LayerNorm: 3-43              1,536\n",
      "|    |    └─FFN: 3-44                    4,722,432\n",
      "|    └─TransformerEncoder: 2-13          --\n",
      "|    |    └─LayerNorm: 3-45              1,536\n",
      "|    |    └─MultiHeadAtt: 3-46           2,360,064\n",
      "|    |    └─LayerNorm: 3-47              1,536\n",
      "|    |    └─FFN: 3-48                    4,722,432\n",
      "├─Linear: 1-3                            7,690\n",
      "├─LayerNorm: 1-4                         20\n",
      "=================================================================\n",
      "Total params: 85,625,118\n",
      "Trainable params: 85,625,118\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─PatchEmbed: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       590,592\n",
       "├─Sequential: 1-2                        --\n",
       "|    └─TransformerEncoder: 2-2           --\n",
       "|    |    └─LayerNorm: 3-1               1,536\n",
       "|    |    └─MultiHeadAtt: 3-2            2,360,064\n",
       "|    |    └─LayerNorm: 3-3               1,536\n",
       "|    |    └─FFN: 3-4                     4,722,432\n",
       "|    └─TransformerEncoder: 2-3           --\n",
       "|    |    └─LayerNorm: 3-5               1,536\n",
       "|    |    └─MultiHeadAtt: 3-6            2,360,064\n",
       "|    |    └─LayerNorm: 3-7               1,536\n",
       "|    |    └─FFN: 3-8                     4,722,432\n",
       "|    └─TransformerEncoder: 2-4           --\n",
       "|    |    └─LayerNorm: 3-9               1,536\n",
       "|    |    └─MultiHeadAtt: 3-10           2,360,064\n",
       "|    |    └─LayerNorm: 3-11              1,536\n",
       "|    |    └─FFN: 3-12                    4,722,432\n",
       "|    └─TransformerEncoder: 2-5           --\n",
       "|    |    └─LayerNorm: 3-13              1,536\n",
       "|    |    └─MultiHeadAtt: 3-14           2,360,064\n",
       "|    |    └─LayerNorm: 3-15              1,536\n",
       "|    |    └─FFN: 3-16                    4,722,432\n",
       "|    └─TransformerEncoder: 2-6           --\n",
       "|    |    └─LayerNorm: 3-17              1,536\n",
       "|    |    └─MultiHeadAtt: 3-18           2,360,064\n",
       "|    |    └─LayerNorm: 3-19              1,536\n",
       "|    |    └─FFN: 3-20                    4,722,432\n",
       "|    └─TransformerEncoder: 2-7           --\n",
       "|    |    └─LayerNorm: 3-21              1,536\n",
       "|    |    └─MultiHeadAtt: 3-22           2,360,064\n",
       "|    |    └─LayerNorm: 3-23              1,536\n",
       "|    |    └─FFN: 3-24                    4,722,432\n",
       "|    └─TransformerEncoder: 2-8           --\n",
       "|    |    └─LayerNorm: 3-25              1,536\n",
       "|    |    └─MultiHeadAtt: 3-26           2,360,064\n",
       "|    |    └─LayerNorm: 3-27              1,536\n",
       "|    |    └─FFN: 3-28                    4,722,432\n",
       "|    └─TransformerEncoder: 2-9           --\n",
       "|    |    └─LayerNorm: 3-29              1,536\n",
       "|    |    └─MultiHeadAtt: 3-30           2,360,064\n",
       "|    |    └─LayerNorm: 3-31              1,536\n",
       "|    |    └─FFN: 3-32                    4,722,432\n",
       "|    └─TransformerEncoder: 2-10          --\n",
       "|    |    └─LayerNorm: 3-33              1,536\n",
       "|    |    └─MultiHeadAtt: 3-34           2,360,064\n",
       "|    |    └─LayerNorm: 3-35              1,536\n",
       "|    |    └─FFN: 3-36                    4,722,432\n",
       "|    └─TransformerEncoder: 2-11          --\n",
       "|    |    └─LayerNorm: 3-37              1,536\n",
       "|    |    └─MultiHeadAtt: 3-38           2,360,064\n",
       "|    |    └─LayerNorm: 3-39              1,536\n",
       "|    |    └─FFN: 3-40                    4,722,432\n",
       "|    └─TransformerEncoder: 2-12          --\n",
       "|    |    └─LayerNorm: 3-41              1,536\n",
       "|    |    └─MultiHeadAtt: 3-42           2,360,064\n",
       "|    |    └─LayerNorm: 3-43              1,536\n",
       "|    |    └─FFN: 3-44                    4,722,432\n",
       "|    └─TransformerEncoder: 2-13          --\n",
       "|    |    └─LayerNorm: 3-45              1,536\n",
       "|    |    └─MultiHeadAtt: 3-46           2,360,064\n",
       "|    |    └─LayerNorm: 3-47              1,536\n",
       "|    |    └─FFN: 3-48                    4,722,432\n",
       "├─Linear: 1-3                            7,690\n",
       "├─LayerNorm: 1-4                         20\n",
       "=================================================================\n",
       "Total params: 85,625,118\n",
       "Trainable params: 85,625,118\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ViT variants -- ViT-B/16\n",
    "n_class = 10\n",
    "n_layer = 12\n",
    "d_model = 768\n",
    "n_head = 12\n",
    "mlp_hidden = 3072\n",
    "\n",
    "vit_encoder = nn.Sequential(*[TransformerEncoder(d_model=d_model, head=n_head, d_ff_hid=mlp_hidden) for i in range(n_layer)])\n",
    "vit_b = ViT(img_size=224, patch_size=16, img_c=3, d_model=d_model, num_class=n_class, encoders=vit_encoder)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = vit_b(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(vit_b, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─PatchEmbed: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       787,456\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─TransformerEncoder: 2-2           --\n",
      "|    |    └─LayerNorm: 3-1               2,048\n",
      "|    |    └─MultiHeadAtt: 3-2            4,195,328\n",
      "|    |    └─LayerNorm: 3-3               2,048\n",
      "|    |    └─FFN: 3-4                     8,393,728\n",
      "|    └─TransformerEncoder: 2-3           --\n",
      "|    |    └─LayerNorm: 3-5               2,048\n",
      "|    |    └─MultiHeadAtt: 3-6            4,195,328\n",
      "|    |    └─LayerNorm: 3-7               2,048\n",
      "|    |    └─FFN: 3-8                     8,393,728\n",
      "|    └─TransformerEncoder: 2-4           --\n",
      "|    |    └─LayerNorm: 3-9               2,048\n",
      "|    |    └─MultiHeadAtt: 3-10           4,195,328\n",
      "|    |    └─LayerNorm: 3-11              2,048\n",
      "|    |    └─FFN: 3-12                    8,393,728\n",
      "|    └─TransformerEncoder: 2-5           --\n",
      "|    |    └─LayerNorm: 3-13              2,048\n",
      "|    |    └─MultiHeadAtt: 3-14           4,195,328\n",
      "|    |    └─LayerNorm: 3-15              2,048\n",
      "|    |    └─FFN: 3-16                    8,393,728\n",
      "|    └─TransformerEncoder: 2-6           --\n",
      "|    |    └─LayerNorm: 3-17              2,048\n",
      "|    |    └─MultiHeadAtt: 3-18           4,195,328\n",
      "|    |    └─LayerNorm: 3-19              2,048\n",
      "|    |    └─FFN: 3-20                    8,393,728\n",
      "|    └─TransformerEncoder: 2-7           --\n",
      "|    |    └─LayerNorm: 3-21              2,048\n",
      "|    |    └─MultiHeadAtt: 3-22           4,195,328\n",
      "|    |    └─LayerNorm: 3-23              2,048\n",
      "|    |    └─FFN: 3-24                    8,393,728\n",
      "|    └─TransformerEncoder: 2-8           --\n",
      "|    |    └─LayerNorm: 3-25              2,048\n",
      "|    |    └─MultiHeadAtt: 3-26           4,195,328\n",
      "|    |    └─LayerNorm: 3-27              2,048\n",
      "|    |    └─FFN: 3-28                    8,393,728\n",
      "|    └─TransformerEncoder: 2-9           --\n",
      "|    |    └─LayerNorm: 3-29              2,048\n",
      "|    |    └─MultiHeadAtt: 3-30           4,195,328\n",
      "|    |    └─LayerNorm: 3-31              2,048\n",
      "|    |    └─FFN: 3-32                    8,393,728\n",
      "|    └─TransformerEncoder: 2-10          --\n",
      "|    |    └─LayerNorm: 3-33              2,048\n",
      "|    |    └─MultiHeadAtt: 3-34           4,195,328\n",
      "|    |    └─LayerNorm: 3-35              2,048\n",
      "|    |    └─FFN: 3-36                    8,393,728\n",
      "|    └─TransformerEncoder: 2-11          --\n",
      "|    |    └─LayerNorm: 3-37              2,048\n",
      "|    |    └─MultiHeadAtt: 3-38           4,195,328\n",
      "|    |    └─LayerNorm: 3-39              2,048\n",
      "|    |    └─FFN: 3-40                    8,393,728\n",
      "|    └─TransformerEncoder: 2-12          --\n",
      "|    |    └─LayerNorm: 3-41              2,048\n",
      "|    |    └─MultiHeadAtt: 3-42           4,195,328\n",
      "|    |    └─LayerNorm: 3-43              2,048\n",
      "|    |    └─FFN: 3-44                    8,393,728\n",
      "|    └─TransformerEncoder: 2-13          --\n",
      "|    |    └─LayerNorm: 3-45              2,048\n",
      "|    |    └─MultiHeadAtt: 3-46           4,195,328\n",
      "|    |    └─LayerNorm: 3-47              2,048\n",
      "|    |    └─FFN: 3-48                    8,393,728\n",
      "|    └─TransformerEncoder: 2-14          --\n",
      "|    |    └─LayerNorm: 3-49              2,048\n",
      "|    |    └─MultiHeadAtt: 3-50           4,195,328\n",
      "|    |    └─LayerNorm: 3-51              2,048\n",
      "|    |    └─FFN: 3-52                    8,393,728\n",
      "|    └─TransformerEncoder: 2-15          --\n",
      "|    |    └─LayerNorm: 3-53              2,048\n",
      "|    |    └─MultiHeadAtt: 3-54           4,195,328\n",
      "|    |    └─LayerNorm: 3-55              2,048\n",
      "|    |    └─FFN: 3-56                    8,393,728\n",
      "|    └─TransformerEncoder: 2-16          --\n",
      "|    |    └─LayerNorm: 3-57              2,048\n",
      "|    |    └─MultiHeadAtt: 3-58           4,195,328\n",
      "|    |    └─LayerNorm: 3-59              2,048\n",
      "|    |    └─FFN: 3-60                    8,393,728\n",
      "|    └─TransformerEncoder: 2-17          --\n",
      "|    |    └─LayerNorm: 3-61              2,048\n",
      "|    |    └─MultiHeadAtt: 3-62           4,195,328\n",
      "|    |    └─LayerNorm: 3-63              2,048\n",
      "|    |    └─FFN: 3-64                    8,393,728\n",
      "|    └─TransformerEncoder: 2-18          --\n",
      "|    |    └─LayerNorm: 3-65              2,048\n",
      "|    |    └─MultiHeadAtt: 3-66           4,195,328\n",
      "|    |    └─LayerNorm: 3-67              2,048\n",
      "|    |    └─FFN: 3-68                    8,393,728\n",
      "|    └─TransformerEncoder: 2-19          --\n",
      "|    |    └─LayerNorm: 3-69              2,048\n",
      "|    |    └─MultiHeadAtt: 3-70           4,195,328\n",
      "|    |    └─LayerNorm: 3-71              2,048\n",
      "|    |    └─FFN: 3-72                    8,393,728\n",
      "|    └─TransformerEncoder: 2-20          --\n",
      "|    |    └─LayerNorm: 3-73              2,048\n",
      "|    |    └─MultiHeadAtt: 3-74           4,195,328\n",
      "|    |    └─LayerNorm: 3-75              2,048\n",
      "|    |    └─FFN: 3-76                    8,393,728\n",
      "|    └─TransformerEncoder: 2-21          --\n",
      "|    |    └─LayerNorm: 3-77              2,048\n",
      "|    |    └─MultiHeadAtt: 3-78           4,195,328\n",
      "|    |    └─LayerNorm: 3-79              2,048\n",
      "|    |    └─FFN: 3-80                    8,393,728\n",
      "|    └─TransformerEncoder: 2-22          --\n",
      "|    |    └─LayerNorm: 3-81              2,048\n",
      "|    |    └─MultiHeadAtt: 3-82           4,195,328\n",
      "|    |    └─LayerNorm: 3-83              2,048\n",
      "|    |    └─FFN: 3-84                    8,393,728\n",
      "|    └─TransformerEncoder: 2-23          --\n",
      "|    |    └─LayerNorm: 3-85              2,048\n",
      "|    |    └─MultiHeadAtt: 3-86           4,195,328\n",
      "|    |    └─LayerNorm: 3-87              2,048\n",
      "|    |    └─FFN: 3-88                    8,393,728\n",
      "|    └─TransformerEncoder: 2-24          --\n",
      "|    |    └─LayerNorm: 3-89              2,048\n",
      "|    |    └─MultiHeadAtt: 3-90           4,195,328\n",
      "|    |    └─LayerNorm: 3-91              2,048\n",
      "|    |    └─FFN: 3-92                    8,393,728\n",
      "|    └─TransformerEncoder: 2-25          --\n",
      "|    |    └─LayerNorm: 3-93              2,048\n",
      "|    |    └─MultiHeadAtt: 3-94           4,195,328\n",
      "|    |    └─LayerNorm: 3-95              2,048\n",
      "|    |    └─FFN: 3-96                    8,393,728\n",
      "├─Linear: 1-3                            10,250\n",
      "├─LayerNorm: 1-4                         20\n",
      "=================================================================\n",
      "Total params: 303,033,374\n",
      "Trainable params: 303,033,374\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─PatchEmbed: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       787,456\n",
       "├─Sequential: 1-2                        --\n",
       "|    └─TransformerEncoder: 2-2           --\n",
       "|    |    └─LayerNorm: 3-1               2,048\n",
       "|    |    └─MultiHeadAtt: 3-2            4,195,328\n",
       "|    |    └─LayerNorm: 3-3               2,048\n",
       "|    |    └─FFN: 3-4                     8,393,728\n",
       "|    └─TransformerEncoder: 2-3           --\n",
       "|    |    └─LayerNorm: 3-5               2,048\n",
       "|    |    └─MultiHeadAtt: 3-6            4,195,328\n",
       "|    |    └─LayerNorm: 3-7               2,048\n",
       "|    |    └─FFN: 3-8                     8,393,728\n",
       "|    └─TransformerEncoder: 2-4           --\n",
       "|    |    └─LayerNorm: 3-9               2,048\n",
       "|    |    └─MultiHeadAtt: 3-10           4,195,328\n",
       "|    |    └─LayerNorm: 3-11              2,048\n",
       "|    |    └─FFN: 3-12                    8,393,728\n",
       "|    └─TransformerEncoder: 2-5           --\n",
       "|    |    └─LayerNorm: 3-13              2,048\n",
       "|    |    └─MultiHeadAtt: 3-14           4,195,328\n",
       "|    |    └─LayerNorm: 3-15              2,048\n",
       "|    |    └─FFN: 3-16                    8,393,728\n",
       "|    └─TransformerEncoder: 2-6           --\n",
       "|    |    └─LayerNorm: 3-17              2,048\n",
       "|    |    └─MultiHeadAtt: 3-18           4,195,328\n",
       "|    |    └─LayerNorm: 3-19              2,048\n",
       "|    |    └─FFN: 3-20                    8,393,728\n",
       "|    └─TransformerEncoder: 2-7           --\n",
       "|    |    └─LayerNorm: 3-21              2,048\n",
       "|    |    └─MultiHeadAtt: 3-22           4,195,328\n",
       "|    |    └─LayerNorm: 3-23              2,048\n",
       "|    |    └─FFN: 3-24                    8,393,728\n",
       "|    └─TransformerEncoder: 2-8           --\n",
       "|    |    └─LayerNorm: 3-25              2,048\n",
       "|    |    └─MultiHeadAtt: 3-26           4,195,328\n",
       "|    |    └─LayerNorm: 3-27              2,048\n",
       "|    |    └─FFN: 3-28                    8,393,728\n",
       "|    └─TransformerEncoder: 2-9           --\n",
       "|    |    └─LayerNorm: 3-29              2,048\n",
       "|    |    └─MultiHeadAtt: 3-30           4,195,328\n",
       "|    |    └─LayerNorm: 3-31              2,048\n",
       "|    |    └─FFN: 3-32                    8,393,728\n",
       "|    └─TransformerEncoder: 2-10          --\n",
       "|    |    └─LayerNorm: 3-33              2,048\n",
       "|    |    └─MultiHeadAtt: 3-34           4,195,328\n",
       "|    |    └─LayerNorm: 3-35              2,048\n",
       "|    |    └─FFN: 3-36                    8,393,728\n",
       "|    └─TransformerEncoder: 2-11          --\n",
       "|    |    └─LayerNorm: 3-37              2,048\n",
       "|    |    └─MultiHeadAtt: 3-38           4,195,328\n",
       "|    |    └─LayerNorm: 3-39              2,048\n",
       "|    |    └─FFN: 3-40                    8,393,728\n",
       "|    └─TransformerEncoder: 2-12          --\n",
       "|    |    └─LayerNorm: 3-41              2,048\n",
       "|    |    └─MultiHeadAtt: 3-42           4,195,328\n",
       "|    |    └─LayerNorm: 3-43              2,048\n",
       "|    |    └─FFN: 3-44                    8,393,728\n",
       "|    └─TransformerEncoder: 2-13          --\n",
       "|    |    └─LayerNorm: 3-45              2,048\n",
       "|    |    └─MultiHeadAtt: 3-46           4,195,328\n",
       "|    |    └─LayerNorm: 3-47              2,048\n",
       "|    |    └─FFN: 3-48                    8,393,728\n",
       "|    └─TransformerEncoder: 2-14          --\n",
       "|    |    └─LayerNorm: 3-49              2,048\n",
       "|    |    └─MultiHeadAtt: 3-50           4,195,328\n",
       "|    |    └─LayerNorm: 3-51              2,048\n",
       "|    |    └─FFN: 3-52                    8,393,728\n",
       "|    └─TransformerEncoder: 2-15          --\n",
       "|    |    └─LayerNorm: 3-53              2,048\n",
       "|    |    └─MultiHeadAtt: 3-54           4,195,328\n",
       "|    |    └─LayerNorm: 3-55              2,048\n",
       "|    |    └─FFN: 3-56                    8,393,728\n",
       "|    └─TransformerEncoder: 2-16          --\n",
       "|    |    └─LayerNorm: 3-57              2,048\n",
       "|    |    └─MultiHeadAtt: 3-58           4,195,328\n",
       "|    |    └─LayerNorm: 3-59              2,048\n",
       "|    |    └─FFN: 3-60                    8,393,728\n",
       "|    └─TransformerEncoder: 2-17          --\n",
       "|    |    └─LayerNorm: 3-61              2,048\n",
       "|    |    └─MultiHeadAtt: 3-62           4,195,328\n",
       "|    |    └─LayerNorm: 3-63              2,048\n",
       "|    |    └─FFN: 3-64                    8,393,728\n",
       "|    └─TransformerEncoder: 2-18          --\n",
       "|    |    └─LayerNorm: 3-65              2,048\n",
       "|    |    └─MultiHeadAtt: 3-66           4,195,328\n",
       "|    |    └─LayerNorm: 3-67              2,048\n",
       "|    |    └─FFN: 3-68                    8,393,728\n",
       "|    └─TransformerEncoder: 2-19          --\n",
       "|    |    └─LayerNorm: 3-69              2,048\n",
       "|    |    └─MultiHeadAtt: 3-70           4,195,328\n",
       "|    |    └─LayerNorm: 3-71              2,048\n",
       "|    |    └─FFN: 3-72                    8,393,728\n",
       "|    └─TransformerEncoder: 2-20          --\n",
       "|    |    └─LayerNorm: 3-73              2,048\n",
       "|    |    └─MultiHeadAtt: 3-74           4,195,328\n",
       "|    |    └─LayerNorm: 3-75              2,048\n",
       "|    |    └─FFN: 3-76                    8,393,728\n",
       "|    └─TransformerEncoder: 2-21          --\n",
       "|    |    └─LayerNorm: 3-77              2,048\n",
       "|    |    └─MultiHeadAtt: 3-78           4,195,328\n",
       "|    |    └─LayerNorm: 3-79              2,048\n",
       "|    |    └─FFN: 3-80                    8,393,728\n",
       "|    └─TransformerEncoder: 2-22          --\n",
       "|    |    └─LayerNorm: 3-81              2,048\n",
       "|    |    └─MultiHeadAtt: 3-82           4,195,328\n",
       "|    |    └─LayerNorm: 3-83              2,048\n",
       "|    |    └─FFN: 3-84                    8,393,728\n",
       "|    └─TransformerEncoder: 2-23          --\n",
       "|    |    └─LayerNorm: 3-85              2,048\n",
       "|    |    └─MultiHeadAtt: 3-86           4,195,328\n",
       "|    |    └─LayerNorm: 3-87              2,048\n",
       "|    |    └─FFN: 3-88                    8,393,728\n",
       "|    └─TransformerEncoder: 2-24          --\n",
       "|    |    └─LayerNorm: 3-89              2,048\n",
       "|    |    └─MultiHeadAtt: 3-90           4,195,328\n",
       "|    |    └─LayerNorm: 3-91              2,048\n",
       "|    |    └─FFN: 3-92                    8,393,728\n",
       "|    └─TransformerEncoder: 2-25          --\n",
       "|    |    └─LayerNorm: 3-93              2,048\n",
       "|    |    └─MultiHeadAtt: 3-94           4,195,328\n",
       "|    |    └─LayerNorm: 3-95              2,048\n",
       "|    |    └─FFN: 3-96                    8,393,728\n",
       "├─Linear: 1-3                            10,250\n",
       "├─LayerNorm: 1-4                         20\n",
       "=================================================================\n",
       "Total params: 303,033,374\n",
       "Trainable params: 303,033,374\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ViT variants -- ViT-L/16\n",
    "n_class = 10\n",
    "n_layer = 24\n",
    "d_model = 1024\n",
    "n_head = 16\n",
    "mlp_hidden = 4096\n",
    "\n",
    "vit_encoder = nn.Sequential(*[TransformerEncoder(d_model=d_model, head=n_head, d_ff_hid=mlp_hidden) for i in range(n_layer)])\n",
    "vit_l = ViT(img_size=224, patch_size=16, img_c=3, d_model=d_model, num_class=n_class, encoders=vit_encoder)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = vit_l(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(vit_l, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─PatchEmbed: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       984,320\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─TransformerEncoder: 2-2           --\n",
      "|    |    └─LayerNorm: 3-1               2,560\n",
      "|    |    └─MultiHeadAtt: 3-2            6,554,880\n",
      "|    |    └─LayerNorm: 3-3               2,560\n",
      "|    |    └─FFN: 3-4                     13,113,600\n",
      "|    └─TransformerEncoder: 2-3           --\n",
      "|    |    └─LayerNorm: 3-5               2,560\n",
      "|    |    └─MultiHeadAtt: 3-6            6,554,880\n",
      "|    |    └─LayerNorm: 3-7               2,560\n",
      "|    |    └─FFN: 3-8                     13,113,600\n",
      "|    └─TransformerEncoder: 2-4           --\n",
      "|    |    └─LayerNorm: 3-9               2,560\n",
      "|    |    └─MultiHeadAtt: 3-10           6,554,880\n",
      "|    |    └─LayerNorm: 3-11              2,560\n",
      "|    |    └─FFN: 3-12                    13,113,600\n",
      "|    └─TransformerEncoder: 2-5           --\n",
      "|    |    └─LayerNorm: 3-13              2,560\n",
      "|    |    └─MultiHeadAtt: 3-14           6,554,880\n",
      "|    |    └─LayerNorm: 3-15              2,560\n",
      "|    |    └─FFN: 3-16                    13,113,600\n",
      "|    └─TransformerEncoder: 2-6           --\n",
      "|    |    └─LayerNorm: 3-17              2,560\n",
      "|    |    └─MultiHeadAtt: 3-18           6,554,880\n",
      "|    |    └─LayerNorm: 3-19              2,560\n",
      "|    |    └─FFN: 3-20                    13,113,600\n",
      "|    └─TransformerEncoder: 2-7           --\n",
      "|    |    └─LayerNorm: 3-21              2,560\n",
      "|    |    └─MultiHeadAtt: 3-22           6,554,880\n",
      "|    |    └─LayerNorm: 3-23              2,560\n",
      "|    |    └─FFN: 3-24                    13,113,600\n",
      "|    └─TransformerEncoder: 2-8           --\n",
      "|    |    └─LayerNorm: 3-25              2,560\n",
      "|    |    └─MultiHeadAtt: 3-26           6,554,880\n",
      "|    |    └─LayerNorm: 3-27              2,560\n",
      "|    |    └─FFN: 3-28                    13,113,600\n",
      "|    └─TransformerEncoder: 2-9           --\n",
      "|    |    └─LayerNorm: 3-29              2,560\n",
      "|    |    └─MultiHeadAtt: 3-30           6,554,880\n",
      "|    |    └─LayerNorm: 3-31              2,560\n",
      "|    |    └─FFN: 3-32                    13,113,600\n",
      "|    └─TransformerEncoder: 2-10          --\n",
      "|    |    └─LayerNorm: 3-33              2,560\n",
      "|    |    └─MultiHeadAtt: 3-34           6,554,880\n",
      "|    |    └─LayerNorm: 3-35              2,560\n",
      "|    |    └─FFN: 3-36                    13,113,600\n",
      "|    └─TransformerEncoder: 2-11          --\n",
      "|    |    └─LayerNorm: 3-37              2,560\n",
      "|    |    └─MultiHeadAtt: 3-38           6,554,880\n",
      "|    |    └─LayerNorm: 3-39              2,560\n",
      "|    |    └─FFN: 3-40                    13,113,600\n",
      "|    └─TransformerEncoder: 2-12          --\n",
      "|    |    └─LayerNorm: 3-41              2,560\n",
      "|    |    └─MultiHeadAtt: 3-42           6,554,880\n",
      "|    |    └─LayerNorm: 3-43              2,560\n",
      "|    |    └─FFN: 3-44                    13,113,600\n",
      "|    └─TransformerEncoder: 2-13          --\n",
      "|    |    └─LayerNorm: 3-45              2,560\n",
      "|    |    └─MultiHeadAtt: 3-46           6,554,880\n",
      "|    |    └─LayerNorm: 3-47              2,560\n",
      "|    |    └─FFN: 3-48                    13,113,600\n",
      "|    └─TransformerEncoder: 2-14          --\n",
      "|    |    └─LayerNorm: 3-49              2,560\n",
      "|    |    └─MultiHeadAtt: 3-50           6,554,880\n",
      "|    |    └─LayerNorm: 3-51              2,560\n",
      "|    |    └─FFN: 3-52                    13,113,600\n",
      "|    └─TransformerEncoder: 2-15          --\n",
      "|    |    └─LayerNorm: 3-53              2,560\n",
      "|    |    └─MultiHeadAtt: 3-54           6,554,880\n",
      "|    |    └─LayerNorm: 3-55              2,560\n",
      "|    |    └─FFN: 3-56                    13,113,600\n",
      "|    └─TransformerEncoder: 2-16          --\n",
      "|    |    └─LayerNorm: 3-57              2,560\n",
      "|    |    └─MultiHeadAtt: 3-58           6,554,880\n",
      "|    |    └─LayerNorm: 3-59              2,560\n",
      "|    |    └─FFN: 3-60                    13,113,600\n",
      "|    └─TransformerEncoder: 2-17          --\n",
      "|    |    └─LayerNorm: 3-61              2,560\n",
      "|    |    └─MultiHeadAtt: 3-62           6,554,880\n",
      "|    |    └─LayerNorm: 3-63              2,560\n",
      "|    |    └─FFN: 3-64                    13,113,600\n",
      "|    └─TransformerEncoder: 2-18          --\n",
      "|    |    └─LayerNorm: 3-65              2,560\n",
      "|    |    └─MultiHeadAtt: 3-66           6,554,880\n",
      "|    |    └─LayerNorm: 3-67              2,560\n",
      "|    |    └─FFN: 3-68                    13,113,600\n",
      "|    └─TransformerEncoder: 2-19          --\n",
      "|    |    └─LayerNorm: 3-69              2,560\n",
      "|    |    └─MultiHeadAtt: 3-70           6,554,880\n",
      "|    |    └─LayerNorm: 3-71              2,560\n",
      "|    |    └─FFN: 3-72                    13,113,600\n",
      "|    └─TransformerEncoder: 2-20          --\n",
      "|    |    └─LayerNorm: 3-73              2,560\n",
      "|    |    └─MultiHeadAtt: 3-74           6,554,880\n",
      "|    |    └─LayerNorm: 3-75              2,560\n",
      "|    |    └─FFN: 3-76                    13,113,600\n",
      "|    └─TransformerEncoder: 2-21          --\n",
      "|    |    └─LayerNorm: 3-77              2,560\n",
      "|    |    └─MultiHeadAtt: 3-78           6,554,880\n",
      "|    |    └─LayerNorm: 3-79              2,560\n",
      "|    |    └─FFN: 3-80                    13,113,600\n",
      "|    └─TransformerEncoder: 2-22          --\n",
      "|    |    └─LayerNorm: 3-81              2,560\n",
      "|    |    └─MultiHeadAtt: 3-82           6,554,880\n",
      "|    |    └─LayerNorm: 3-83              2,560\n",
      "|    |    └─FFN: 3-84                    13,113,600\n",
      "|    └─TransformerEncoder: 2-23          --\n",
      "|    |    └─LayerNorm: 3-85              2,560\n",
      "|    |    └─MultiHeadAtt: 3-86           6,554,880\n",
      "|    |    └─LayerNorm: 3-87              2,560\n",
      "|    |    └─FFN: 3-88                    13,113,600\n",
      "|    └─TransformerEncoder: 2-24          --\n",
      "|    |    └─LayerNorm: 3-89              2,560\n",
      "|    |    └─MultiHeadAtt: 3-90           6,554,880\n",
      "|    |    └─LayerNorm: 3-91              2,560\n",
      "|    |    └─FFN: 3-92                    13,113,600\n",
      "|    └─TransformerEncoder: 2-25          --\n",
      "|    |    └─LayerNorm: 3-93              2,560\n",
      "|    |    └─MultiHeadAtt: 3-94           6,554,880\n",
      "|    |    └─LayerNorm: 3-95              2,560\n",
      "|    |    └─FFN: 3-96                    13,113,600\n",
      "|    └─TransformerEncoder: 2-26          --\n",
      "|    |    └─LayerNorm: 3-97              2,560\n",
      "|    |    └─MultiHeadAtt: 3-98           6,554,880\n",
      "|    |    └─LayerNorm: 3-99              2,560\n",
      "|    |    └─FFN: 3-100                   13,113,600\n",
      "|    └─TransformerEncoder: 2-27          --\n",
      "|    |    └─LayerNorm: 3-101             2,560\n",
      "|    |    └─MultiHeadAtt: 3-102          6,554,880\n",
      "|    |    └─LayerNorm: 3-103             2,560\n",
      "|    |    └─FFN: 3-104                   13,113,600\n",
      "|    └─TransformerEncoder: 2-28          --\n",
      "|    |    └─LayerNorm: 3-105             2,560\n",
      "|    |    └─MultiHeadAtt: 3-106          6,554,880\n",
      "|    |    └─LayerNorm: 3-107             2,560\n",
      "|    |    └─FFN: 3-108                   13,113,600\n",
      "|    └─TransformerEncoder: 2-29          --\n",
      "|    |    └─LayerNorm: 3-109             2,560\n",
      "|    |    └─MultiHeadAtt: 3-110          6,554,880\n",
      "|    |    └─LayerNorm: 3-111             2,560\n",
      "|    |    └─FFN: 3-112                   13,113,600\n",
      "|    └─TransformerEncoder: 2-30          --\n",
      "|    |    └─LayerNorm: 3-113             2,560\n",
      "|    |    └─MultiHeadAtt: 3-114          6,554,880\n",
      "|    |    └─LayerNorm: 3-115             2,560\n",
      "|    |    └─FFN: 3-116                   13,113,600\n",
      "|    └─TransformerEncoder: 2-31          --\n",
      "|    |    └─LayerNorm: 3-117             2,560\n",
      "|    |    └─MultiHeadAtt: 3-118          6,554,880\n",
      "|    |    └─LayerNorm: 3-119             2,560\n",
      "|    |    └─FFN: 3-120                   13,113,600\n",
      "|    └─TransformerEncoder: 2-32          --\n",
      "|    |    └─LayerNorm: 3-121             2,560\n",
      "|    |    └─MultiHeadAtt: 3-122          6,554,880\n",
      "|    |    └─LayerNorm: 3-123             2,560\n",
      "|    |    └─FFN: 3-124                   13,113,600\n",
      "|    └─TransformerEncoder: 2-33          --\n",
      "|    |    └─LayerNorm: 3-125             2,560\n",
      "|    |    └─MultiHeadAtt: 3-126          6,554,880\n",
      "|    |    └─LayerNorm: 3-127             2,560\n",
      "|    |    └─FFN: 3-128                   13,113,600\n",
      "├─Linear: 1-3                            12,810\n",
      "├─LayerNorm: 1-4                         20\n",
      "=================================================================\n",
      "Total params: 630,552,350\n",
      "Trainable params: 630,552,350\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─PatchEmbed: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       984,320\n",
       "├─Sequential: 1-2                        --\n",
       "|    └─TransformerEncoder: 2-2           --\n",
       "|    |    └─LayerNorm: 3-1               2,560\n",
       "|    |    └─MultiHeadAtt: 3-2            6,554,880\n",
       "|    |    └─LayerNorm: 3-3               2,560\n",
       "|    |    └─FFN: 3-4                     13,113,600\n",
       "|    └─TransformerEncoder: 2-3           --\n",
       "|    |    └─LayerNorm: 3-5               2,560\n",
       "|    |    └─MultiHeadAtt: 3-6            6,554,880\n",
       "|    |    └─LayerNorm: 3-7               2,560\n",
       "|    |    └─FFN: 3-8                     13,113,600\n",
       "|    └─TransformerEncoder: 2-4           --\n",
       "|    |    └─LayerNorm: 3-9               2,560\n",
       "|    |    └─MultiHeadAtt: 3-10           6,554,880\n",
       "|    |    └─LayerNorm: 3-11              2,560\n",
       "|    |    └─FFN: 3-12                    13,113,600\n",
       "|    └─TransformerEncoder: 2-5           --\n",
       "|    |    └─LayerNorm: 3-13              2,560\n",
       "|    |    └─MultiHeadAtt: 3-14           6,554,880\n",
       "|    |    └─LayerNorm: 3-15              2,560\n",
       "|    |    └─FFN: 3-16                    13,113,600\n",
       "|    └─TransformerEncoder: 2-6           --\n",
       "|    |    └─LayerNorm: 3-17              2,560\n",
       "|    |    └─MultiHeadAtt: 3-18           6,554,880\n",
       "|    |    └─LayerNorm: 3-19              2,560\n",
       "|    |    └─FFN: 3-20                    13,113,600\n",
       "|    └─TransformerEncoder: 2-7           --\n",
       "|    |    └─LayerNorm: 3-21              2,560\n",
       "|    |    └─MultiHeadAtt: 3-22           6,554,880\n",
       "|    |    └─LayerNorm: 3-23              2,560\n",
       "|    |    └─FFN: 3-24                    13,113,600\n",
       "|    └─TransformerEncoder: 2-8           --\n",
       "|    |    └─LayerNorm: 3-25              2,560\n",
       "|    |    └─MultiHeadAtt: 3-26           6,554,880\n",
       "|    |    └─LayerNorm: 3-27              2,560\n",
       "|    |    └─FFN: 3-28                    13,113,600\n",
       "|    └─TransformerEncoder: 2-9           --\n",
       "|    |    └─LayerNorm: 3-29              2,560\n",
       "|    |    └─MultiHeadAtt: 3-30           6,554,880\n",
       "|    |    └─LayerNorm: 3-31              2,560\n",
       "|    |    └─FFN: 3-32                    13,113,600\n",
       "|    └─TransformerEncoder: 2-10          --\n",
       "|    |    └─LayerNorm: 3-33              2,560\n",
       "|    |    └─MultiHeadAtt: 3-34           6,554,880\n",
       "|    |    └─LayerNorm: 3-35              2,560\n",
       "|    |    └─FFN: 3-36                    13,113,600\n",
       "|    └─TransformerEncoder: 2-11          --\n",
       "|    |    └─LayerNorm: 3-37              2,560\n",
       "|    |    └─MultiHeadAtt: 3-38           6,554,880\n",
       "|    |    └─LayerNorm: 3-39              2,560\n",
       "|    |    └─FFN: 3-40                    13,113,600\n",
       "|    └─TransformerEncoder: 2-12          --\n",
       "|    |    └─LayerNorm: 3-41              2,560\n",
       "|    |    └─MultiHeadAtt: 3-42           6,554,880\n",
       "|    |    └─LayerNorm: 3-43              2,560\n",
       "|    |    └─FFN: 3-44                    13,113,600\n",
       "|    └─TransformerEncoder: 2-13          --\n",
       "|    |    └─LayerNorm: 3-45              2,560\n",
       "|    |    └─MultiHeadAtt: 3-46           6,554,880\n",
       "|    |    └─LayerNorm: 3-47              2,560\n",
       "|    |    └─FFN: 3-48                    13,113,600\n",
       "|    └─TransformerEncoder: 2-14          --\n",
       "|    |    └─LayerNorm: 3-49              2,560\n",
       "|    |    └─MultiHeadAtt: 3-50           6,554,880\n",
       "|    |    └─LayerNorm: 3-51              2,560\n",
       "|    |    └─FFN: 3-52                    13,113,600\n",
       "|    └─TransformerEncoder: 2-15          --\n",
       "|    |    └─LayerNorm: 3-53              2,560\n",
       "|    |    └─MultiHeadAtt: 3-54           6,554,880\n",
       "|    |    └─LayerNorm: 3-55              2,560\n",
       "|    |    └─FFN: 3-56                    13,113,600\n",
       "|    └─TransformerEncoder: 2-16          --\n",
       "|    |    └─LayerNorm: 3-57              2,560\n",
       "|    |    └─MultiHeadAtt: 3-58           6,554,880\n",
       "|    |    └─LayerNorm: 3-59              2,560\n",
       "|    |    └─FFN: 3-60                    13,113,600\n",
       "|    └─TransformerEncoder: 2-17          --\n",
       "|    |    └─LayerNorm: 3-61              2,560\n",
       "|    |    └─MultiHeadAtt: 3-62           6,554,880\n",
       "|    |    └─LayerNorm: 3-63              2,560\n",
       "|    |    └─FFN: 3-64                    13,113,600\n",
       "|    └─TransformerEncoder: 2-18          --\n",
       "|    |    └─LayerNorm: 3-65              2,560\n",
       "|    |    └─MultiHeadAtt: 3-66           6,554,880\n",
       "|    |    └─LayerNorm: 3-67              2,560\n",
       "|    |    └─FFN: 3-68                    13,113,600\n",
       "|    └─TransformerEncoder: 2-19          --\n",
       "|    |    └─LayerNorm: 3-69              2,560\n",
       "|    |    └─MultiHeadAtt: 3-70           6,554,880\n",
       "|    |    └─LayerNorm: 3-71              2,560\n",
       "|    |    └─FFN: 3-72                    13,113,600\n",
       "|    └─TransformerEncoder: 2-20          --\n",
       "|    |    └─LayerNorm: 3-73              2,560\n",
       "|    |    └─MultiHeadAtt: 3-74           6,554,880\n",
       "|    |    └─LayerNorm: 3-75              2,560\n",
       "|    |    └─FFN: 3-76                    13,113,600\n",
       "|    └─TransformerEncoder: 2-21          --\n",
       "|    |    └─LayerNorm: 3-77              2,560\n",
       "|    |    └─MultiHeadAtt: 3-78           6,554,880\n",
       "|    |    └─LayerNorm: 3-79              2,560\n",
       "|    |    └─FFN: 3-80                    13,113,600\n",
       "|    └─TransformerEncoder: 2-22          --\n",
       "|    |    └─LayerNorm: 3-81              2,560\n",
       "|    |    └─MultiHeadAtt: 3-82           6,554,880\n",
       "|    |    └─LayerNorm: 3-83              2,560\n",
       "|    |    └─FFN: 3-84                    13,113,600\n",
       "|    └─TransformerEncoder: 2-23          --\n",
       "|    |    └─LayerNorm: 3-85              2,560\n",
       "|    |    └─MultiHeadAtt: 3-86           6,554,880\n",
       "|    |    └─LayerNorm: 3-87              2,560\n",
       "|    |    └─FFN: 3-88                    13,113,600\n",
       "|    └─TransformerEncoder: 2-24          --\n",
       "|    |    └─LayerNorm: 3-89              2,560\n",
       "|    |    └─MultiHeadAtt: 3-90           6,554,880\n",
       "|    |    └─LayerNorm: 3-91              2,560\n",
       "|    |    └─FFN: 3-92                    13,113,600\n",
       "|    └─TransformerEncoder: 2-25          --\n",
       "|    |    └─LayerNorm: 3-93              2,560\n",
       "|    |    └─MultiHeadAtt: 3-94           6,554,880\n",
       "|    |    └─LayerNorm: 3-95              2,560\n",
       "|    |    └─FFN: 3-96                    13,113,600\n",
       "|    └─TransformerEncoder: 2-26          --\n",
       "|    |    └─LayerNorm: 3-97              2,560\n",
       "|    |    └─MultiHeadAtt: 3-98           6,554,880\n",
       "|    |    └─LayerNorm: 3-99              2,560\n",
       "|    |    └─FFN: 3-100                   13,113,600\n",
       "|    └─TransformerEncoder: 2-27          --\n",
       "|    |    └─LayerNorm: 3-101             2,560\n",
       "|    |    └─MultiHeadAtt: 3-102          6,554,880\n",
       "|    |    └─LayerNorm: 3-103             2,560\n",
       "|    |    └─FFN: 3-104                   13,113,600\n",
       "|    └─TransformerEncoder: 2-28          --\n",
       "|    |    └─LayerNorm: 3-105             2,560\n",
       "|    |    └─MultiHeadAtt: 3-106          6,554,880\n",
       "|    |    └─LayerNorm: 3-107             2,560\n",
       "|    |    └─FFN: 3-108                   13,113,600\n",
       "|    └─TransformerEncoder: 2-29          --\n",
       "|    |    └─LayerNorm: 3-109             2,560\n",
       "|    |    └─MultiHeadAtt: 3-110          6,554,880\n",
       "|    |    └─LayerNorm: 3-111             2,560\n",
       "|    |    └─FFN: 3-112                   13,113,600\n",
       "|    └─TransformerEncoder: 2-30          --\n",
       "|    |    └─LayerNorm: 3-113             2,560\n",
       "|    |    └─MultiHeadAtt: 3-114          6,554,880\n",
       "|    |    └─LayerNorm: 3-115             2,560\n",
       "|    |    └─FFN: 3-116                   13,113,600\n",
       "|    └─TransformerEncoder: 2-31          --\n",
       "|    |    └─LayerNorm: 3-117             2,560\n",
       "|    |    └─MultiHeadAtt: 3-118          6,554,880\n",
       "|    |    └─LayerNorm: 3-119             2,560\n",
       "|    |    └─FFN: 3-120                   13,113,600\n",
       "|    └─TransformerEncoder: 2-32          --\n",
       "|    |    └─LayerNorm: 3-121             2,560\n",
       "|    |    └─MultiHeadAtt: 3-122          6,554,880\n",
       "|    |    └─LayerNorm: 3-123             2,560\n",
       "|    |    └─FFN: 3-124                   13,113,600\n",
       "|    └─TransformerEncoder: 2-33          --\n",
       "|    |    └─LayerNorm: 3-125             2,560\n",
       "|    |    └─MultiHeadAtt: 3-126          6,554,880\n",
       "|    |    └─LayerNorm: 3-127             2,560\n",
       "|    |    └─FFN: 3-128                   13,113,600\n",
       "├─Linear: 1-3                            12,810\n",
       "├─LayerNorm: 1-4                         20\n",
       "=================================================================\n",
       "Total params: 630,552,350\n",
       "Trainable params: 630,552,350\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ViT variants -- ViT-H/16\n",
    "n_class = 10\n",
    "n_layer = 32\n",
    "d_model = 1280\n",
    "n_head = 16\n",
    "mlp_hidden = 5120\n",
    "\n",
    "vit_encoder = nn.Sequential(*[TransformerEncoder(d_model=d_model, head=n_head, d_ff_hid=mlp_hidden) for i in range(n_layer)])\n",
    "vit_h = ViT(img_size=224, patch_size=16, img_c=3, d_model=d_model, num_class=n_class, encoders=vit_encoder)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = vit_h(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(vit_h, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://github.com/zhoudaquan/dvit_repo/blob/master/models/layers.py\n",
    "class ReAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    It is observed that similarity along same batch of data is extremely large. \n",
    "    Thus can reduce the bs dimension when calculating the attention map.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,expansion_ratio = 3, apply_transform=True, transform_scale=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.apply_transform = apply_transform\n",
    "        \n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        if apply_transform:\n",
    "            self.reatten_matrix = nn.Conv2d(self.num_heads,self.num_heads, 1, 1)\n",
    "            self.var_norm = nn.BatchNorm2d(self.num_heads)\n",
    "            self.qkv = nn.Linear(dim, dim * expansion_ratio, bias=qkv_bias)\n",
    "            self.reatten_scale = self.scale if transform_scale else 1.0\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, dim * expansion_ratio, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x, atten=None):\n",
    "        B, N, C = x.shape\n",
    "        # x = self.fc(x)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        if self.apply_transform:\n",
    "            attn = self.var_norm(self.reatten_matrix(attn)) * self.reatten_scale\n",
    "        attn_next = attn\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReAttention(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.randn(12, 196, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 196, 64]) torch.Size([12, 8, 196, 196])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].shape, output[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/cait.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAttention(MultiHeadAtt):\n",
    "    \"\"\"New module of CaiT, ClassAttention only utilize the tokens to compute the final class for class token\n",
    "    \n",
    "    This is the simplified version referenced from:\n",
    "    https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/cait.py#L74-L106\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x N x C tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - N: number of token\n",
    "        - C: number of channel\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        queries = self.W_q(\"practice\") # Only takes class token as query\n",
    "        keys = self.W_k(x) # B x N x head*d_head\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        queries = queries.reshape(B, 1, self.head, self.d_head).permute(0, 2, 1, 3) # B x head x N x d_head\n",
    "        keys = keys.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        values = values.reshape(B, N, self.head, self.d_head).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (queries @ keys.transpose(-2, -1)) / self.d_head ** 0.5\n",
    "        attn = F.softmax(attn, dim=-1) # B x head x 1 x N\n",
    "        \n",
    "        x = attn @ values # B x head x 1 x h_head\n",
    "        x = x.transpose(1, 2) # B x 1 x head x h_head\n",
    "        x = x.reshape(B, 1, C) # B x 1 x head*h_head - Remind: d_model = C = head*h_head\n",
    "        \n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerScale Architecture\n",
    "![](assets/layer_scale.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale(nn.Module):\n",
    "    \"\"\"Proposed layer of CaiT to make the optimization more stable\"\"\"\n",
    "    def __init__(self, n_channel, agg_block, init_val=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(init_val * torch.ones((\"practice\")))\n",
    "        self.layer_norm = nn.LayerNorm(n_channel)\n",
    "        self.agg_block = agg_block\n",
    "    \n",
    "    def forward(self, x, x_res):\n",
    "        return x_res + self.gamma * self.agg_block(self.layer_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CABlock(nn.Module):\n",
    "    \"\"\"Basic block of CaiT, utilize at the end of the network\"\"\"\n",
    "    def __init__(self, d_model, head, mlp_hidden):\n",
    "        super().__init__()\n",
    "        self.cls_attn = LayerScale(n_channel=d_model, agg_block=ClassAttention(d_model, head))\n",
    "        self.mlp = LayerScale(n_channel=d_model, agg_block=FFN(d_model, mlp_hidden))\n",
    "    \n",
    "    def forward(self, x, x_cls):\n",
    "        u = torch.cat([x_cls, x], dim=1)\n",
    "        x_cls = self.cls_attn(u, x_cls)\n",
    "        x_cls = self.mlp(x_cls, x_cls)\n",
    "        return x_cls\n",
    "\n",
    "class SABlock(nn.Module):\n",
    "    \"\"\"Basic block of CaiT, utilize at the beginning of the network\"\"\"\n",
    "    def __init__(self, d_model, head, mlp_hidden):\n",
    "        super().__init__()\n",
    "        self.attn = LayerScale(n_channel=d_model, agg_block=MultiHeadAtt(d_model, head))\n",
    "        self.mlp = LayerScale(n_channel=d_model, agg_block=FFN(d_model, mlp_hidden))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attn(x, x)\n",
    "        x = self.mlp(x, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaiT(nn.Module):\n",
    "    \"\"\"A skeleton of a typical CaiT model\"\"\"\n",
    "    def __init__(self, img_size, patch_size, img_c, d_model, head, mlp_hidden, num_class, n_sa, n_ca):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.patch_embeder = PatchEmbed(img_size, patch_size, img_c, d_model)\n",
    "        \n",
    "        num_patches = self.patch_embeder.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, d_model))\n",
    "        \n",
    "        self.sa_blocks = nn.Sequential(*[SABlock(d_model, head, mlp_hidden) for i in range(n_sa)])\n",
    "        self.ca_blocks = nn.ModuleList([CABlock(d_model, head, mlp_hidden) for i in range(n_ca)])\n",
    "        self.mlp_head = nn.Linear(d_model, num_class)\n",
    "        self.cls_morm = nn.LayerNorm(num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: a B x C x H x W image tensor\n",
    "        \n",
    "        Annotations:\n",
    "        - B: batch size\n",
    "        - C: number of channel\n",
    "        - H: height\n",
    "        - W: width\n",
    "        \"\"\"\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x_embed = self.patch_embeder(x)\n",
    "        x_embed = x_embed + self.pos_embed\n",
    "        \n",
    "        x_embed = self.sa_blocks(x_embed)\n",
    "        \n",
    "        for block in self.ca_blocks:\n",
    "            cls_token = block(x_embed, cls_token)\n",
    "\n",
    "        cls_logits = self.mlp_head(cls_token) \n",
    "        cls_logits = self.cls_morm(cls_logits) # B x 1 x n_class\n",
    "        return cls_logits.squeeze(dim=1) # B x n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─PatchEmbed: 1-1                             --\n",
      "|    └─Conv2d: 2-1                            221,472\n",
      "├─Sequential: 1-2                             --\n",
      "|    └─SABlock: 2-2                           --\n",
      "|    |    └─LayerScale: 3-1                   332,928\n",
      "|    |    └─LayerScale: 3-2                   665,856\n",
      "|    └─SABlock: 2-3                           --\n",
      "|    |    └─LayerScale: 3-3                   332,928\n",
      "|    |    └─LayerScale: 3-4                   665,856\n",
      "|    └─SABlock: 2-4                           --\n",
      "|    |    └─LayerScale: 3-5                   332,928\n",
      "|    |    └─LayerScale: 3-6                   665,856\n",
      "|    └─SABlock: 2-5                           --\n",
      "|    |    └─LayerScale: 3-7                   332,928\n",
      "|    |    └─LayerScale: 3-8                   665,856\n",
      "|    └─SABlock: 2-6                           --\n",
      "|    |    └─LayerScale: 3-9                   332,928\n",
      "|    |    └─LayerScale: 3-10                  665,856\n",
      "|    └─SABlock: 2-7                           --\n",
      "|    |    └─LayerScale: 3-11                  332,928\n",
      "|    |    └─LayerScale: 3-12                  665,856\n",
      "|    └─SABlock: 2-8                           --\n",
      "|    |    └─LayerScale: 3-13                  332,928\n",
      "|    |    └─LayerScale: 3-14                  665,856\n",
      "|    └─SABlock: 2-9                           --\n",
      "|    |    └─LayerScale: 3-15                  332,928\n",
      "|    |    └─LayerScale: 3-16                  665,856\n",
      "|    └─SABlock: 2-10                          --\n",
      "|    |    └─LayerScale: 3-17                  332,928\n",
      "|    |    └─LayerScale: 3-18                  665,856\n",
      "|    └─SABlock: 2-11                          --\n",
      "|    |    └─LayerScale: 3-19                  332,928\n",
      "|    |    └─LayerScale: 3-20                  665,856\n",
      "|    └─SABlock: 2-12                          --\n",
      "|    |    └─LayerScale: 3-21                  332,928\n",
      "|    |    └─LayerScale: 3-22                  665,856\n",
      "|    └─SABlock: 2-13                          --\n",
      "|    |    └─LayerScale: 3-23                  332,928\n",
      "|    |    └─LayerScale: 3-24                  665,856\n",
      "|    └─SABlock: 2-14                          --\n",
      "|    |    └─LayerScale: 3-25                  332,928\n",
      "|    |    └─LayerScale: 3-26                  665,856\n",
      "|    └─SABlock: 2-15                          --\n",
      "|    |    └─LayerScale: 3-27                  332,928\n",
      "|    |    └─LayerScale: 3-28                  665,856\n",
      "|    └─SABlock: 2-16                          --\n",
      "|    |    └─LayerScale: 3-29                  332,928\n",
      "|    |    └─LayerScale: 3-30                  665,856\n",
      "|    └─SABlock: 2-17                          --\n",
      "|    |    └─LayerScale: 3-31                  332,928\n",
      "|    |    └─LayerScale: 3-32                  665,856\n",
      "|    └─SABlock: 2-18                          --\n",
      "|    |    └─LayerScale: 3-33                  332,928\n",
      "|    |    └─LayerScale: 3-34                  665,856\n",
      "|    └─SABlock: 2-19                          --\n",
      "|    |    └─LayerScale: 3-35                  332,928\n",
      "|    |    └─LayerScale: 3-36                  665,856\n",
      "|    └─SABlock: 2-20                          --\n",
      "|    |    └─LayerScale: 3-37                  332,928\n",
      "|    |    └─LayerScale: 3-38                  665,856\n",
      "|    └─SABlock: 2-21                          --\n",
      "|    |    └─LayerScale: 3-39                  332,928\n",
      "|    |    └─LayerScale: 3-40                  665,856\n",
      "|    └─SABlock: 2-22                          --\n",
      "|    |    └─LayerScale: 3-41                  332,928\n",
      "|    |    └─LayerScale: 3-42                  665,856\n",
      "|    └─SABlock: 2-23                          --\n",
      "|    |    └─LayerScale: 3-43                  332,928\n",
      "|    |    └─LayerScale: 3-44                  665,856\n",
      "|    └─SABlock: 2-24                          --\n",
      "|    |    └─LayerScale: 3-45                  332,928\n",
      "|    |    └─LayerScale: 3-46                  665,856\n",
      "|    └─SABlock: 2-25                          --\n",
      "|    |    └─LayerScale: 3-47                  332,928\n",
      "|    |    └─LayerScale: 3-48                  665,856\n",
      "|    └─SABlock: 2-26                          --\n",
      "|    |    └─LayerScale: 3-49                  332,928\n",
      "|    |    └─LayerScale: 3-50                  665,856\n",
      "|    └─SABlock: 2-27                          --\n",
      "|    |    └─LayerScale: 3-51                  332,928\n",
      "|    |    └─LayerScale: 3-52                  665,856\n",
      "|    └─SABlock: 2-28                          --\n",
      "|    |    └─LayerScale: 3-53                  332,928\n",
      "|    |    └─LayerScale: 3-54                  665,856\n",
      "|    └─SABlock: 2-29                          --\n",
      "|    |    └─LayerScale: 3-55                  332,928\n",
      "|    |    └─LayerScale: 3-56                  665,856\n",
      "|    └─SABlock: 2-30                          --\n",
      "|    |    └─LayerScale: 3-57                  332,928\n",
      "|    |    └─LayerScale: 3-58                  665,856\n",
      "|    └─SABlock: 2-31                          --\n",
      "|    |    └─LayerScale: 3-59                  332,928\n",
      "|    |    └─LayerScale: 3-60                  665,856\n",
      "|    └─SABlock: 2-32                          --\n",
      "|    |    └─LayerScale: 3-61                  332,928\n",
      "|    |    └─LayerScale: 3-62                  665,856\n",
      "|    └─SABlock: 2-33                          --\n",
      "|    |    └─LayerScale: 3-63                  332,928\n",
      "|    |    └─LayerScale: 3-64                  665,856\n",
      "|    └─SABlock: 2-34                          --\n",
      "|    |    └─LayerScale: 3-65                  332,928\n",
      "|    |    └─LayerScale: 3-66                  665,856\n",
      "|    └─SABlock: 2-35                          --\n",
      "|    |    └─LayerScale: 3-67                  332,928\n",
      "|    |    └─LayerScale: 3-68                  665,856\n",
      "|    └─SABlock: 2-36                          --\n",
      "|    |    └─LayerScale: 3-69                  332,928\n",
      "|    |    └─LayerScale: 3-70                  665,856\n",
      "|    └─SABlock: 2-37                          --\n",
      "|    |    └─LayerScale: 3-71                  332,928\n",
      "|    |    └─LayerScale: 3-72                  665,856\n",
      "├─ModuleList: 1-3                             --\n",
      "|    └─CABlock: 2-38                          --\n",
      "|    |    └─LayerScale: 3-73                  332,928\n",
      "|    |    └─LayerScale: 3-74                  665,856\n",
      "|    └─CABlock: 2-39                          --\n",
      "|    |    └─LayerScale: 3-75                  332,928\n",
      "|    |    └─LayerScale: 3-76                  665,856\n",
      "├─Linear: 1-4                                 2,890\n",
      "├─LayerNorm: 1-5                              20\n",
      "======================================================================\n",
      "Total params: 38,178,174\n",
      "Trainable params: 38,178,174\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─PatchEmbed: 1-1                             --\n",
       "|    └─Conv2d: 2-1                            221,472\n",
       "├─Sequential: 1-2                             --\n",
       "|    └─SABlock: 2-2                           --\n",
       "|    |    └─LayerScale: 3-1                   332,928\n",
       "|    |    └─LayerScale: 3-2                   665,856\n",
       "|    └─SABlock: 2-3                           --\n",
       "|    |    └─LayerScale: 3-3                   332,928\n",
       "|    |    └─LayerScale: 3-4                   665,856\n",
       "|    └─SABlock: 2-4                           --\n",
       "|    |    └─LayerScale: 3-5                   332,928\n",
       "|    |    └─LayerScale: 3-6                   665,856\n",
       "|    └─SABlock: 2-5                           --\n",
       "|    |    └─LayerScale: 3-7                   332,928\n",
       "|    |    └─LayerScale: 3-8                   665,856\n",
       "|    └─SABlock: 2-6                           --\n",
       "|    |    └─LayerScale: 3-9                   332,928\n",
       "|    |    └─LayerScale: 3-10                  665,856\n",
       "|    └─SABlock: 2-7                           --\n",
       "|    |    └─LayerScale: 3-11                  332,928\n",
       "|    |    └─LayerScale: 3-12                  665,856\n",
       "|    └─SABlock: 2-8                           --\n",
       "|    |    └─LayerScale: 3-13                  332,928\n",
       "|    |    └─LayerScale: 3-14                  665,856\n",
       "|    └─SABlock: 2-9                           --\n",
       "|    |    └─LayerScale: 3-15                  332,928\n",
       "|    |    └─LayerScale: 3-16                  665,856\n",
       "|    └─SABlock: 2-10                          --\n",
       "|    |    └─LayerScale: 3-17                  332,928\n",
       "|    |    └─LayerScale: 3-18                  665,856\n",
       "|    └─SABlock: 2-11                          --\n",
       "|    |    └─LayerScale: 3-19                  332,928\n",
       "|    |    └─LayerScale: 3-20                  665,856\n",
       "|    └─SABlock: 2-12                          --\n",
       "|    |    └─LayerScale: 3-21                  332,928\n",
       "|    |    └─LayerScale: 3-22                  665,856\n",
       "|    └─SABlock: 2-13                          --\n",
       "|    |    └─LayerScale: 3-23                  332,928\n",
       "|    |    └─LayerScale: 3-24                  665,856\n",
       "|    └─SABlock: 2-14                          --\n",
       "|    |    └─LayerScale: 3-25                  332,928\n",
       "|    |    └─LayerScale: 3-26                  665,856\n",
       "|    └─SABlock: 2-15                          --\n",
       "|    |    └─LayerScale: 3-27                  332,928\n",
       "|    |    └─LayerScale: 3-28                  665,856\n",
       "|    └─SABlock: 2-16                          --\n",
       "|    |    └─LayerScale: 3-29                  332,928\n",
       "|    |    └─LayerScale: 3-30                  665,856\n",
       "|    └─SABlock: 2-17                          --\n",
       "|    |    └─LayerScale: 3-31                  332,928\n",
       "|    |    └─LayerScale: 3-32                  665,856\n",
       "|    └─SABlock: 2-18                          --\n",
       "|    |    └─LayerScale: 3-33                  332,928\n",
       "|    |    └─LayerScale: 3-34                  665,856\n",
       "|    └─SABlock: 2-19                          --\n",
       "|    |    └─LayerScale: 3-35                  332,928\n",
       "|    |    └─LayerScale: 3-36                  665,856\n",
       "|    └─SABlock: 2-20                          --\n",
       "|    |    └─LayerScale: 3-37                  332,928\n",
       "|    |    └─LayerScale: 3-38                  665,856\n",
       "|    └─SABlock: 2-21                          --\n",
       "|    |    └─LayerScale: 3-39                  332,928\n",
       "|    |    └─LayerScale: 3-40                  665,856\n",
       "|    └─SABlock: 2-22                          --\n",
       "|    |    └─LayerScale: 3-41                  332,928\n",
       "|    |    └─LayerScale: 3-42                  665,856\n",
       "|    └─SABlock: 2-23                          --\n",
       "|    |    └─LayerScale: 3-43                  332,928\n",
       "|    |    └─LayerScale: 3-44                  665,856\n",
       "|    └─SABlock: 2-24                          --\n",
       "|    |    └─LayerScale: 3-45                  332,928\n",
       "|    |    └─LayerScale: 3-46                  665,856\n",
       "|    └─SABlock: 2-25                          --\n",
       "|    |    └─LayerScale: 3-47                  332,928\n",
       "|    |    └─LayerScale: 3-48                  665,856\n",
       "|    └─SABlock: 2-26                          --\n",
       "|    |    └─LayerScale: 3-49                  332,928\n",
       "|    |    └─LayerScale: 3-50                  665,856\n",
       "|    └─SABlock: 2-27                          --\n",
       "|    |    └─LayerScale: 3-51                  332,928\n",
       "|    |    └─LayerScale: 3-52                  665,856\n",
       "|    └─SABlock: 2-28                          --\n",
       "|    |    └─LayerScale: 3-53                  332,928\n",
       "|    |    └─LayerScale: 3-54                  665,856\n",
       "|    └─SABlock: 2-29                          --\n",
       "|    |    └─LayerScale: 3-55                  332,928\n",
       "|    |    └─LayerScale: 3-56                  665,856\n",
       "|    └─SABlock: 2-30                          --\n",
       "|    |    └─LayerScale: 3-57                  332,928\n",
       "|    |    └─LayerScale: 3-58                  665,856\n",
       "|    └─SABlock: 2-31                          --\n",
       "|    |    └─LayerScale: 3-59                  332,928\n",
       "|    |    └─LayerScale: 3-60                  665,856\n",
       "|    └─SABlock: 2-32                          --\n",
       "|    |    └─LayerScale: 3-61                  332,928\n",
       "|    |    └─LayerScale: 3-62                  665,856\n",
       "|    └─SABlock: 2-33                          --\n",
       "|    |    └─LayerScale: 3-63                  332,928\n",
       "|    |    └─LayerScale: 3-64                  665,856\n",
       "|    └─SABlock: 2-34                          --\n",
       "|    |    └─LayerScale: 3-65                  332,928\n",
       "|    |    └─LayerScale: 3-66                  665,856\n",
       "|    └─SABlock: 2-35                          --\n",
       "|    |    └─LayerScale: 3-67                  332,928\n",
       "|    |    └─LayerScale: 3-68                  665,856\n",
       "|    └─SABlock: 2-36                          --\n",
       "|    |    └─LayerScale: 3-69                  332,928\n",
       "|    |    └─LayerScale: 3-70                  665,856\n",
       "|    └─SABlock: 2-37                          --\n",
       "|    |    └─LayerScale: 3-71                  332,928\n",
       "|    |    └─LayerScale: 3-72                  665,856\n",
       "├─ModuleList: 1-3                             --\n",
       "|    └─CABlock: 2-38                          --\n",
       "|    |    └─LayerScale: 3-73                  332,928\n",
       "|    |    └─LayerScale: 3-74                  665,856\n",
       "|    └─CABlock: 2-39                          --\n",
       "|    |    └─LayerScale: 3-75                  332,928\n",
       "|    |    └─LayerScale: 3-76                  665,856\n",
       "├─Linear: 1-4                                 2,890\n",
       "├─LayerNorm: 1-5                              20\n",
       "======================================================================\n",
       "Total params: 38,178,174\n",
       "Trainable params: 38,178,174\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CaiT variants -- CaiT-XS36\n",
    "n_class = 10\n",
    "d_model = 288\n",
    "n_head = 6 # equals d_model/48\n",
    "mlp_hidden = 4 * d_model\n",
    "n_sa = 36\n",
    "n_ca = 2\n",
    "\n",
    "cait_xs36 = CaiT(img_size=224, patch_size=16, img_c=3, num_class=n_class,\n",
    "                d_model=d_model, head=n_head, mlp_hidden=mlp_hidden, n_sa=n_sa, n_ca=n_ca)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = cait_xs36(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(cait_xs36, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─PatchEmbed: 1-1                             --\n",
      "|    └─Conv2d: 2-1                            295,296\n",
      "├─Sequential: 1-2                             --\n",
      "|    └─SABlock: 2-2                           --\n",
      "|    |    └─LayerScale: 3-1                   591,360\n",
      "|    |    └─LayerScale: 3-2                   1,182,720\n",
      "|    └─SABlock: 2-3                           --\n",
      "|    |    └─LayerScale: 3-3                   591,360\n",
      "|    |    └─LayerScale: 3-4                   1,182,720\n",
      "|    └─SABlock: 2-4                           --\n",
      "|    |    └─LayerScale: 3-5                   591,360\n",
      "|    |    └─LayerScale: 3-6                   1,182,720\n",
      "|    └─SABlock: 2-5                           --\n",
      "|    |    └─LayerScale: 3-7                   591,360\n",
      "|    |    └─LayerScale: 3-8                   1,182,720\n",
      "|    └─SABlock: 2-6                           --\n",
      "|    |    └─LayerScale: 3-9                   591,360\n",
      "|    |    └─LayerScale: 3-10                  1,182,720\n",
      "|    └─SABlock: 2-7                           --\n",
      "|    |    └─LayerScale: 3-11                  591,360\n",
      "|    |    └─LayerScale: 3-12                  1,182,720\n",
      "|    └─SABlock: 2-8                           --\n",
      "|    |    └─LayerScale: 3-13                  591,360\n",
      "|    |    └─LayerScale: 3-14                  1,182,720\n",
      "|    └─SABlock: 2-9                           --\n",
      "|    |    └─LayerScale: 3-15                  591,360\n",
      "|    |    └─LayerScale: 3-16                  1,182,720\n",
      "|    └─SABlock: 2-10                          --\n",
      "|    |    └─LayerScale: 3-17                  591,360\n",
      "|    |    └─LayerScale: 3-18                  1,182,720\n",
      "|    └─SABlock: 2-11                          --\n",
      "|    |    └─LayerScale: 3-19                  591,360\n",
      "|    |    └─LayerScale: 3-20                  1,182,720\n",
      "|    └─SABlock: 2-12                          --\n",
      "|    |    └─LayerScale: 3-21                  591,360\n",
      "|    |    └─LayerScale: 3-22                  1,182,720\n",
      "|    └─SABlock: 2-13                          --\n",
      "|    |    └─LayerScale: 3-23                  591,360\n",
      "|    |    └─LayerScale: 3-24                  1,182,720\n",
      "|    └─SABlock: 2-14                          --\n",
      "|    |    └─LayerScale: 3-25                  591,360\n",
      "|    |    └─LayerScale: 3-26                  1,182,720\n",
      "|    └─SABlock: 2-15                          --\n",
      "|    |    └─LayerScale: 3-27                  591,360\n",
      "|    |    └─LayerScale: 3-28                  1,182,720\n",
      "|    └─SABlock: 2-16                          --\n",
      "|    |    └─LayerScale: 3-29                  591,360\n",
      "|    |    └─LayerScale: 3-30                  1,182,720\n",
      "|    └─SABlock: 2-17                          --\n",
      "|    |    └─LayerScale: 3-31                  591,360\n",
      "|    |    └─LayerScale: 3-32                  1,182,720\n",
      "|    └─SABlock: 2-18                          --\n",
      "|    |    └─LayerScale: 3-33                  591,360\n",
      "|    |    └─LayerScale: 3-34                  1,182,720\n",
      "|    └─SABlock: 2-19                          --\n",
      "|    |    └─LayerScale: 3-35                  591,360\n",
      "|    |    └─LayerScale: 3-36                  1,182,720\n",
      "|    └─SABlock: 2-20                          --\n",
      "|    |    └─LayerScale: 3-37                  591,360\n",
      "|    |    └─LayerScale: 3-38                  1,182,720\n",
      "|    └─SABlock: 2-21                          --\n",
      "|    |    └─LayerScale: 3-39                  591,360\n",
      "|    |    └─LayerScale: 3-40                  1,182,720\n",
      "|    └─SABlock: 2-22                          --\n",
      "|    |    └─LayerScale: 3-41                  591,360\n",
      "|    |    └─LayerScale: 3-42                  1,182,720\n",
      "|    └─SABlock: 2-23                          --\n",
      "|    |    └─LayerScale: 3-43                  591,360\n",
      "|    |    └─LayerScale: 3-44                  1,182,720\n",
      "|    └─SABlock: 2-24                          --\n",
      "|    |    └─LayerScale: 3-45                  591,360\n",
      "|    |    └─LayerScale: 3-46                  1,182,720\n",
      "|    └─SABlock: 2-25                          --\n",
      "|    |    └─LayerScale: 3-47                  591,360\n",
      "|    |    └─LayerScale: 3-48                  1,182,720\n",
      "|    └─SABlock: 2-26                          --\n",
      "|    |    └─LayerScale: 3-49                  591,360\n",
      "|    |    └─LayerScale: 3-50                  1,182,720\n",
      "|    └─SABlock: 2-27                          --\n",
      "|    |    └─LayerScale: 3-51                  591,360\n",
      "|    |    └─LayerScale: 3-52                  1,182,720\n",
      "|    └─SABlock: 2-28                          --\n",
      "|    |    └─LayerScale: 3-53                  591,360\n",
      "|    |    └─LayerScale: 3-54                  1,182,720\n",
      "|    └─SABlock: 2-29                          --\n",
      "|    |    └─LayerScale: 3-55                  591,360\n",
      "|    |    └─LayerScale: 3-56                  1,182,720\n",
      "|    └─SABlock: 2-30                          --\n",
      "|    |    └─LayerScale: 3-57                  591,360\n",
      "|    |    └─LayerScale: 3-58                  1,182,720\n",
      "|    └─SABlock: 2-31                          --\n",
      "|    |    └─LayerScale: 3-59                  591,360\n",
      "|    |    └─LayerScale: 3-60                  1,182,720\n",
      "|    └─SABlock: 2-32                          --\n",
      "|    |    └─LayerScale: 3-61                  591,360\n",
      "|    |    └─LayerScale: 3-62                  1,182,720\n",
      "|    └─SABlock: 2-33                          --\n",
      "|    |    └─LayerScale: 3-63                  591,360\n",
      "|    |    └─LayerScale: 3-64                  1,182,720\n",
      "|    └─SABlock: 2-34                          --\n",
      "|    |    └─LayerScale: 3-65                  591,360\n",
      "|    |    └─LayerScale: 3-66                  1,182,720\n",
      "|    └─SABlock: 2-35                          --\n",
      "|    |    └─LayerScale: 3-67                  591,360\n",
      "|    |    └─LayerScale: 3-68                  1,182,720\n",
      "|    └─SABlock: 2-36                          --\n",
      "|    |    └─LayerScale: 3-69                  591,360\n",
      "|    |    └─LayerScale: 3-70                  1,182,720\n",
      "|    └─SABlock: 2-37                          --\n",
      "|    |    └─LayerScale: 3-71                  591,360\n",
      "|    |    └─LayerScale: 3-72                  1,182,720\n",
      "├─ModuleList: 1-3                             --\n",
      "|    └─CABlock: 2-38                          --\n",
      "|    |    └─LayerScale: 3-73                  591,360\n",
      "|    |    └─LayerScale: 3-74                  1,182,720\n",
      "|    └─CABlock: 2-39                          --\n",
      "|    |    └─LayerScale: 3-75                  591,360\n",
      "|    |    └─LayerScale: 3-76                  1,182,720\n",
      "├─Linear: 1-4                                 3,850\n",
      "├─LayerNorm: 1-5                              20\n",
      "======================================================================\n",
      "Total params: 67,714,206\n",
      "Trainable params: 67,714,206\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─PatchEmbed: 1-1                             --\n",
       "|    └─Conv2d: 2-1                            295,296\n",
       "├─Sequential: 1-2                             --\n",
       "|    └─SABlock: 2-2                           --\n",
       "|    |    └─LayerScale: 3-1                   591,360\n",
       "|    |    └─LayerScale: 3-2                   1,182,720\n",
       "|    └─SABlock: 2-3                           --\n",
       "|    |    └─LayerScale: 3-3                   591,360\n",
       "|    |    └─LayerScale: 3-4                   1,182,720\n",
       "|    └─SABlock: 2-4                           --\n",
       "|    |    └─LayerScale: 3-5                   591,360\n",
       "|    |    └─LayerScale: 3-6                   1,182,720\n",
       "|    └─SABlock: 2-5                           --\n",
       "|    |    └─LayerScale: 3-7                   591,360\n",
       "|    |    └─LayerScale: 3-8                   1,182,720\n",
       "|    └─SABlock: 2-6                           --\n",
       "|    |    └─LayerScale: 3-9                   591,360\n",
       "|    |    └─LayerScale: 3-10                  1,182,720\n",
       "|    └─SABlock: 2-7                           --\n",
       "|    |    └─LayerScale: 3-11                  591,360\n",
       "|    |    └─LayerScale: 3-12                  1,182,720\n",
       "|    └─SABlock: 2-8                           --\n",
       "|    |    └─LayerScale: 3-13                  591,360\n",
       "|    |    └─LayerScale: 3-14                  1,182,720\n",
       "|    └─SABlock: 2-9                           --\n",
       "|    |    └─LayerScale: 3-15                  591,360\n",
       "|    |    └─LayerScale: 3-16                  1,182,720\n",
       "|    └─SABlock: 2-10                          --\n",
       "|    |    └─LayerScale: 3-17                  591,360\n",
       "|    |    └─LayerScale: 3-18                  1,182,720\n",
       "|    └─SABlock: 2-11                          --\n",
       "|    |    └─LayerScale: 3-19                  591,360\n",
       "|    |    └─LayerScale: 3-20                  1,182,720\n",
       "|    └─SABlock: 2-12                          --\n",
       "|    |    └─LayerScale: 3-21                  591,360\n",
       "|    |    └─LayerScale: 3-22                  1,182,720\n",
       "|    └─SABlock: 2-13                          --\n",
       "|    |    └─LayerScale: 3-23                  591,360\n",
       "|    |    └─LayerScale: 3-24                  1,182,720\n",
       "|    └─SABlock: 2-14                          --\n",
       "|    |    └─LayerScale: 3-25                  591,360\n",
       "|    |    └─LayerScale: 3-26                  1,182,720\n",
       "|    └─SABlock: 2-15                          --\n",
       "|    |    └─LayerScale: 3-27                  591,360\n",
       "|    |    └─LayerScale: 3-28                  1,182,720\n",
       "|    └─SABlock: 2-16                          --\n",
       "|    |    └─LayerScale: 3-29                  591,360\n",
       "|    |    └─LayerScale: 3-30                  1,182,720\n",
       "|    └─SABlock: 2-17                          --\n",
       "|    |    └─LayerScale: 3-31                  591,360\n",
       "|    |    └─LayerScale: 3-32                  1,182,720\n",
       "|    └─SABlock: 2-18                          --\n",
       "|    |    └─LayerScale: 3-33                  591,360\n",
       "|    |    └─LayerScale: 3-34                  1,182,720\n",
       "|    └─SABlock: 2-19                          --\n",
       "|    |    └─LayerScale: 3-35                  591,360\n",
       "|    |    └─LayerScale: 3-36                  1,182,720\n",
       "|    └─SABlock: 2-20                          --\n",
       "|    |    └─LayerScale: 3-37                  591,360\n",
       "|    |    └─LayerScale: 3-38                  1,182,720\n",
       "|    └─SABlock: 2-21                          --\n",
       "|    |    └─LayerScale: 3-39                  591,360\n",
       "|    |    └─LayerScale: 3-40                  1,182,720\n",
       "|    └─SABlock: 2-22                          --\n",
       "|    |    └─LayerScale: 3-41                  591,360\n",
       "|    |    └─LayerScale: 3-42                  1,182,720\n",
       "|    └─SABlock: 2-23                          --\n",
       "|    |    └─LayerScale: 3-43                  591,360\n",
       "|    |    └─LayerScale: 3-44                  1,182,720\n",
       "|    └─SABlock: 2-24                          --\n",
       "|    |    └─LayerScale: 3-45                  591,360\n",
       "|    |    └─LayerScale: 3-46                  1,182,720\n",
       "|    └─SABlock: 2-25                          --\n",
       "|    |    └─LayerScale: 3-47                  591,360\n",
       "|    |    └─LayerScale: 3-48                  1,182,720\n",
       "|    └─SABlock: 2-26                          --\n",
       "|    |    └─LayerScale: 3-49                  591,360\n",
       "|    |    └─LayerScale: 3-50                  1,182,720\n",
       "|    └─SABlock: 2-27                          --\n",
       "|    |    └─LayerScale: 3-51                  591,360\n",
       "|    |    └─LayerScale: 3-52                  1,182,720\n",
       "|    └─SABlock: 2-28                          --\n",
       "|    |    └─LayerScale: 3-53                  591,360\n",
       "|    |    └─LayerScale: 3-54                  1,182,720\n",
       "|    └─SABlock: 2-29                          --\n",
       "|    |    └─LayerScale: 3-55                  591,360\n",
       "|    |    └─LayerScale: 3-56                  1,182,720\n",
       "|    └─SABlock: 2-30                          --\n",
       "|    |    └─LayerScale: 3-57                  591,360\n",
       "|    |    └─LayerScale: 3-58                  1,182,720\n",
       "|    └─SABlock: 2-31                          --\n",
       "|    |    └─LayerScale: 3-59                  591,360\n",
       "|    |    └─LayerScale: 3-60                  1,182,720\n",
       "|    └─SABlock: 2-32                          --\n",
       "|    |    └─LayerScale: 3-61                  591,360\n",
       "|    |    └─LayerScale: 3-62                  1,182,720\n",
       "|    └─SABlock: 2-33                          --\n",
       "|    |    └─LayerScale: 3-63                  591,360\n",
       "|    |    └─LayerScale: 3-64                  1,182,720\n",
       "|    └─SABlock: 2-34                          --\n",
       "|    |    └─LayerScale: 3-65                  591,360\n",
       "|    |    └─LayerScale: 3-66                  1,182,720\n",
       "|    └─SABlock: 2-35                          --\n",
       "|    |    └─LayerScale: 3-67                  591,360\n",
       "|    |    └─LayerScale: 3-68                  1,182,720\n",
       "|    └─SABlock: 2-36                          --\n",
       "|    |    └─LayerScale: 3-69                  591,360\n",
       "|    |    └─LayerScale: 3-70                  1,182,720\n",
       "|    └─SABlock: 2-37                          --\n",
       "|    |    └─LayerScale: 3-71                  591,360\n",
       "|    |    └─LayerScale: 3-72                  1,182,720\n",
       "├─ModuleList: 1-3                             --\n",
       "|    └─CABlock: 2-38                          --\n",
       "|    |    └─LayerScale: 3-73                  591,360\n",
       "|    |    └─LayerScale: 3-74                  1,182,720\n",
       "|    └─CABlock: 2-39                          --\n",
       "|    |    └─LayerScale: 3-75                  591,360\n",
       "|    |    └─LayerScale: 3-76                  1,182,720\n",
       "├─Linear: 1-4                                 3,850\n",
       "├─LayerNorm: 1-5                              20\n",
       "======================================================================\n",
       "Total params: 67,714,206\n",
       "Trainable params: 67,714,206\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CaiT variants -- CaiT-S36\n",
    "n_class = 10\n",
    "d_model = 384\n",
    "n_head = 8\n",
    "mlp_hidden = 4 * d_model\n",
    "n_sa = 36\n",
    "n_ca = 2\n",
    "\n",
    "cait_s36 = CaiT(img_size=224, patch_size=16, img_c=3, num_class=n_class,\n",
    "                d_model=d_model, head=n_head, mlp_hidden=mlp_hidden, n_sa=n_sa, n_ca=n_ca)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = cait_s36(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(cait_s36, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─PatchEmbed: 1-1                             --\n",
      "|    └─Conv2d: 2-1                            590,592\n",
      "├─Sequential: 1-2                             --\n",
      "|    └─SABlock: 2-2                           --\n",
      "|    |    └─LayerScale: 3-1                   2,362,368\n",
      "|    |    └─LayerScale: 3-2                   4,724,736\n",
      "|    └─SABlock: 2-3                           --\n",
      "|    |    └─LayerScale: 3-3                   2,362,368\n",
      "|    |    └─LayerScale: 3-4                   4,724,736\n",
      "|    └─SABlock: 2-4                           --\n",
      "|    |    └─LayerScale: 3-5                   2,362,368\n",
      "|    |    └─LayerScale: 3-6                   4,724,736\n",
      "|    └─SABlock: 2-5                           --\n",
      "|    |    └─LayerScale: 3-7                   2,362,368\n",
      "|    |    └─LayerScale: 3-8                   4,724,736\n",
      "|    └─SABlock: 2-6                           --\n",
      "|    |    └─LayerScale: 3-9                   2,362,368\n",
      "|    |    └─LayerScale: 3-10                  4,724,736\n",
      "|    └─SABlock: 2-7                           --\n",
      "|    |    └─LayerScale: 3-11                  2,362,368\n",
      "|    |    └─LayerScale: 3-12                  4,724,736\n",
      "|    └─SABlock: 2-8                           --\n",
      "|    |    └─LayerScale: 3-13                  2,362,368\n",
      "|    |    └─LayerScale: 3-14                  4,724,736\n",
      "|    └─SABlock: 2-9                           --\n",
      "|    |    └─LayerScale: 3-15                  2,362,368\n",
      "|    |    └─LayerScale: 3-16                  4,724,736\n",
      "|    └─SABlock: 2-10                          --\n",
      "|    |    └─LayerScale: 3-17                  2,362,368\n",
      "|    |    └─LayerScale: 3-18                  4,724,736\n",
      "|    └─SABlock: 2-11                          --\n",
      "|    |    └─LayerScale: 3-19                  2,362,368\n",
      "|    |    └─LayerScale: 3-20                  4,724,736\n",
      "|    └─SABlock: 2-12                          --\n",
      "|    |    └─LayerScale: 3-21                  2,362,368\n",
      "|    |    └─LayerScale: 3-22                  4,724,736\n",
      "|    └─SABlock: 2-13                          --\n",
      "|    |    └─LayerScale: 3-23                  2,362,368\n",
      "|    |    └─LayerScale: 3-24                  4,724,736\n",
      "|    └─SABlock: 2-14                          --\n",
      "|    |    └─LayerScale: 3-25                  2,362,368\n",
      "|    |    └─LayerScale: 3-26                  4,724,736\n",
      "|    └─SABlock: 2-15                          --\n",
      "|    |    └─LayerScale: 3-27                  2,362,368\n",
      "|    |    └─LayerScale: 3-28                  4,724,736\n",
      "|    └─SABlock: 2-16                          --\n",
      "|    |    └─LayerScale: 3-29                  2,362,368\n",
      "|    |    └─LayerScale: 3-30                  4,724,736\n",
      "|    └─SABlock: 2-17                          --\n",
      "|    |    └─LayerScale: 3-31                  2,362,368\n",
      "|    |    └─LayerScale: 3-32                  4,724,736\n",
      "|    └─SABlock: 2-18                          --\n",
      "|    |    └─LayerScale: 3-33                  2,362,368\n",
      "|    |    └─LayerScale: 3-34                  4,724,736\n",
      "|    └─SABlock: 2-19                          --\n",
      "|    |    └─LayerScale: 3-35                  2,362,368\n",
      "|    |    └─LayerScale: 3-36                  4,724,736\n",
      "|    └─SABlock: 2-20                          --\n",
      "|    |    └─LayerScale: 3-37                  2,362,368\n",
      "|    |    └─LayerScale: 3-38                  4,724,736\n",
      "|    └─SABlock: 2-21                          --\n",
      "|    |    └─LayerScale: 3-39                  2,362,368\n",
      "|    |    └─LayerScale: 3-40                  4,724,736\n",
      "|    └─SABlock: 2-22                          --\n",
      "|    |    └─LayerScale: 3-41                  2,362,368\n",
      "|    |    └─LayerScale: 3-42                  4,724,736\n",
      "|    └─SABlock: 2-23                          --\n",
      "|    |    └─LayerScale: 3-43                  2,362,368\n",
      "|    |    └─LayerScale: 3-44                  4,724,736\n",
      "|    └─SABlock: 2-24                          --\n",
      "|    |    └─LayerScale: 3-45                  2,362,368\n",
      "|    |    └─LayerScale: 3-46                  4,724,736\n",
      "|    └─SABlock: 2-25                          --\n",
      "|    |    └─LayerScale: 3-47                  2,362,368\n",
      "|    |    └─LayerScale: 3-48                  4,724,736\n",
      "|    └─SABlock: 2-26                          --\n",
      "|    |    └─LayerScale: 3-49                  2,362,368\n",
      "|    |    └─LayerScale: 3-50                  4,724,736\n",
      "|    └─SABlock: 2-27                          --\n",
      "|    |    └─LayerScale: 3-51                  2,362,368\n",
      "|    |    └─LayerScale: 3-52                  4,724,736\n",
      "|    └─SABlock: 2-28                          --\n",
      "|    |    └─LayerScale: 3-53                  2,362,368\n",
      "|    |    └─LayerScale: 3-54                  4,724,736\n",
      "|    └─SABlock: 2-29                          --\n",
      "|    |    └─LayerScale: 3-55                  2,362,368\n",
      "|    |    └─LayerScale: 3-56                  4,724,736\n",
      "|    └─SABlock: 2-30                          --\n",
      "|    |    └─LayerScale: 3-57                  2,362,368\n",
      "|    |    └─LayerScale: 3-58                  4,724,736\n",
      "|    └─SABlock: 2-31                          --\n",
      "|    |    └─LayerScale: 3-59                  2,362,368\n",
      "|    |    └─LayerScale: 3-60                  4,724,736\n",
      "|    └─SABlock: 2-32                          --\n",
      "|    |    └─LayerScale: 3-61                  2,362,368\n",
      "|    |    └─LayerScale: 3-62                  4,724,736\n",
      "|    └─SABlock: 2-33                          --\n",
      "|    |    └─LayerScale: 3-63                  2,362,368\n",
      "|    |    └─LayerScale: 3-64                  4,724,736\n",
      "|    └─SABlock: 2-34                          --\n",
      "|    |    └─LayerScale: 3-65                  2,362,368\n",
      "|    |    └─LayerScale: 3-66                  4,724,736\n",
      "|    └─SABlock: 2-35                          --\n",
      "|    |    └─LayerScale: 3-67                  2,362,368\n",
      "|    |    └─LayerScale: 3-68                  4,724,736\n",
      "|    └─SABlock: 2-36                          --\n",
      "|    |    └─LayerScale: 3-69                  2,362,368\n",
      "|    |    └─LayerScale: 3-70                  4,724,736\n",
      "|    └─SABlock: 2-37                          --\n",
      "|    |    └─LayerScale: 3-71                  2,362,368\n",
      "|    |    └─LayerScale: 3-72                  4,724,736\n",
      "├─ModuleList: 1-3                             --\n",
      "|    └─CABlock: 2-38                          --\n",
      "|    |    └─LayerScale: 3-73                  2,362,368\n",
      "|    |    └─LayerScale: 3-74                  4,724,736\n",
      "|    └─CABlock: 2-39                          --\n",
      "|    |    └─LayerScale: 3-75                  2,362,368\n",
      "|    |    └─LayerScale: 3-76                  4,724,736\n",
      "├─Linear: 1-4                                 7,690\n",
      "├─LayerNorm: 1-5                              20\n",
      "======================================================================\n",
      "Total params: 269,908,254\n",
      "Trainable params: 269,908,254\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─PatchEmbed: 1-1                             --\n",
       "|    └─Conv2d: 2-1                            590,592\n",
       "├─Sequential: 1-2                             --\n",
       "|    └─SABlock: 2-2                           --\n",
       "|    |    └─LayerScale: 3-1                   2,362,368\n",
       "|    |    └─LayerScale: 3-2                   4,724,736\n",
       "|    └─SABlock: 2-3                           --\n",
       "|    |    └─LayerScale: 3-3                   2,362,368\n",
       "|    |    └─LayerScale: 3-4                   4,724,736\n",
       "|    └─SABlock: 2-4                           --\n",
       "|    |    └─LayerScale: 3-5                   2,362,368\n",
       "|    |    └─LayerScale: 3-6                   4,724,736\n",
       "|    └─SABlock: 2-5                           --\n",
       "|    |    └─LayerScale: 3-7                   2,362,368\n",
       "|    |    └─LayerScale: 3-8                   4,724,736\n",
       "|    └─SABlock: 2-6                           --\n",
       "|    |    └─LayerScale: 3-9                   2,362,368\n",
       "|    |    └─LayerScale: 3-10                  4,724,736\n",
       "|    └─SABlock: 2-7                           --\n",
       "|    |    └─LayerScale: 3-11                  2,362,368\n",
       "|    |    └─LayerScale: 3-12                  4,724,736\n",
       "|    └─SABlock: 2-8                           --\n",
       "|    |    └─LayerScale: 3-13                  2,362,368\n",
       "|    |    └─LayerScale: 3-14                  4,724,736\n",
       "|    └─SABlock: 2-9                           --\n",
       "|    |    └─LayerScale: 3-15                  2,362,368\n",
       "|    |    └─LayerScale: 3-16                  4,724,736\n",
       "|    └─SABlock: 2-10                          --\n",
       "|    |    └─LayerScale: 3-17                  2,362,368\n",
       "|    |    └─LayerScale: 3-18                  4,724,736\n",
       "|    └─SABlock: 2-11                          --\n",
       "|    |    └─LayerScale: 3-19                  2,362,368\n",
       "|    |    └─LayerScale: 3-20                  4,724,736\n",
       "|    └─SABlock: 2-12                          --\n",
       "|    |    └─LayerScale: 3-21                  2,362,368\n",
       "|    |    └─LayerScale: 3-22                  4,724,736\n",
       "|    └─SABlock: 2-13                          --\n",
       "|    |    └─LayerScale: 3-23                  2,362,368\n",
       "|    |    └─LayerScale: 3-24                  4,724,736\n",
       "|    └─SABlock: 2-14                          --\n",
       "|    |    └─LayerScale: 3-25                  2,362,368\n",
       "|    |    └─LayerScale: 3-26                  4,724,736\n",
       "|    └─SABlock: 2-15                          --\n",
       "|    |    └─LayerScale: 3-27                  2,362,368\n",
       "|    |    └─LayerScale: 3-28                  4,724,736\n",
       "|    └─SABlock: 2-16                          --\n",
       "|    |    └─LayerScale: 3-29                  2,362,368\n",
       "|    |    └─LayerScale: 3-30                  4,724,736\n",
       "|    └─SABlock: 2-17                          --\n",
       "|    |    └─LayerScale: 3-31                  2,362,368\n",
       "|    |    └─LayerScale: 3-32                  4,724,736\n",
       "|    └─SABlock: 2-18                          --\n",
       "|    |    └─LayerScale: 3-33                  2,362,368\n",
       "|    |    └─LayerScale: 3-34                  4,724,736\n",
       "|    └─SABlock: 2-19                          --\n",
       "|    |    └─LayerScale: 3-35                  2,362,368\n",
       "|    |    └─LayerScale: 3-36                  4,724,736\n",
       "|    └─SABlock: 2-20                          --\n",
       "|    |    └─LayerScale: 3-37                  2,362,368\n",
       "|    |    └─LayerScale: 3-38                  4,724,736\n",
       "|    └─SABlock: 2-21                          --\n",
       "|    |    └─LayerScale: 3-39                  2,362,368\n",
       "|    |    └─LayerScale: 3-40                  4,724,736\n",
       "|    └─SABlock: 2-22                          --\n",
       "|    |    └─LayerScale: 3-41                  2,362,368\n",
       "|    |    └─LayerScale: 3-42                  4,724,736\n",
       "|    └─SABlock: 2-23                          --\n",
       "|    |    └─LayerScale: 3-43                  2,362,368\n",
       "|    |    └─LayerScale: 3-44                  4,724,736\n",
       "|    └─SABlock: 2-24                          --\n",
       "|    |    └─LayerScale: 3-45                  2,362,368\n",
       "|    |    └─LayerScale: 3-46                  4,724,736\n",
       "|    └─SABlock: 2-25                          --\n",
       "|    |    └─LayerScale: 3-47                  2,362,368\n",
       "|    |    └─LayerScale: 3-48                  4,724,736\n",
       "|    └─SABlock: 2-26                          --\n",
       "|    |    └─LayerScale: 3-49                  2,362,368\n",
       "|    |    └─LayerScale: 3-50                  4,724,736\n",
       "|    └─SABlock: 2-27                          --\n",
       "|    |    └─LayerScale: 3-51                  2,362,368\n",
       "|    |    └─LayerScale: 3-52                  4,724,736\n",
       "|    └─SABlock: 2-28                          --\n",
       "|    |    └─LayerScale: 3-53                  2,362,368\n",
       "|    |    └─LayerScale: 3-54                  4,724,736\n",
       "|    └─SABlock: 2-29                          --\n",
       "|    |    └─LayerScale: 3-55                  2,362,368\n",
       "|    |    └─LayerScale: 3-56                  4,724,736\n",
       "|    └─SABlock: 2-30                          --\n",
       "|    |    └─LayerScale: 3-57                  2,362,368\n",
       "|    |    └─LayerScale: 3-58                  4,724,736\n",
       "|    └─SABlock: 2-31                          --\n",
       "|    |    └─LayerScale: 3-59                  2,362,368\n",
       "|    |    └─LayerScale: 3-60                  4,724,736\n",
       "|    └─SABlock: 2-32                          --\n",
       "|    |    └─LayerScale: 3-61                  2,362,368\n",
       "|    |    └─LayerScale: 3-62                  4,724,736\n",
       "|    └─SABlock: 2-33                          --\n",
       "|    |    └─LayerScale: 3-63                  2,362,368\n",
       "|    |    └─LayerScale: 3-64                  4,724,736\n",
       "|    └─SABlock: 2-34                          --\n",
       "|    |    └─LayerScale: 3-65                  2,362,368\n",
       "|    |    └─LayerScale: 3-66                  4,724,736\n",
       "|    └─SABlock: 2-35                          --\n",
       "|    |    └─LayerScale: 3-67                  2,362,368\n",
       "|    |    └─LayerScale: 3-68                  4,724,736\n",
       "|    └─SABlock: 2-36                          --\n",
       "|    |    └─LayerScale: 3-69                  2,362,368\n",
       "|    |    └─LayerScale: 3-70                  4,724,736\n",
       "|    └─SABlock: 2-37                          --\n",
       "|    |    └─LayerScale: 3-71                  2,362,368\n",
       "|    |    └─LayerScale: 3-72                  4,724,736\n",
       "├─ModuleList: 1-3                             --\n",
       "|    └─CABlock: 2-38                          --\n",
       "|    |    └─LayerScale: 3-73                  2,362,368\n",
       "|    |    └─LayerScale: 3-74                  4,724,736\n",
       "|    └─CABlock: 2-39                          --\n",
       "|    |    └─LayerScale: 3-75                  2,362,368\n",
       "|    |    └─LayerScale: 3-76                  4,724,736\n",
       "├─Linear: 1-4                                 7,690\n",
       "├─LayerNorm: 1-5                              20\n",
       "======================================================================\n",
       "Total params: 269,908,254\n",
       "Trainable params: 269,908,254\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CaiT variants -- CaiT-M36\n",
    "n_class = 10\n",
    "d_model = 768\n",
    "n_head = 16 \n",
    "mlp_hidden = 4 * d_model\n",
    "n_sa = 36\n",
    "n_ca = 2\n",
    "\n",
    "cait_m36 = CaiT(img_size=224, patch_size=16, img_c=3, num_class=n_class,\n",
    "                d_model=d_model, head=n_head, mlp_hidden=mlp_hidden, n_sa=n_sa, n_ca=n_ca)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = cait_m36(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(cait_m36, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─PatchEmbed: 1-1                             --\n",
      "|    └─Conv2d: 2-1                            590,592\n",
      "├─Sequential: 1-2                             --\n",
      "|    └─SABlock: 2-2                           --\n",
      "|    |    └─LayerScale: 3-1                   2,362,368\n",
      "|    |    └─LayerScale: 3-2                   4,724,736\n",
      "|    └─SABlock: 2-3                           --\n",
      "|    |    └─LayerScale: 3-3                   2,362,368\n",
      "|    |    └─LayerScale: 3-4                   4,724,736\n",
      "|    └─SABlock: 2-4                           --\n",
      "|    |    └─LayerScale: 3-5                   2,362,368\n",
      "|    |    └─LayerScale: 3-6                   4,724,736\n",
      "|    └─SABlock: 2-5                           --\n",
      "|    |    └─LayerScale: 3-7                   2,362,368\n",
      "|    |    └─LayerScale: 3-8                   4,724,736\n",
      "|    └─SABlock: 2-6                           --\n",
      "|    |    └─LayerScale: 3-9                   2,362,368\n",
      "|    |    └─LayerScale: 3-10                  4,724,736\n",
      "|    └─SABlock: 2-7                           --\n",
      "|    |    └─LayerScale: 3-11                  2,362,368\n",
      "|    |    └─LayerScale: 3-12                  4,724,736\n",
      "|    └─SABlock: 2-8                           --\n",
      "|    |    └─LayerScale: 3-13                  2,362,368\n",
      "|    |    └─LayerScale: 3-14                  4,724,736\n",
      "|    └─SABlock: 2-9                           --\n",
      "|    |    └─LayerScale: 3-15                  2,362,368\n",
      "|    |    └─LayerScale: 3-16                  4,724,736\n",
      "|    └─SABlock: 2-10                          --\n",
      "|    |    └─LayerScale: 3-17                  2,362,368\n",
      "|    |    └─LayerScale: 3-18                  4,724,736\n",
      "|    └─SABlock: 2-11                          --\n",
      "|    |    └─LayerScale: 3-19                  2,362,368\n",
      "|    |    └─LayerScale: 3-20                  4,724,736\n",
      "|    └─SABlock: 2-12                          --\n",
      "|    |    └─LayerScale: 3-21                  2,362,368\n",
      "|    |    └─LayerScale: 3-22                  4,724,736\n",
      "|    └─SABlock: 2-13                          --\n",
      "|    |    └─LayerScale: 3-23                  2,362,368\n",
      "|    |    └─LayerScale: 3-24                  4,724,736\n",
      "|    └─SABlock: 2-14                          --\n",
      "|    |    └─LayerScale: 3-25                  2,362,368\n",
      "|    |    └─LayerScale: 3-26                  4,724,736\n",
      "|    └─SABlock: 2-15                          --\n",
      "|    |    └─LayerScale: 3-27                  2,362,368\n",
      "|    |    └─LayerScale: 3-28                  4,724,736\n",
      "|    └─SABlock: 2-16                          --\n",
      "|    |    └─LayerScale: 3-29                  2,362,368\n",
      "|    |    └─LayerScale: 3-30                  4,724,736\n",
      "|    └─SABlock: 2-17                          --\n",
      "|    |    └─LayerScale: 3-31                  2,362,368\n",
      "|    |    └─LayerScale: 3-32                  4,724,736\n",
      "|    └─SABlock: 2-18                          --\n",
      "|    |    └─LayerScale: 3-33                  2,362,368\n",
      "|    |    └─LayerScale: 3-34                  4,724,736\n",
      "|    └─SABlock: 2-19                          --\n",
      "|    |    └─LayerScale: 3-35                  2,362,368\n",
      "|    |    └─LayerScale: 3-36                  4,724,736\n",
      "|    └─SABlock: 2-20                          --\n",
      "|    |    └─LayerScale: 3-37                  2,362,368\n",
      "|    |    └─LayerScale: 3-38                  4,724,736\n",
      "|    └─SABlock: 2-21                          --\n",
      "|    |    └─LayerScale: 3-39                  2,362,368\n",
      "|    |    └─LayerScale: 3-40                  4,724,736\n",
      "|    └─SABlock: 2-22                          --\n",
      "|    |    └─LayerScale: 3-41                  2,362,368\n",
      "|    |    └─LayerScale: 3-42                  4,724,736\n",
      "|    └─SABlock: 2-23                          --\n",
      "|    |    └─LayerScale: 3-43                  2,362,368\n",
      "|    |    └─LayerScale: 3-44                  4,724,736\n",
      "|    └─SABlock: 2-24                          --\n",
      "|    |    └─LayerScale: 3-45                  2,362,368\n",
      "|    |    └─LayerScale: 3-46                  4,724,736\n",
      "|    └─SABlock: 2-25                          --\n",
      "|    |    └─LayerScale: 3-47                  2,362,368\n",
      "|    |    └─LayerScale: 3-48                  4,724,736\n",
      "|    └─SABlock: 2-26                          --\n",
      "|    |    └─LayerScale: 3-49                  2,362,368\n",
      "|    |    └─LayerScale: 3-50                  4,724,736\n",
      "|    └─SABlock: 2-27                          --\n",
      "|    |    └─LayerScale: 3-51                  2,362,368\n",
      "|    |    └─LayerScale: 3-52                  4,724,736\n",
      "|    └─SABlock: 2-28                          --\n",
      "|    |    └─LayerScale: 3-53                  2,362,368\n",
      "|    |    └─LayerScale: 3-54                  4,724,736\n",
      "|    └─SABlock: 2-29                          --\n",
      "|    |    └─LayerScale: 3-55                  2,362,368\n",
      "|    |    └─LayerScale: 3-56                  4,724,736\n",
      "|    └─SABlock: 2-30                          --\n",
      "|    |    └─LayerScale: 3-57                  2,362,368\n",
      "|    |    └─LayerScale: 3-58                  4,724,736\n",
      "|    └─SABlock: 2-31                          --\n",
      "|    |    └─LayerScale: 3-59                  2,362,368\n",
      "|    |    └─LayerScale: 3-60                  4,724,736\n",
      "|    └─SABlock: 2-32                          --\n",
      "|    |    └─LayerScale: 3-61                  2,362,368\n",
      "|    |    └─LayerScale: 3-62                  4,724,736\n",
      "|    └─SABlock: 2-33                          --\n",
      "|    |    └─LayerScale: 3-63                  2,362,368\n",
      "|    |    └─LayerScale: 3-64                  4,724,736\n",
      "|    └─SABlock: 2-34                          --\n",
      "|    |    └─LayerScale: 3-65                  2,362,368\n",
      "|    |    └─LayerScale: 3-66                  4,724,736\n",
      "|    └─SABlock: 2-35                          --\n",
      "|    |    └─LayerScale: 3-67                  2,362,368\n",
      "|    |    └─LayerScale: 3-68                  4,724,736\n",
      "|    └─SABlock: 2-36                          --\n",
      "|    |    └─LayerScale: 3-69                  2,362,368\n",
      "|    |    └─LayerScale: 3-70                  4,724,736\n",
      "|    └─SABlock: 2-37                          --\n",
      "|    |    └─LayerScale: 3-71                  2,362,368\n",
      "|    |    └─LayerScale: 3-72                  4,724,736\n",
      "|    └─SABlock: 2-38                          --\n",
      "|    |    └─LayerScale: 3-73                  2,362,368\n",
      "|    |    └─LayerScale: 3-74                  4,724,736\n",
      "|    └─SABlock: 2-39                          --\n",
      "|    |    └─LayerScale: 3-75                  2,362,368\n",
      "|    |    └─LayerScale: 3-76                  4,724,736\n",
      "|    └─SABlock: 2-40                          --\n",
      "|    |    └─LayerScale: 3-77                  2,362,368\n",
      "|    |    └─LayerScale: 3-78                  4,724,736\n",
      "|    └─SABlock: 2-41                          --\n",
      "|    |    └─LayerScale: 3-79                  2,362,368\n",
      "|    |    └─LayerScale: 3-80                  4,724,736\n",
      "|    └─SABlock: 2-42                          --\n",
      "|    |    └─LayerScale: 3-81                  2,362,368\n",
      "|    |    └─LayerScale: 3-82                  4,724,736\n",
      "|    └─SABlock: 2-43                          --\n",
      "|    |    └─LayerScale: 3-83                  2,362,368\n",
      "|    |    └─LayerScale: 3-84                  4,724,736\n",
      "|    └─SABlock: 2-44                          --\n",
      "|    |    └─LayerScale: 3-85                  2,362,368\n",
      "|    |    └─LayerScale: 3-86                  4,724,736\n",
      "|    └─SABlock: 2-45                          --\n",
      "|    |    └─LayerScale: 3-87                  2,362,368\n",
      "|    |    └─LayerScale: 3-88                  4,724,736\n",
      "|    └─SABlock: 2-46                          --\n",
      "|    |    └─LayerScale: 3-89                  2,362,368\n",
      "|    |    └─LayerScale: 3-90                  4,724,736\n",
      "|    └─SABlock: 2-47                          --\n",
      "|    |    └─LayerScale: 3-91                  2,362,368\n",
      "|    |    └─LayerScale: 3-92                  4,724,736\n",
      "|    └─SABlock: 2-48                          --\n",
      "|    |    └─LayerScale: 3-93                  2,362,368\n",
      "|    |    └─LayerScale: 3-94                  4,724,736\n",
      "|    └─SABlock: 2-49                          --\n",
      "|    |    └─LayerScale: 3-95                  2,362,368\n",
      "|    |    └─LayerScale: 3-96                  4,724,736\n",
      "├─ModuleList: 1-3                             --\n",
      "|    └─CABlock: 2-50                          --\n",
      "|    |    └─LayerScale: 3-97                  2,362,368\n",
      "|    |    └─LayerScale: 3-98                  4,724,736\n",
      "|    └─CABlock: 2-51                          --\n",
      "|    |    └─LayerScale: 3-99                  2,362,368\n",
      "|    |    └─LayerScale: 3-100                 4,724,736\n",
      "├─Linear: 1-4                                 7,690\n",
      "├─LayerNorm: 1-5                              20\n",
      "======================================================================\n",
      "Total params: 354,953,502\n",
      "Trainable params: 354,953,502\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─PatchEmbed: 1-1                             --\n",
       "|    └─Conv2d: 2-1                            590,592\n",
       "├─Sequential: 1-2                             --\n",
       "|    └─SABlock: 2-2                           --\n",
       "|    |    └─LayerScale: 3-1                   2,362,368\n",
       "|    |    └─LayerScale: 3-2                   4,724,736\n",
       "|    └─SABlock: 2-3                           --\n",
       "|    |    └─LayerScale: 3-3                   2,362,368\n",
       "|    |    └─LayerScale: 3-4                   4,724,736\n",
       "|    └─SABlock: 2-4                           --\n",
       "|    |    └─LayerScale: 3-5                   2,362,368\n",
       "|    |    └─LayerScale: 3-6                   4,724,736\n",
       "|    └─SABlock: 2-5                           --\n",
       "|    |    └─LayerScale: 3-7                   2,362,368\n",
       "|    |    └─LayerScale: 3-8                   4,724,736\n",
       "|    └─SABlock: 2-6                           --\n",
       "|    |    └─LayerScale: 3-9                   2,362,368\n",
       "|    |    └─LayerScale: 3-10                  4,724,736\n",
       "|    └─SABlock: 2-7                           --\n",
       "|    |    └─LayerScale: 3-11                  2,362,368\n",
       "|    |    └─LayerScale: 3-12                  4,724,736\n",
       "|    └─SABlock: 2-8                           --\n",
       "|    |    └─LayerScale: 3-13                  2,362,368\n",
       "|    |    └─LayerScale: 3-14                  4,724,736\n",
       "|    └─SABlock: 2-9                           --\n",
       "|    |    └─LayerScale: 3-15                  2,362,368\n",
       "|    |    └─LayerScale: 3-16                  4,724,736\n",
       "|    └─SABlock: 2-10                          --\n",
       "|    |    └─LayerScale: 3-17                  2,362,368\n",
       "|    |    └─LayerScale: 3-18                  4,724,736\n",
       "|    └─SABlock: 2-11                          --\n",
       "|    |    └─LayerScale: 3-19                  2,362,368\n",
       "|    |    └─LayerScale: 3-20                  4,724,736\n",
       "|    └─SABlock: 2-12                          --\n",
       "|    |    └─LayerScale: 3-21                  2,362,368\n",
       "|    |    └─LayerScale: 3-22                  4,724,736\n",
       "|    └─SABlock: 2-13                          --\n",
       "|    |    └─LayerScale: 3-23                  2,362,368\n",
       "|    |    └─LayerScale: 3-24                  4,724,736\n",
       "|    └─SABlock: 2-14                          --\n",
       "|    |    └─LayerScale: 3-25                  2,362,368\n",
       "|    |    └─LayerScale: 3-26                  4,724,736\n",
       "|    └─SABlock: 2-15                          --\n",
       "|    |    └─LayerScale: 3-27                  2,362,368\n",
       "|    |    └─LayerScale: 3-28                  4,724,736\n",
       "|    └─SABlock: 2-16                          --\n",
       "|    |    └─LayerScale: 3-29                  2,362,368\n",
       "|    |    └─LayerScale: 3-30                  4,724,736\n",
       "|    └─SABlock: 2-17                          --\n",
       "|    |    └─LayerScale: 3-31                  2,362,368\n",
       "|    |    └─LayerScale: 3-32                  4,724,736\n",
       "|    └─SABlock: 2-18                          --\n",
       "|    |    └─LayerScale: 3-33                  2,362,368\n",
       "|    |    └─LayerScale: 3-34                  4,724,736\n",
       "|    └─SABlock: 2-19                          --\n",
       "|    |    └─LayerScale: 3-35                  2,362,368\n",
       "|    |    └─LayerScale: 3-36                  4,724,736\n",
       "|    └─SABlock: 2-20                          --\n",
       "|    |    └─LayerScale: 3-37                  2,362,368\n",
       "|    |    └─LayerScale: 3-38                  4,724,736\n",
       "|    └─SABlock: 2-21                          --\n",
       "|    |    └─LayerScale: 3-39                  2,362,368\n",
       "|    |    └─LayerScale: 3-40                  4,724,736\n",
       "|    └─SABlock: 2-22                          --\n",
       "|    |    └─LayerScale: 3-41                  2,362,368\n",
       "|    |    └─LayerScale: 3-42                  4,724,736\n",
       "|    └─SABlock: 2-23                          --\n",
       "|    |    └─LayerScale: 3-43                  2,362,368\n",
       "|    |    └─LayerScale: 3-44                  4,724,736\n",
       "|    └─SABlock: 2-24                          --\n",
       "|    |    └─LayerScale: 3-45                  2,362,368\n",
       "|    |    └─LayerScale: 3-46                  4,724,736\n",
       "|    └─SABlock: 2-25                          --\n",
       "|    |    └─LayerScale: 3-47                  2,362,368\n",
       "|    |    └─LayerScale: 3-48                  4,724,736\n",
       "|    └─SABlock: 2-26                          --\n",
       "|    |    └─LayerScale: 3-49                  2,362,368\n",
       "|    |    └─LayerScale: 3-50                  4,724,736\n",
       "|    └─SABlock: 2-27                          --\n",
       "|    |    └─LayerScale: 3-51                  2,362,368\n",
       "|    |    └─LayerScale: 3-52                  4,724,736\n",
       "|    └─SABlock: 2-28                          --\n",
       "|    |    └─LayerScale: 3-53                  2,362,368\n",
       "|    |    └─LayerScale: 3-54                  4,724,736\n",
       "|    └─SABlock: 2-29                          --\n",
       "|    |    └─LayerScale: 3-55                  2,362,368\n",
       "|    |    └─LayerScale: 3-56                  4,724,736\n",
       "|    └─SABlock: 2-30                          --\n",
       "|    |    └─LayerScale: 3-57                  2,362,368\n",
       "|    |    └─LayerScale: 3-58                  4,724,736\n",
       "|    └─SABlock: 2-31                          --\n",
       "|    |    └─LayerScale: 3-59                  2,362,368\n",
       "|    |    └─LayerScale: 3-60                  4,724,736\n",
       "|    └─SABlock: 2-32                          --\n",
       "|    |    └─LayerScale: 3-61                  2,362,368\n",
       "|    |    └─LayerScale: 3-62                  4,724,736\n",
       "|    └─SABlock: 2-33                          --\n",
       "|    |    └─LayerScale: 3-63                  2,362,368\n",
       "|    |    └─LayerScale: 3-64                  4,724,736\n",
       "|    └─SABlock: 2-34                          --\n",
       "|    |    └─LayerScale: 3-65                  2,362,368\n",
       "|    |    └─LayerScale: 3-66                  4,724,736\n",
       "|    └─SABlock: 2-35                          --\n",
       "|    |    └─LayerScale: 3-67                  2,362,368\n",
       "|    |    └─LayerScale: 3-68                  4,724,736\n",
       "|    └─SABlock: 2-36                          --\n",
       "|    |    └─LayerScale: 3-69                  2,362,368\n",
       "|    |    └─LayerScale: 3-70                  4,724,736\n",
       "|    └─SABlock: 2-37                          --\n",
       "|    |    └─LayerScale: 3-71                  2,362,368\n",
       "|    |    └─LayerScale: 3-72                  4,724,736\n",
       "|    └─SABlock: 2-38                          --\n",
       "|    |    └─LayerScale: 3-73                  2,362,368\n",
       "|    |    └─LayerScale: 3-74                  4,724,736\n",
       "|    └─SABlock: 2-39                          --\n",
       "|    |    └─LayerScale: 3-75                  2,362,368\n",
       "|    |    └─LayerScale: 3-76                  4,724,736\n",
       "|    └─SABlock: 2-40                          --\n",
       "|    |    └─LayerScale: 3-77                  2,362,368\n",
       "|    |    └─LayerScale: 3-78                  4,724,736\n",
       "|    └─SABlock: 2-41                          --\n",
       "|    |    └─LayerScale: 3-79                  2,362,368\n",
       "|    |    └─LayerScale: 3-80                  4,724,736\n",
       "|    └─SABlock: 2-42                          --\n",
       "|    |    └─LayerScale: 3-81                  2,362,368\n",
       "|    |    └─LayerScale: 3-82                  4,724,736\n",
       "|    └─SABlock: 2-43                          --\n",
       "|    |    └─LayerScale: 3-83                  2,362,368\n",
       "|    |    └─LayerScale: 3-84                  4,724,736\n",
       "|    └─SABlock: 2-44                          --\n",
       "|    |    └─LayerScale: 3-85                  2,362,368\n",
       "|    |    └─LayerScale: 3-86                  4,724,736\n",
       "|    └─SABlock: 2-45                          --\n",
       "|    |    └─LayerScale: 3-87                  2,362,368\n",
       "|    |    └─LayerScale: 3-88                  4,724,736\n",
       "|    └─SABlock: 2-46                          --\n",
       "|    |    └─LayerScale: 3-89                  2,362,368\n",
       "|    |    └─LayerScale: 3-90                  4,724,736\n",
       "|    └─SABlock: 2-47                          --\n",
       "|    |    └─LayerScale: 3-91                  2,362,368\n",
       "|    |    └─LayerScale: 3-92                  4,724,736\n",
       "|    └─SABlock: 2-48                          --\n",
       "|    |    └─LayerScale: 3-93                  2,362,368\n",
       "|    |    └─LayerScale: 3-94                  4,724,736\n",
       "|    └─SABlock: 2-49                          --\n",
       "|    |    └─LayerScale: 3-95                  2,362,368\n",
       "|    |    └─LayerScale: 3-96                  4,724,736\n",
       "├─ModuleList: 1-3                             --\n",
       "|    └─CABlock: 2-50                          --\n",
       "|    |    └─LayerScale: 3-97                  2,362,368\n",
       "|    |    └─LayerScale: 3-98                  4,724,736\n",
       "|    └─CABlock: 2-51                          --\n",
       "|    |    └─LayerScale: 3-99                  2,362,368\n",
       "|    |    └─LayerScale: 3-100                 4,724,736\n",
       "├─Linear: 1-4                                 7,690\n",
       "├─LayerNorm: 1-5                              20\n",
       "======================================================================\n",
       "Total params: 354,953,502\n",
       "Trainable params: 354,953,502\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CaiT variants -- CaiT-M48\n",
    "n_class = 10\n",
    "d_model = 768\n",
    "n_head = 16 \n",
    "mlp_hidden = 4 * d_model\n",
    "n_sa = 48\n",
    "n_ca = 2\n",
    "\n",
    "cait_m48 = CaiT(img_size=224, patch_size=16, img_c=3, num_class=n_class,\n",
    "                d_model=d_model, head=n_head, mlp_hidden=mlp_hidden, n_sa=n_sa, n_ca=n_ca)\n",
    "\n",
    "input_tensor = torch.ones((1, 3, 224, 224))\n",
    "\n",
    "output_tensor = cait_m48(input_tensor)\n",
    "assert output_tensor.shape == (1, n_class), output_tensor.shape\n",
    "summary(cait_m48, input_size=(3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/leff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LeFF(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim = 192, scale = 4, depth_kernel = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        scale_dim = dim*scale\n",
    "        self.up_proj = nn.Sequential(nn.Linear(dim, scale_dim),\n",
    "                                    Rearrange('b n c -> b c n'),\n",
    "                                    nn.BatchNorm1d(scale_dim),\n",
    "                                    nn.GELU(),\n",
    "                                    Rearrange('b c \"practice\" -> b c h w', h=\"practice\", w=\"practice\")\n",
    "                                    )\n",
    "        \n",
    "        self.depth_conv =  nn.Sequential(nn.Conv2d(scale_dim, scale_dim, kernel_size=depth_kernel, padding=1, groups=scale_dim, bias=False),\n",
    "                          nn.BatchNorm2d(scale_dim),\n",
    "                          nn.GELU(),\n",
    "                          Rearrange('b c h w -> b (h w) c', h=\"practice\", w=\"practice\")\n",
    "                          )\n",
    "        \n",
    "        self.down_proj = nn.Sequential(nn.Linear(scale_dim, dim),\n",
    "                                    Rearrange('b n c -> b c n'),\n",
    "                                    nn.BatchNorm1d(dim),\n",
    "                                    nn.GELU(),\n",
    "                                    Rearrange('b c n -> b n c')\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.up_proj(x)\n",
    "        x = self.depth_conv(x)\n",
    "        x = self.down_proj(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TransformerLeFF(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, scale = 4, depth_kernel = 3, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
    "                Residual(PreNorm(dim, LeFF(dim, scale, depth_kernel)))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        c = list()\n",
    "        for attn, leff in self.layers:\n",
    "            x = attn(x)\n",
    "            cls_tokens = x[:, 0]\n",
    "            c.append(cls_tokens)\n",
    "            x = leff(\"practice\")\n",
    "            x = torch.cat((cls_tokens.unsqueeze(1), x), dim=1) \n",
    "        return x, torch.stack(c).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLeFF(192, 2, 4, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.randn(12, 197, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 192])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/lca.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        q = q[:, :, -1, :].unsqueeze(2) # Only Lth element use as query\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "    \n",
    "class LCA(nn.Module):\n",
    "    # I remove Residual connection from here, in paper author didn't explicitly mentioned to use Residual connection, \n",
    "    # so I removed it, althougth with Residual connection also this code will work.\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, LCAttention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x[:, -1].unsqueeze(1)\n",
    "            \n",
    "            x = x[:, -1].unsqueeze(1) + ff(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LCA(192, 4, 64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(output[1])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 192])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Volo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/volo_attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlookAttention(nn.Module):\n",
    "    \"\"\"Custom module VOLO\n",
    "    \n",
    "    This is the simplified version referenced from, we assume that stride=1:\n",
    "    https://github.com/sail-sg/volo/blob/main/models/volo.py#L45-L100\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, head, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // head\n",
    "        self.head = head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn = nn.Linear(d_model, head * kernel_size**4, bias=False)\n",
    "        \n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=self.padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        \n",
    "        attn_map = self.attn(x).reshape(B, H * W, self.head, self.kernel_size**2, self.kernel_size**2)\n",
    "        attn_map = attn_map.permute(0, 2, 1, 3, 4) / self.head_dim**0.5\n",
    "        attn_map = F.softmax(attn_map, dim=-1) # B x head x N x k^2 x k^2\n",
    "        \n",
    "        v = self.W_v(x).permute(0, 3, 1, 2) # B x head*head_dim x H x W\n",
    "        unfolded_v = self.unfold(v) # B x head*head_dim x H x W x k^2\n",
    "        unfolded_v = unfolded_v.reshape(B, self.head, -1, self.kernel_size**2, H*W) # B x head x head_dim x k^2 x N\n",
    "        unfolded_v = unfolded_v.permute(0, 1, 4, 3, 2) # B x head x N x k^2 x head_dim\n",
    "        \n",
    "        agg_val = attn_map @ unfolded_v #  B x head x N x k^2 x head_dim\n",
    "        agg_val = agg_val.permute(0, 1, 4, 3, 2).reshape(B, C * self.kernel_size**2, H*W) # B x d_model*k^2 x N\n",
    "        folded_val = F.fold(agg_val, output_size=(H, W),\n",
    "                            kernel_size=self.kernel_size, padding=self.padding) # B x d_model x H x W\n",
    "        folded_val = folded_val.permute(0, 2, 3, 1)\n",
    "        \n",
    "        prj_val = self.proj(folded_val)\n",
    "        \n",
    "        return prj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "module = OutlookAttention(d_model=256, head=8, kernel_size=3)\n",
    "input_tensor = torch.ones((1, 16, 16, 256))\n",
    "\n",
    "output_tensor = module(input_tensor)\n",
    "assert output_tensor.shape == (1, 16, 16, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outlooker(nn.Module):\n",
    "    \"\"\"Basic block of VOLO that utilize at the beginning of the network\"\"\"\n",
    "    def __init__(self, d_model, d_mlp_hidden, head, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm_attn = nn.LayerNorm(d_model)\n",
    "        self.outlook_attn = OutlookAttention(d_model, head, kernel_size)\n",
    "        \n",
    "        self.layer_norm_mlp = nn.LayerNorm(d_model)\n",
    "        self.mlp = FFN(d_model, d_mlp_hidden)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.outlook_attn(self.layer_norm_attn(x))\n",
    "        x = x + self.mlp(self.layer_norm_mlp(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
