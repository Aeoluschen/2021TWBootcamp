{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "transformer_handson.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmt0_KdcgcYQ"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cg15oTysy9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e51cbbe2-c11b-40d1-f179-6a53cfe9e8c0"
      },
      "source": [
        "\"\"\"\n",
        "Create dummy dataset\n",
        "\"\"\"\n",
        "BATCH_SIZE = 32\n",
        "NUM_DATA = BATCH_SIZE * 10\n",
        "NUM_CLASSES = 10\n",
        "DIM = 100\n",
        "LR = 1e-2\n",
        "VOCAB_SIZE = 100     # index 0 for PAD\n",
        "MIN_L = 10\n",
        "MAX_L = 50\n",
        "\n",
        "DEVICE = \"cpu\"\n",
        "if torch.cuda.is_available:\n",
        "    DEVICE = \"cuda:0\"\n",
        "\n",
        "# dummy sequence data generation\n",
        "# data format = [\n",
        "#  [5, 21, 5, 9, 47, 8, 38, 1, 2],\n",
        "#  [9, 5, 3, 1, 4, 6, 7, 24, 44, 12, 2, 1, 3],\n",
        "#  [5, 1, 6, 4],\n",
        "#  ...\n",
        "# ]\n",
        "dummy_data = [[random.randint(1, VOCAB_SIZE - 1) for _ in range(random.randint(MIN_L, MAX_L))] for _ in range(NUM_DATA)]\n",
        "\n",
        "\n",
        "# (NUM_CLASS - 1e-10) is to avoid the random variable is 10.\n",
        "dummy_label = np.random.uniform(0, NUM_CLASSES - 1e-10, size=(NUM_DATA,)).astype(int) # \n",
        "\n",
        "print(\"max length: \", max([len(d) for d in dummy_data]))\n",
        "print(\"min length: \", min([len(d) for d in dummy_data]))\n",
        "print(\"max vocab index\", max([max(d) for d in dummy_data]))\n",
        "\n",
        "print(len(dummy_data))\n",
        "print(dummy_label.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length:  50\n",
            "min length:  10\n",
            "max vocab index 99\n",
            "320\n",
            "(320,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4WsmeeZgcYs"
      },
      "source": [
        "\"\"\"\n",
        "Data Module\n",
        "\"\"\" \n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, transformations):\n",
        "        # data is composed of [xs, ys]\n",
        "        # where xs.shape = [size of total data, dimension]\n",
        "        # where ys.shape = [size of total data]\n",
        "\n",
        "        self.data = data\n",
        "        self.transformations = transformations\n",
        "\n",
        "        assert len(self.data) == len(self.transformations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return tuple(transformation(data_[index]) if transformation else data_[index] \n",
        "                     for data_, transformation in zip(self.data, self.transformations))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])\n",
        "\n",
        "def getData_dataloader(x, y, batchSize):\n",
        "    # Return a dataloader object\n",
        "\n",
        "    x_transformation = None\n",
        "\n",
        "    y_transformation = None\n",
        "\n",
        "    d = MyDataset([x, y], [x_transformation, y_transformation])\n",
        "\n",
        "    ### TODO \n",
        "    # Create a dataloader object and return it\n",
        "\n",
        "    def custom_collate_func(batch):\n",
        "        ## get sequence lengths\n",
        "\n",
        "        data_, label_ = list(zip(*batch))\n",
        "\n",
        "        lengths = torch.LongTensor([len(d) for d in data_])\n",
        "        ## padd\n",
        "        data_ = [torch.LongTensor(d) for d in data_]\n",
        "\n",
        "        data_ = torch.nn.utils.rnn.pad_sequence(data_, batch_first=True)\n",
        "        ## compute mask\n",
        "        mask = (data_ != 0)\n",
        "\n",
        "        label_ = torch.LongTensor(label_)\n",
        "\n",
        "        return data_, label_, lengths, mask\n",
        "\n",
        "    return torch.utils.data.DataLoader(d, batch_size=batchSize, shuffle=True, num_workers=2, collate_fn=custom_collate_func)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcBOOv-J0fY3"
      },
      "source": [
        "# 1. Implementation for Attention Block in Tranformer's Encoder\n",
        "\n",
        "The output shape should be torch.Size([32, L, 512])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyJhwcTGRHbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "011290d7-03db-49c7-8489-764af70cb1c1"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, q_dim, v_dim):\n",
        "        super(TransformerAttention, self).__init__()\n",
        "        # the dimension of q and k should be the same\n",
        "        self.q_dim = q_dim\n",
        "        self.k_dim = q_dim \n",
        "        self.v_dim = v_dim\n",
        "\n",
        "        self.inf = 1e8 # use in masks\n",
        "\n",
        "        # TODO\n",
        "        # create the projection layers for query, key, value\n",
        "\n",
        "        self.linear = nn.Linear(v_dim, output_dim)\n",
        "\n",
        "    def forward(self, data, lengths, weight_mask):\n",
        "        # TODO 1\n",
        "        # Do the linear transformation for query, key, value\n",
        "\n",
        "        # TODO 2\n",
        "        # Compute the self-attention from query and key\n",
        "        \n",
        "        # TODO 3\n",
        "        # Get the output from weight-sum of values \n",
        "        # output = YourFunction()\n",
        "        \n",
        "        return self.linear(output)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # create the block of attention\n",
        "        self.attention = TransformerAttention(model_dim, model_dim, q_dim, v_dim)\n",
        "        \n",
        "    def forward(self, data, lengths, weight_mask):\n",
        "        output = self.attention(data, lengths, weight_mask)\n",
        "        return output\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        # Projection layer for embedding\n",
        "        self.embedding = nn.Embedding(vocab_dim, model_dim)\n",
        "\n",
        "        # create the transformer-encoding layers\n",
        "        self.transformer_layers = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.transformer_layers.append(TransformerBlock(model_dim=model_dim, q_dim=q_dim, v_dim=v_dim, num_layers=num_layers, num_heads=num_heads))\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList(self.transformer_layers)\n",
        "\n",
        "    def forward(self, data, lengths, masks):\n",
        "        # transform word to embedding\n",
        "        output = self.embedding(data)\n",
        "\n",
        "        # transform mask to weight_mask, which for masking out the weights computes in attention.\n",
        "        masks = masks.float()\n",
        "        weight_mask = masks.unsqueeze(-1).bmm(masks.unsqueeze(1))\n",
        "\n",
        "        for t_layer in self.transformer_layers:\n",
        "            output = t_layer(output, lengths, weight_mask)\n",
        "            # print(output.shape)\n",
        "\n",
        "        return output\n",
        "######## \n",
        "# test #\n",
        "########\n",
        "\n",
        "def test(train_data):\n",
        "    model = TransformerEncoder(VOCAB_SIZE, 512)\n",
        "    for batch in getData_dataloader(train_data[0], train_data[1], BATCH_SIZE):\n",
        "        # only run for one epoch\n",
        "        data, labels, lengths, masks = batch\n",
        "        print(data.shape, labels.shape, lengths.shape, masks.shape)\n",
        "        print(model(data, lengths, masks).shape)\n",
        "        break\n",
        "        \n",
        "\n",
        "test([dummy_data, dummy_label])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 48]) torch.Size([32]) torch.Size([32]) torch.Size([32, 48])\n",
            "torch.Size([32, 48, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y5ft88c1Wpp"
      },
      "source": [
        "# 2. Implementation for Multi-Head Attention Block in Tranformer's Encoder\n",
        "\n",
        "The output shape should be torch.Size([32, L, 512])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unX0gr_F0eql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "08a305a4-6819-4dcf-adea-20c712c57668"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, q_dim, v_dim, num_heads=3):\n",
        "        super(TransformerMultiHeadAttention, self).__init__()\n",
        "        # the dimension of q and k should be the same\n",
        "        \n",
        "        # TODO\n",
        "        # Create the layers for multi-head attention\n",
        "\n",
        "        self.linear = nn.Linear(num_heads * v_dim, output_dim)\n",
        "\n",
        "    def forward(self, data, lengths, weight_mask):\n",
        "        # TODO\n",
        "        # Computes the output of multi-head attention\n",
        "        # output = MultiHeadAttention()\n",
        "\n",
        "        return self.linear(output)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = TransformerMultiHeadAttention(model_dim, model_dim, q_dim, v_dim)\n",
        "        \n",
        "    def forward(self, data, lengths, weight_mask):\n",
        "        output = self.attention(data, lengths, weight_mask)\n",
        "        return output\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_dim, model_dim)\n",
        "\n",
        "        self.transformer_layers = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.transformer_layers.append(TransformerBlock(model_dim=model_dim, q_dim=q_dim, v_dim=v_dim, num_layers=num_layers, num_heads=num_heads))\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList(self.transformer_layers)\n",
        "\n",
        "    def forward(self, data, lengths, masks):\n",
        "        # transform word to embedding\n",
        "        output = self.embedding(data)\n",
        "\n",
        "        # transform mask to weight_mask, which for masking out the weights computes in attention.\n",
        "        masks = masks.float()\n",
        "        weight_mask = masks.unsqueeze(-1).bmm(masks.unsqueeze(1))\n",
        "\n",
        "        for t_layer in self.transformer_layers:\n",
        "            output = t_layer(output, lengths, weight_mask)\n",
        "            # print(output.shape)\n",
        "\n",
        "        return output\n",
        "######## \n",
        "# test #\n",
        "########\n",
        "\n",
        "def test(train_data):\n",
        "    model = TransformerEncoder(VOCAB_SIZE, 512)\n",
        "    for batch in getData_dataloader(train_data[0], train_data[1], BATCH_SIZE):\n",
        "        # only run for one epoch\n",
        "        data, labels, lengths, masks = batch\n",
        "        print(data.shape, labels.shape, lengths.shape, masks.shape)\n",
        "        print(model(data, lengths, masks).shape)\n",
        "        break\n",
        "        \n",
        "\n",
        "test([dummy_data, dummy_label])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 50]) torch.Size([32]) torch.Size([32]) torch.Size([32, 50])\n",
            "torch.Size([32, 50, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhUWx8fZ1qsg"
      },
      "source": [
        "# 3. Implementation for Feed Forward Network and Layer Normalization in Tranformer's Encoder\n",
        "\n",
        "The output shape should be torch.Size([32, L, 512])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwTM0sSN1lyb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3f260074-159a-437e-abff-6cd7658d73a5"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed forward network\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # Finish this class\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TransformerFFN, self).__init__()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class TransformerAddNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Add and layer normalization module\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # Finish this class\n",
        "    def __init__(self, input_shape):\n",
        "        super(TransformerAddNorm, self).__init__()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = TransformerMultiHeadAttention(model_dim, model_dim, q_dim, v_dim, num_heads)\n",
        "        self.ffn = TransformerFFN(model_dim, model_dim*4, model_dim)\n",
        "\n",
        "        self.addnorm_1 = TransformerAddNorm(model_dim)\n",
        "        self.addnorm_2 = TransformerAddNorm(model_dim)\n",
        "        \n",
        "    def forward(self, data, lengths, weight_mask):\n",
        "        output = self.attention(data, lengths, weight_mask)\n",
        "        output = self.addnorm_1(output)\n",
        "        output = self.ffn(output)\n",
        "        output = self.addnorm_2(output)\n",
        "        return output\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_dim, model_dim)\n",
        "\n",
        "        self.transformer_layers = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.transformer_layers.append(TransformerBlock(model_dim=model_dim, q_dim=q_dim, v_dim=v_dim, num_layers=num_layers, num_heads=num_heads))\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList(self.transformer_layers)\n",
        "\n",
        "    def forward(self, data, lengths, masks):\n",
        "        # transform word to embedding\n",
        "        output = self.embedding(data)\n",
        "\n",
        "        # transform mask to weight_mask, which for masking out the weights computes in attention.\n",
        "        masks = masks.float()\n",
        "        weight_mask = masks.unsqueeze(-1).bmm(masks.unsqueeze(1))\n",
        "\n",
        "        for t_layer in self.transformer_layers:\n",
        "            output = t_layer(output, lengths, weight_mask)\n",
        "            # print(output.shape)\n",
        "\n",
        "        return output\n",
        "######## \n",
        "# test #\n",
        "########\n",
        "\n",
        "def test(train_data):\n",
        "    model = TransformerEncoder(VOCAB_SIZE, 512)\n",
        "    for batch in getData_dataloader(train_data[0], train_data[1], BATCH_SIZE):\n",
        "        # only run for one epoch\n",
        "        data, labels, lengths, masks = batch\n",
        "        print(data.shape, labels.shape, lengths.shape, masks.shape)\n",
        "        print(model(data, lengths, masks).shape)\n",
        "        break\n",
        "        \n",
        "\n",
        "test([dummy_data, dummy_label])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 50]) torch.Size([32]) torch.Size([32]) torch.Size([32, 50])\n",
            "torch.Size([32, 50, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO-HsR30_Fbl"
      },
      "source": [
        "# 4. Positional Encoding in Transformer\n",
        "\n",
        "The output shape should be torch.Size([32, L, 512])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_q1-F9X_LPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b5596df-490f-4a9d-e1c2-596df79ddd19"
      },
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "    # TODO\n",
        "    # Finish this class\n",
        "    def __init__(self, model_dim):\n",
        "        super(PositionEncoding, self).__init__()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_dim, model_dim)\n",
        "        self.pos_embedding = PositionEncoding(model_dim)\n",
        "\n",
        "        self.transformer_layers = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.transformer_layers.append(TransformerBlock(model_dim=model_dim, q_dim=q_dim, v_dim=v_dim, num_layers=num_layers, num_heads=num_heads))\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList(self.transformer_layers)\n",
        "\n",
        "    def forward(self, data, lengths, masks):\n",
        "        # transform word to embedding\n",
        "        output = self.embedding(data)\n",
        "        output = self.pos_embedding(output)\n",
        "\n",
        "        # transform mask to weight_mask, which for masking out the weights computes in attention.\n",
        "        masks = masks.float()\n",
        "        weight_mask = masks.unsqueeze(-1).bmm(masks.unsqueeze(1))\n",
        "\n",
        "        for t_layer in self.transformer_layers:\n",
        "            output = t_layer(output, lengths, weight_mask)\n",
        "            # print(output.shape)\n",
        "\n",
        "        return output\n",
        "######## \n",
        "# test #\n",
        "########\n",
        "\n",
        "def test(train_data):\n",
        "    model = TransformerEncoder(VOCAB_SIZE, 512)\n",
        "    for batch in getData_dataloader(train_data[0], train_data[1], BATCH_SIZE):\n",
        "        # only run for one epoch\n",
        "        data, labels, lengths, masks = batch\n",
        "        print(data.shape, labels.shape, lengths.shape, masks.shape)\n",
        "        print(model(data, lengths, masks).shape)\n",
        "        break\n",
        "        \n",
        "\n",
        "test([dummy_data, dummy_label])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 46]) torch.Size([32]) torch.Size([32]) torch.Size([32, 46])\n",
            "tensor([[[  0.,   0.,   1.,  ..., 254., 255., 255.]],\n",
            "\n",
            "        [[  0.,   0.,   1.,  ..., 254., 255., 255.]],\n",
            "\n",
            "        [[  0.,   0.,   1.,  ..., 254., 255., 255.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[  0.,   0.,   1.,  ..., 254., 255., 255.]],\n",
            "\n",
            "        [[  0.,   0.,   1.,  ..., 254., 255., 255.]],\n",
            "\n",
            "        [[  0.,   0.,   1.,  ..., 254., 255., 255.]]])\n",
            "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04,\n",
            "          1.0366e-04, 1.0366e-04],\n",
            "         [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04,\n",
            "          2.0733e-04, 2.0733e-04],\n",
            "         ...,\n",
            "         [4.3000e+01, 4.3000e+01, 4.1480e+01,  ..., 4.6209e-03,\n",
            "          4.4576e-03, 4.4576e-03],\n",
            "         [4.4000e+01, 4.4000e+01, 4.2445e+01,  ..., 4.7283e-03,\n",
            "          4.5612e-03, 4.5612e-03],\n",
            "         [4.5000e+01, 4.5000e+01, 4.3410e+01,  ..., 4.8358e-03,\n",
            "          4.6649e-03, 4.6649e-03]],\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04,\n",
            "          1.0366e-04, 1.0366e-04],\n",
            "         [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04,\n",
            "          2.0733e-04, 2.0733e-04],\n",
            "         ...,\n",
            "         [4.3000e+01, 4.3000e+01, 4.1480e+01,  ..., 4.6209e-03,\n",
            "          4.4576e-03, 4.4576e-03],\n",
            "         [4.4000e+01, 4.4000e+01, 4.2445e+01,  ..., 4.7283e-03,\n",
            "          4.5612e-03, 4.5612e-03],\n",
            "         [4.5000e+01, 4.5000e+01, 4.3410e+01,  ..., 4.8358e-03,\n",
            "          4.6649e-03, 4.6649e-03]],\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04,\n",
            "          1.0366e-04, 1.0366e-04],\n",
            "         [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04,\n",
            "          2.0733e-04, 2.0733e-04],\n",
            "         ...,\n",
            "         [4.3000e+01, 4.3000e+01, 4.1480e+01,  ..., 4.6209e-03,\n",
            "          4.4576e-03, 4.4576e-03],\n",
            "         [4.4000e+01, 4.4000e+01, 4.2445e+01,  ..., 4.7283e-03,\n",
            "          4.5612e-03, 4.5612e-03],\n",
            "         [4.5000e+01, 4.5000e+01, 4.3410e+01,  ..., 4.8358e-03,\n",
            "          4.6649e-03, 4.6649e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04,\n",
            "          1.0366e-04, 1.0366e-04],\n",
            "         [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04,\n",
            "          2.0733e-04, 2.0733e-04],\n",
            "         ...,\n",
            "         [4.3000e+01, 4.3000e+01, 4.1480e+01,  ..., 4.6209e-03,\n",
            "          4.4576e-03, 4.4576e-03],\n",
            "         [4.4000e+01, 4.4000e+01, 4.2445e+01,  ..., 4.7283e-03,\n",
            "          4.5612e-03, 4.5612e-03],\n",
            "         [4.5000e+01, 4.5000e+01, 4.3410e+01,  ..., 4.8358e-03,\n",
            "          4.6649e-03, 4.6649e-03]],\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04,\n",
            "          1.0366e-04, 1.0366e-04],\n",
            "         [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04,\n",
            "          2.0733e-04, 2.0733e-04],\n",
            "         ...,\n",
            "         [4.3000e+01, 4.3000e+01, 4.1480e+01,  ..., 4.6209e-03,\n",
            "          4.4576e-03, 4.4576e-03],\n",
            "         [4.4000e+01, 4.4000e+01, 4.2445e+01,  ..., 4.7283e-03,\n",
            "          4.5612e-03, 4.5612e-03],\n",
            "         [4.5000e+01, 4.5000e+01, 4.3410e+01,  ..., 4.8358e-03,\n",
            "          4.6649e-03, 4.6649e-03]],\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04,\n",
            "          1.0366e-04, 1.0366e-04],\n",
            "         [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04,\n",
            "          2.0733e-04, 2.0733e-04],\n",
            "         ...,\n",
            "         [4.3000e+01, 4.3000e+01, 4.1480e+01,  ..., 4.6209e-03,\n",
            "          4.4576e-03, 4.4576e-03],\n",
            "         [4.4000e+01, 4.4000e+01, 4.2445e+01,  ..., 4.7283e-03,\n",
            "          4.5612e-03, 4.5612e-03],\n",
            "         [4.5000e+01, 4.5000e+01, 4.3410e+01,  ..., 4.8358e-03,\n",
            "          4.6649e-03, 4.6649e-03]]])\n",
            "torch.Size([32, 46, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEAUPhvS2N78"
      },
      "source": [
        "# 5. Test Back Propagation of Your Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH-ynawm1_GN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "3215e694-9239-4132-b68c-bbbd93abb8af"
      },
      "source": [
        "import time \n",
        "class TransformerClassification(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, num_classes=2, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerClassification, self).__init__()\n",
        "        self.encoder = TransformerEncoder(vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3)\n",
        "        self.output_layer = nn.Linear(model_dim, num_classes)\n",
        "\n",
        "    def forward(self, data, lengths, masks):\n",
        "        return self.output_layer(self.encoder(data, lengths, masks)[:, 0])\n",
        "def test(train_data):\n",
        "    model = TransformerClassification(VOCAB_SIZE, 512, num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for epoch in range(20):\n",
        "        losses = []\n",
        "        s = time.time()\n",
        "        for batch in getData_dataloader(train_data[0], train_data[1], BATCH_SIZE):\n",
        "            # only run for one epoch\n",
        "            data, labels, lengths, masks = batch\n",
        "            data, labels, lengths, masks = data.to(DEVICE), labels.to(DEVICE), lengths.to(DEVICE), masks.to(DEVICE)\n",
        "            output = model(data, lengths, masks)\n",
        "            loss = loss_function(output, labels)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "        print(time.time() - s)\n",
        "        print(f\"epoch {epoch}, loss: {np.mean(losses):.3f}\")\n",
        "        \n",
        "\n",
        "test([dummy_data, dummy_label])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5074501037597656\n",
            "epoch 0, loss: 2.746\n",
            "0.5081014633178711\n",
            "epoch 1, loss: 2.385\n",
            "0.49962329864501953\n",
            "epoch 2, loss: 2.325\n",
            "0.5067362785339355\n",
            "epoch 3, loss: 2.248\n",
            "0.5004696846008301\n",
            "epoch 4, loss: 2.114\n",
            "0.5036993026733398\n",
            "epoch 5, loss: 1.933\n",
            "0.5031535625457764\n",
            "epoch 6, loss: 1.654\n",
            "0.5001282691955566\n",
            "epoch 7, loss: 1.433\n",
            "0.5062453746795654\n",
            "epoch 8, loss: 1.113\n",
            "0.5050177574157715\n",
            "epoch 9, loss: 0.598\n",
            "0.5157895088195801\n",
            "epoch 10, loss: 0.315\n",
            "0.4961247444152832\n",
            "epoch 11, loss: 0.229\n",
            "0.4985818862915039\n",
            "epoch 12, loss: 0.197\n",
            "0.5039937496185303\n",
            "epoch 13, loss: 0.206\n",
            "0.5086092948913574\n",
            "epoch 14, loss: 0.158\n",
            "0.5034294128417969\n",
            "epoch 15, loss: 0.102\n",
            "0.506798267364502\n",
            "epoch 16, loss: 0.067\n",
            "0.5046491622924805\n",
            "epoch 17, loss: 0.054\n",
            "0.5069537162780762\n",
            "epoch 18, loss: 0.031\n",
            "0.501335620880127\n",
            "epoch 19, loss: 0.025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNobtfLFhvTX"
      },
      "source": [
        "# Transformer Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pBckTNu3wmY"
      },
      "source": [
        "# TODO\n",
        "# Finish this class\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "    def forward(self, data, max_length, encoder_outputs):\n",
        "        assert len(encoder_outputs) == self.num_layers, \"The number of encoding layers should be the same as decoding layers\"\n",
        "\n",
        "# TODO\n",
        "# Finish the class\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3, max_gen_length=50):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.max_gen_length = max_gen_length\n",
        "        self.encoder = TransformerEncoder(vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3)\n",
        "        self.decoder = TransformerDecoder(vocab_dim, model_dim=512, q_dim=64, v_dim=64, num_layers=2, num_heads=3)\n",
        "\n",
        "    def forward(self, data, lengths, masks):\n",
        "        e_outputs = self.encoder(data, lengths, masks)\n",
        "\n",
        "        return d_output\n",
        "\n",
        "\n",
        "######## \n",
        "# test #\n",
        "########\n",
        "\n",
        "def test(train_data):\n",
        "    model = Transformer(VOCAB_SIZE, 512)\n",
        "    for batch in getData_dataloader(train_data[0], train_data[1], BATCH_SIZE):\n",
        "        # only run for one epoch\n",
        "        data, labels, lengths, masks = batch\n",
        "        print(data.shape, labels.shape, lengths.shape, masks.shape)\n",
        "        print(model(data, lengths, masks))\n",
        "        break\n",
        "        \n",
        "\n",
        "test([dummy_data, dummy_label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAhlMupGEV80"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}