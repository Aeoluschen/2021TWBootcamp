{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "## Data download:\n",
    "https://drive.google.com/file/d/1zvc4_mKBpEhFWVju91KRIM0uQc6kR-S0/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note, add some commets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./IMDB_data/labeledTrain.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7c0ab06dedd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./IMDB_data/labeledTrain.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./IMDB_data/labeledTrain.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./IMDB_data/labeledTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quick experiments, we only pick 100 data\n",
    "\n",
    "train_data = data['review'].tolist()[:100]\n",
    "train_target = data['sentiment'].tolist()[:100]\n",
    "\n",
    "test_data = data['review'].tolist()[100:150]\n",
    "test_target = data['sentiment'].tolist()[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords, using NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "stopWordDict = dict(zip(sw, list(range(len(sw)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 12450.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 8356.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Checking the SW is in sentnece or not\n",
    "\n",
    "train_remove_sw = []\n",
    "for review in tqdm(train_data):\n",
    "    temp = ''\n",
    "    for word in review.split():\n",
    "        if word not in stopWordDict:\n",
    "            temp += word+' '\n",
    "    train_remove_sw.append(temp.strip())\n",
    "    \n",
    "test_remove_sw = []\n",
    "for review in tqdm(test_data):\n",
    "    temp = ''\n",
    "    for word in review.split():\n",
    "        if word not in stopWordDict:\n",
    "            temp += word+' '\n",
    "    test_remove_sw.append(temp.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 25048.10it/s]\n",
      "100it [00:00, 7161.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Building the dictionary(Vocabulary)\n",
    "token_index = {}\n",
    "\n",
    "for sample in tqdm(train_remove_sw):\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)+ 1\n",
    "\n",
    "#looking for max_length (All data time step follow this one)\n",
    "max_length = 0\n",
    "for s in train_remove_sw:\n",
    "    if len(s)>max_length:\n",
    "        max_length = len(s)\n",
    "\n",
    "train_results = np.zeros((len(train_remove_sw), max_length, max(token_index.values())+1)) #(data_size, time_step, word_dim)\n",
    "for i,smaple in tqdm(enumerate(train_remove_sw)):\n",
    "    for j,word in list(enumerate(sample.split())):\n",
    "        train_index = token_index.get(word)\n",
    "        train_results[i,j,train_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3938, 4704)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 6267.08it/s]\n"
     ]
    }
   ],
   "source": [
    "test_results = np.zeros((len(test_remove_sw), max_length, max(token_index.values())+1))\n",
    "for i,smaple in tqdm(enumerate(test_remove_sw)):\n",
    "    for j,word in list(enumerate(sample.split())):\n",
    "        test_index = token_index.get(word)\n",
    "        test_results[i,j,test_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3938, 4704)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do \"mean\" to represent the sentence\n",
    "\n",
    "X_train = train_results.mean(1)\n",
    "y_train = train_target\n",
    "\n",
    "X_test = test_results.mean(1)\n",
    "y_test = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        28\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.56        50\n",
      "   macro avg       0.28      0.50      0.36        50\n",
      "weighted avg       0.31      0.56      0.40        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Simple Linear\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "#Metric for CLS task\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sklearn to use TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(train_remove_sw)\n",
    "X_test = tfidf_vectorizer.transform(test_remove_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4550)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.96      0.73        28\n",
      "           1       0.75      0.14      0.23        22\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.67      0.55      0.48        50\n",
      "weighted avg       0.66      0.60      0.51        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 1\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bi-gram data\n",
    "\n",
    "bigram=[]\n",
    "for sentence in train_remove_sw:\n",
    "    tmp = sentence.split()\n",
    "    for i in range(len(tmp)-1):\n",
    "        bigram.append((tmp[i], tmp[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('going', 'moment')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabulary\n",
    "\n",
    "words=[]\n",
    "for s in train_remove_sw:\n",
    "    tmp = s.split()\n",
    "    for w in tmp:\n",
    "        words.append(w)\n",
    "        \n",
    "vocb = set(words)\n",
    "vocb.add('unk')\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dealing': 0,\n",
       " 'represents': 1,\n",
       " 'generator': 2,\n",
       " 'rate': 3,\n",
       " 'ending': 4,\n",
       " 'pascow': 5,\n",
       " 'hes': 6,\n",
       " 'uniquely': 7,\n",
       " 'clichés': 8,\n",
       " 'charactersmy': 9,\n",
       " 'section': 10,\n",
       " 'ham': 11,\n",
       " 'graduate': 12,\n",
       " 'townein': 13,\n",
       " '1600': 14,\n",
       " 'scenebottom': 15,\n",
       " 'hisher': 16,\n",
       " 'argues': 17,\n",
       " 'nose-dives': 18,\n",
       " 'botched': 19,\n",
       " 'hammerhead:': 20,\n",
       " 'accident': 21,\n",
       " 'familial': 22,\n",
       " 'enchanting': 23,\n",
       " 'surprising': 24,\n",
       " 'harm': 25,\n",
       " 'nikah': 26,\n",
       " 'claim': 27,\n",
       " 'subplot': 28,\n",
       " 'noted': 29,\n",
       " 'eric': 30,\n",
       " 'soul': 31,\n",
       " 'ruth': 32,\n",
       " 'incarnate': 33,\n",
       " '93%': 34,\n",
       " 'restavoid': 35,\n",
       " 'tackier': 36,\n",
       " 'unpredictable': 37,\n",
       " 'fiona': 38,\n",
       " 'evocation': 39,\n",
       " 'rent': 40,\n",
       " 'prado': 41,\n",
       " 'nudity!': 42,\n",
       " 'anxious': 43,\n",
       " 'free!': 44,\n",
       " 'yummy': 45,\n",
       " 'security': 46,\n",
       " 'go': 47,\n",
       " 'loving': 48,\n",
       " 'want': 49,\n",
       " 'cops': 50,\n",
       " 'disown': 51,\n",
       " 'choo-chooceleste': 52,\n",
       " 'tried': 53,\n",
       " 'persuasive': 54,\n",
       " 'execution': 55,\n",
       " 'thrown-together': 56,\n",
       " 'carters': 57,\n",
       " 'gigli': 58,\n",
       " 'him;': 59,\n",
       " 'rank': 60,\n",
       " 'hodet': 61,\n",
       " 'flashpoint': 62,\n",
       " 'seenthe': 63,\n",
       " 'local': 64,\n",
       " 'installments': 65,\n",
       " 'parody': 66,\n",
       " 'among': 67,\n",
       " 'equate': 68,\n",
       " 'happen': 69,\n",
       " 'middle': 70,\n",
       " 'remember': 71,\n",
       " 'reflected': 72,\n",
       " 'real-time': 73,\n",
       " 'loosely': 74,\n",
       " 'quest': 75,\n",
       " 'block': 76,\n",
       " 'simon': 77,\n",
       " 'lees': 78,\n",
       " 'ad': 79,\n",
       " 'forms': 80,\n",
       " 'persons': 81,\n",
       " 'fans': 82,\n",
       " '089$': 83,\n",
       " 'transforms': 84,\n",
       " 'insipid': 85,\n",
       " 'turner': 86,\n",
       " 'furthermore': 87,\n",
       " 'palette': 88,\n",
       " 'inspiring': 89,\n",
       " 'sun': 90,\n",
       " 'flick:': 91,\n",
       " 'pouring': 92,\n",
       " 'vigorously': 93,\n",
       " 'motherking': 94,\n",
       " 'cancelled': 95,\n",
       " 'won!': 96,\n",
       " 'ratings': 97,\n",
       " 'technicolour': 98,\n",
       " 'unmistakably': 99,\n",
       " 'farm': 100,\n",
       " 'video': 101,\n",
       " 'priest': 102,\n",
       " 'chastity': 103,\n",
       " 'matched': 104,\n",
       " 'encountered': 105,\n",
       " 'kingsley': 106,\n",
       " 'might': 107,\n",
       " 'twists': 108,\n",
       " 'fan': 109,\n",
       " 'bloody': 110,\n",
       " 'instead': 111,\n",
       " 'it`reloaded': 112,\n",
       " 'temperamental': 113,\n",
       " 'rubbish': 114,\n",
       " 'invisible': 115,\n",
       " 'epic': 116,\n",
       " 'character': 117,\n",
       " 'halmark': 118,\n",
       " 'champagne': 119,\n",
       " 'intentionally': 120,\n",
       " 'finish': 121,\n",
       " 'boring': 122,\n",
       " 'b': 123,\n",
       " 'sistine': 124,\n",
       " 'nerves': 125,\n",
       " 'review': 126,\n",
       " 'allows': 127,\n",
       " 'cameo': 128,\n",
       " 'sequences': 129,\n",
       " 'gasmann': 130,\n",
       " 'maria': 131,\n",
       " 'stein': 132,\n",
       " 'informer': 133,\n",
       " 'career': 134,\n",
       " 'summarised': 135,\n",
       " 'presentation': 136,\n",
       " 'milks': 137,\n",
       " 'feattherefore': 138,\n",
       " 'litman': 139,\n",
       " 'ettore': 140,\n",
       " 'others': 141,\n",
       " 'bulletproof': 142,\n",
       " 'daddy': 143,\n",
       " 'ties': 144,\n",
       " 'dives': 145,\n",
       " 'massachusetts': 146,\n",
       " 'silent': 147,\n",
       " 'almost': 148,\n",
       " 'amusingly': 149,\n",
       " 'arrangements': 150,\n",
       " 'solid': 151,\n",
       " 'couples': 152,\n",
       " 'club': 153,\n",
       " 'swift': 154,\n",
       " 'blackwoodcastle': 155,\n",
       " 'ideas': 156,\n",
       " 'flattered': 157,\n",
       " 'psychopathic': 158,\n",
       " 'walking': 159,\n",
       " 'smilodons': 160,\n",
       " 'trying': 161,\n",
       " 'finale': 162,\n",
       " 'thanksgiving': 163,\n",
       " 'preparing': 164,\n",
       " 'decide': 165,\n",
       " 'tie': 166,\n",
       " 'alcohol': 167,\n",
       " 'blew': 168,\n",
       " 'billion': 169,\n",
       " 'custody': 170,\n",
       " 'flashlight': 171,\n",
       " 'overall': 172,\n",
       " 'frenzy': 173,\n",
       " 'supposedly': 174,\n",
       " 'chan': 175,\n",
       " 'meets': 176,\n",
       " 'bunuel': 177,\n",
       " 'presumably': 178,\n",
       " 'commentating': 179,\n",
       " 'thornton': 180,\n",
       " 'dvd': 181,\n",
       " 'themes': 182,\n",
       " 'transfer': 183,\n",
       " 'spring': 184,\n",
       " 'meaning': 185,\n",
       " 'audrey': 186,\n",
       " 'yeah': 187,\n",
       " 'hotdog': 188,\n",
       " '!!!!!!': 189,\n",
       " 'everywhere:': 190,\n",
       " 'headed': 191,\n",
       " 'redhead': 192,\n",
       " 'esther': 193,\n",
       " 'city': 194,\n",
       " 'pants': 195,\n",
       " 'skills': 196,\n",
       " 'elderly': 197,\n",
       " 'macdonald': 198,\n",
       " 'reviewing': 199,\n",
       " 'horrors': 200,\n",
       " 'overkill': 201,\n",
       " 'envelops': 202,\n",
       " 'purpose': 203,\n",
       " 'died': 204,\n",
       " 'christian': 205,\n",
       " 'related': 206,\n",
       " 'hidden': 207,\n",
       " 'assassination': 208,\n",
       " 'pressures': 209,\n",
       " 'network': 210,\n",
       " 'picture': 211,\n",
       " 'married': 212,\n",
       " 'bartel': 213,\n",
       " 'overheard': 214,\n",
       " 'humour': 215,\n",
       " 'spaghetti': 216,\n",
       " 'twinkle': 217,\n",
       " 'cecil': 218,\n",
       " 'sensitive': 219,\n",
       " 'dangling': 220,\n",
       " 'uncanny': 221,\n",
       " 'hidesthis': 222,\n",
       " 'jared': 223,\n",
       " 'faith': 224,\n",
       " 'simmons': 225,\n",
       " 'interview': 226,\n",
       " 'n': 227,\n",
       " 'sheppard': 228,\n",
       " 'dupery': 229,\n",
       " 'previous': 230,\n",
       " 'ill-laid': 231,\n",
       " 'hideous': 232,\n",
       " 'permanent': 233,\n",
       " 'stroke': 234,\n",
       " 'second': 235,\n",
       " 'revenge': 236,\n",
       " 'canned': 237,\n",
       " 'flat': 238,\n",
       " 'absorbing': 239,\n",
       " 'armin': 240,\n",
       " 'title': 241,\n",
       " 'p*rn': 242,\n",
       " 'max': 243,\n",
       " 'rented': 244,\n",
       " 'horror': 245,\n",
       " 'rudyard': 246,\n",
       " '4510': 247,\n",
       " 'scope': 248,\n",
       " 'emotional': 249,\n",
       " 'wall': 250,\n",
       " 'create': 251,\n",
       " '=': 252,\n",
       " 'watchthe': 253,\n",
       " 'depardieunaturally': 254,\n",
       " 'implausible': 255,\n",
       " 'badtheres': 256,\n",
       " 'playing': 257,\n",
       " 'tragic': 258,\n",
       " 'holes': 259,\n",
       " 'specter': 260,\n",
       " 'b-movie': 261,\n",
       " 'hustons': 262,\n",
       " 'lifes': 263,\n",
       " 'protecting': 264,\n",
       " 'animals': 265,\n",
       " 'comprehend': 266,\n",
       " 'attracts': 267,\n",
       " 'wb': 268,\n",
       " 'unrealistic': 269,\n",
       " 'bertie': 270,\n",
       " 'inaugural': 271,\n",
       " 'reference': 272,\n",
       " 'technical': 273,\n",
       " 'kindred': 274,\n",
       " 'finds': 275,\n",
       " 'mentor': 276,\n",
       " 'thalmus': 277,\n",
       " 'faithful': 278,\n",
       " 'singers': 279,\n",
       " 'elisabeth': 280,\n",
       " 'differentokay': 281,\n",
       " 'nonstop': 282,\n",
       " 'bukowski': 283,\n",
       " '147': 284,\n",
       " 'methe': 285,\n",
       " 'immortal': 286,\n",
       " 'water': 287,\n",
       " 'pick': 288,\n",
       " 'hard-earned': 289,\n",
       " 'march': 290,\n",
       " 'stalking': 291,\n",
       " 'trash': 292,\n",
       " 'unveiled': 293,\n",
       " 'conflict': 294,\n",
       " 'losing': 295,\n",
       " 'mask': 296,\n",
       " 'beach': 297,\n",
       " 'philosophical': 298,\n",
       " 'disliked': 299,\n",
       " 'rarely': 300,\n",
       " 'spoilers!!!': 301,\n",
       " 'instance': 302,\n",
       " 'paranoia': 303,\n",
       " 'network:': 304,\n",
       " 'killer': 305,\n",
       " 'months:': 306,\n",
       " 'upall': 307,\n",
       " 'screaming': 308,\n",
       " 'skating': 309,\n",
       " 'intensity': 310,\n",
       " 'due': 311,\n",
       " 'wondered': 312,\n",
       " 'really': 313,\n",
       " 'midkiffs': 314,\n",
       " 'dream': 315,\n",
       " 'conducts': 316,\n",
       " 'demand': 317,\n",
       " 'effectsthe': 318,\n",
       " 'script': 319,\n",
       " 'allusion': 320,\n",
       " 'michael': 321,\n",
       " 'performers': 322,\n",
       " 'captain': 323,\n",
       " '`office': 324,\n",
       " 'whether': 325,\n",
       " 'wise': 326,\n",
       " 'homosexuals': 327,\n",
       " 'vegas': 328,\n",
       " 'statements': 329,\n",
       " 'arthur': 330,\n",
       " 'dat': 331,\n",
       " 'horrible': 332,\n",
       " 'ingrid': 333,\n",
       " 'billy': 334,\n",
       " 'well': 335,\n",
       " 'nicholson': 336,\n",
       " 'join': 337,\n",
       " 'star': 338,\n",
       " 'possessing': 339,\n",
       " 'resurrection': 340,\n",
       " 'pescis': 341,\n",
       " 'single': 342,\n",
       " 'lousy': 343,\n",
       " '1985': 344,\n",
       " 'referred': 345,\n",
       " 'period': 346,\n",
       " 'hour': 347,\n",
       " 'difficulties': 348,\n",
       " 'supernatural': 349,\n",
       " 'bette': 350,\n",
       " 'wang': 351,\n",
       " 'eye': 352,\n",
       " 'seeking': 353,\n",
       " 'jr': 354,\n",
       " 'enjoy': 355,\n",
       " 'uninteresting': 356,\n",
       " 'guys': 357,\n",
       " 'yes': 358,\n",
       " 'decisionstory---soap': 359,\n",
       " 'centers': 360,\n",
       " 'movie-of-the-week': 361,\n",
       " 'anthology': 362,\n",
       " 'pursuingmy': 363,\n",
       " 'stick': 364,\n",
       " 'son-of-a-witch!': 365,\n",
       " 'absolutely': 366,\n",
       " 'merchant': 367,\n",
       " 'necks': 368,\n",
       " 'translates': 369,\n",
       " 'realisticgory': 370,\n",
       " 'center': 371,\n",
       " 'socks': 372,\n",
       " 'rather': 373,\n",
       " 'sarro': 374,\n",
       " 'jackie': 375,\n",
       " 'outsider': 376,\n",
       " 'mob': 377,\n",
       " 'critics': 378,\n",
       " 'bore': 379,\n",
       " 'l': 380,\n",
       " 'outwit': 381,\n",
       " '!': 382,\n",
       " 'julie': 383,\n",
       " 'worlds': 384,\n",
       " 'humanity!': 385,\n",
       " 'giancarlo': 386,\n",
       " 'demanding': 387,\n",
       " 'alivethe': 388,\n",
       " 'questions': 389,\n",
       " 'idea': 390,\n",
       " 'ladybug´s': 391,\n",
       " 'clintons': 392,\n",
       " 'cups': 393,\n",
       " 'entertained': 394,\n",
       " 'befalls': 395,\n",
       " 'jp': 396,\n",
       " 'dead': 397,\n",
       " 'praise': 398,\n",
       " 'allow': 399,\n",
       " 'trivia': 400,\n",
       " 'andrew': 401,\n",
       " 'impatience': 402,\n",
       " 'failing': 403,\n",
       " 'minded': 404,\n",
       " 'trick': 405,\n",
       " 'childbirth': 406,\n",
       " 'veronica': 407,\n",
       " 'met': 408,\n",
       " '1986': 409,\n",
       " 'priority': 410,\n",
       " 'vital': 411,\n",
       " 'women': 412,\n",
       " 'events': 413,\n",
       " 'park¨': 414,\n",
       " 'obnoxious': 415,\n",
       " 'rugged': 416,\n",
       " 'professioni': 417,\n",
       " 'asking': 418,\n",
       " 'low': 419,\n",
       " 'itcons:': 420,\n",
       " 'middling': 421,\n",
       " 'timothy': 422,\n",
       " 'interesting': 423,\n",
       " 'prices': 424,\n",
       " 'rise': 425,\n",
       " 'emmerich': 426,\n",
       " 'many': 427,\n",
       " 'wars': 428,\n",
       " 'brand': 429,\n",
       " 'construed': 430,\n",
       " 'anyone': 431,\n",
       " 'maggie': 432,\n",
       " 'performances': 433,\n",
       " 'gore': 434,\n",
       " '-': 435,\n",
       " 'cite': 436,\n",
       " 'loman': 437,\n",
       " 'sf': 438,\n",
       " 'niece': 439,\n",
       " 'possibilities': 440,\n",
       " 'historythe': 441,\n",
       " 'edgar': 442,\n",
       " 'commercialswhile': 443,\n",
       " 'maintain': 444,\n",
       " 'bonham-carters': 445,\n",
       " 'themed': 446,\n",
       " 'wife': 447,\n",
       " 'use': 448,\n",
       " 'spoil': 449,\n",
       " 'expel': 450,\n",
       " 'one-man': 451,\n",
       " 'moral': 452,\n",
       " 'splice': 453,\n",
       " 'slightly': 454,\n",
       " 'lawyer': 455,\n",
       " 'brings': 456,\n",
       " 'possessed': 457,\n",
       " 'blessings': 458,\n",
       " 'agent': 459,\n",
       " 'satisfying': 460,\n",
       " 'tiger': 461,\n",
       " 'votes': 462,\n",
       " 'crawled': 463,\n",
       " 'people': 464,\n",
       " 'giant': 465,\n",
       " 'happy': 466,\n",
       " 'neuroses': 467,\n",
       " 'blockbuster': 468,\n",
       " 'boobs': 469,\n",
       " 'dr': 470,\n",
       " 'poem': 471,\n",
       " 'dorothy': 472,\n",
       " 'pulls': 473,\n",
       " 'since': 474,\n",
       " 'begin': 475,\n",
       " 'minority': 476,\n",
       " 'must-see': 477,\n",
       " 'geoffreys': 478,\n",
       " 'black': 479,\n",
       " 'competitive': 480,\n",
       " 'reality': 481,\n",
       " 'ottiano': 482,\n",
       " 'fast': 483,\n",
       " 'opening': 484,\n",
       " 'now-a-days': 485,\n",
       " 'number': 486,\n",
       " 'stage': 487,\n",
       " 'cured': 488,\n",
       " 'members': 489,\n",
       " 'feet': 490,\n",
       " 'international': 491,\n",
       " 'sibrels': 492,\n",
       " 'thats': 493,\n",
       " 'poor:': 494,\n",
       " 'character!': 495,\n",
       " 'outrageous': 496,\n",
       " 'appealing': 497,\n",
       " 'obvious': 498,\n",
       " 'credibility': 499,\n",
       " 'signing': 500,\n",
       " 'arrives': 501,\n",
       " 'tricky': 502,\n",
       " 'stages': 503,\n",
       " 'jeffreys': 504,\n",
       " 'decides': 505,\n",
       " 'mary': 506,\n",
       " 'completely': 507,\n",
       " 'crude': 508,\n",
       " 'treat': 509,\n",
       " 'stumbles': 510,\n",
       " 'imdb': 511,\n",
       " 'metal': 512,\n",
       " 'crotch': 513,\n",
       " 'changer': 514,\n",
       " 'asked': 515,\n",
       " '£1': 516,\n",
       " 'names': 517,\n",
       " 'encounters': 518,\n",
       " 'schoolthere': 519,\n",
       " 'street': 520,\n",
       " 'murmurs:': 521,\n",
       " 'preventing': 522,\n",
       " 'chubby': 523,\n",
       " 'scarcity': 524,\n",
       " 'fantastic': 525,\n",
       " 'hopelessly': 526,\n",
       " 'lightly': 527,\n",
       " '4th': 528,\n",
       " 'ballantine': 529,\n",
       " 'soapdish': 530,\n",
       " 'farts': 531,\n",
       " 'rocket': 532,\n",
       " 'dissimilar': 533,\n",
       " 'ridicules': 534,\n",
       " '3000': 535,\n",
       " 'cats': 536,\n",
       " 'power': 537,\n",
       " 'dirty': 538,\n",
       " 'follow-up': 539,\n",
       " 'naked': 540,\n",
       " 'diverse': 541,\n",
       " 'cigar': 542,\n",
       " 'hollywoods': 543,\n",
       " 'writer': 544,\n",
       " 'juvenile': 545,\n",
       " '1950s': 546,\n",
       " 'wait': 547,\n",
       " 'believable': 548,\n",
       " 'hammerhead': 549,\n",
       " 'lights': 550,\n",
       " 'dilemma': 551,\n",
       " 'racially': 552,\n",
       " 'fallen': 553,\n",
       " 'joy': 554,\n",
       " 'buying': 555,\n",
       " 'sandler': 556,\n",
       " 'ha!': 557,\n",
       " 'victim': 558,\n",
       " 'first-person': 559,\n",
       " 'believe': 560,\n",
       " 'thoughts:': 561,\n",
       " 'sign': 562,\n",
       " 'recognisable': 563,\n",
       " 'start': 564,\n",
       " 'bueller': 565,\n",
       " 'sarcastic': 566,\n",
       " 'lovable': 567,\n",
       " 'mj': 568,\n",
       " 'lenser': 569,\n",
       " 'richard': 570,\n",
       " 'vote': 571,\n",
       " 'davis': 572,\n",
       " 'verite': 573,\n",
       " 'rediscoveries:': 574,\n",
       " 'viewings': 575,\n",
       " 'jeroen': 576,\n",
       " 'handle': 577,\n",
       " 'fallout': 578,\n",
       " 'person': 579,\n",
       " 'formally': 580,\n",
       " 'pliers': 581,\n",
       " 'innocence': 582,\n",
       " 'paying': 583,\n",
       " 'poorness': 584,\n",
       " 'blows': 585,\n",
       " 'robot': 586,\n",
       " 'cartoon': 587,\n",
       " 'ers': 588,\n",
       " 'surfing': 589,\n",
       " 'urbania': 590,\n",
       " 'baked': 591,\n",
       " '!!!!': 592,\n",
       " 'subjects': 593,\n",
       " 'nature': 594,\n",
       " 'brilliant': 595,\n",
       " 'didactic': 596,\n",
       " 'eighttitle': 597,\n",
       " 'later': 598,\n",
       " 'emails': 599,\n",
       " 'buy': 600,\n",
       " 'sounding': 601,\n",
       " 'meatloaf': 602,\n",
       " 'farcically': 603,\n",
       " 'cake': 604,\n",
       " 'memorial': 605,\n",
       " 'cb': 606,\n",
       " 'involves': 607,\n",
       " 'crypt': 608,\n",
       " 'truly': 609,\n",
       " 'dawn': 610,\n",
       " 'betsy': 611,\n",
       " 'added': 612,\n",
       " 'performer': 613,\n",
       " 'disenfranchised': 614,\n",
       " 'recall': 615,\n",
       " 'chick': 616,\n",
       " 'credulity': 617,\n",
       " 'outstanding': 618,\n",
       " 'version': 619,\n",
       " 'fingers': 620,\n",
       " 'school': 621,\n",
       " 'critical!~*~cupidgrl~*~': 622,\n",
       " 'ingenue;': 623,\n",
       " 'download': 624,\n",
       " 'think': 625,\n",
       " 'shortcomings': 626,\n",
       " 'inept': 627,\n",
       " 'atmospheric': 628,\n",
       " 'wow!': 629,\n",
       " 'realistically': 630,\n",
       " 'rod': 631,\n",
       " 'emma': 632,\n",
       " 'resist': 633,\n",
       " 'reaction': 634,\n",
       " 'poetic': 635,\n",
       " 'tangle': 636,\n",
       " 'natural': 637,\n",
       " 'moviethe': 638,\n",
       " 'axe': 639,\n",
       " 'daytime': 640,\n",
       " 'nonsensical': 641,\n",
       " 'poepoe': 642,\n",
       " 'done': 643,\n",
       " 'wordsthe': 644,\n",
       " 'extase': 645,\n",
       " 'pleasure': 646,\n",
       " 'origins': 647,\n",
       " 'dialogs': 648,\n",
       " 'engrossing': 649,\n",
       " 'childless': 650,\n",
       " 'beheading': 651,\n",
       " '`so': 652,\n",
       " 'creating': 653,\n",
       " 'falls': 654,\n",
       " 'poetical': 655,\n",
       " 'meenas': 656,\n",
       " 'rosie': 657,\n",
       " 'dated': 658,\n",
       " 'yawn': 659,\n",
       " 'scenario': 660,\n",
       " 'hate': 661,\n",
       " 'romantic-comedy': 662,\n",
       " 'watch': 663,\n",
       " 'sebastien': 664,\n",
       " 'inside': 665,\n",
       " 'dialogue': 666,\n",
       " 'wifehepburn': 667,\n",
       " 'purchased': 668,\n",
       " 'pillars': 669,\n",
       " 'requisite': 670,\n",
       " 'fit': 671,\n",
       " 'link': 672,\n",
       " 'challenges': 673,\n",
       " '2004': 674,\n",
       " 'dies': 675,\n",
       " 'monumental': 676,\n",
       " 'debt': 677,\n",
       " 'faye': 678,\n",
       " 'sessions': 679,\n",
       " 'background': 680,\n",
       " 'moss': 681,\n",
       " 'react': 682,\n",
       " 'laurie': 683,\n",
       " 'missionary': 684,\n",
       " 'kitchens': 685,\n",
       " 'lovers': 686,\n",
       " 'suggest': 687,\n",
       " 'explodes': 688,\n",
       " 'submission': 689,\n",
       " 'adventure': 690,\n",
       " 'last': 691,\n",
       " 'hyping': 692,\n",
       " 'rain': 693,\n",
       " 'clean': 694,\n",
       " 'swashbuckler': 695,\n",
       " 'cushing': 696,\n",
       " 'mambo': 697,\n",
       " 'mars': 698,\n",
       " 'adam': 699,\n",
       " 'neurotic': 700,\n",
       " '1951': 701,\n",
       " 'kramer': 702,\n",
       " 'tragedy': 703,\n",
       " 'embarrassed': 704,\n",
       " 'gliss': 705,\n",
       " 'shark-thing': 706,\n",
       " 'designed': 707,\n",
       " 'saddles': 708,\n",
       " 'wiz': 709,\n",
       " 'starts': 710,\n",
       " 'mine': 711,\n",
       " 'rasulala': 712,\n",
       " 'cover;': 713,\n",
       " 'dry': 714,\n",
       " 'world-class': 715,\n",
       " 'raped': 716,\n",
       " 'vain': 717,\n",
       " 'bouncy': 718,\n",
       " 'happening': 719,\n",
       " 'perhaps': 720,\n",
       " 'oscar-winner': 721,\n",
       " 'cruel': 722,\n",
       " 'couldve': 723,\n",
       " 'vannet': 724,\n",
       " 'kong': 725,\n",
       " 'gordon': 726,\n",
       " 'shave': 727,\n",
       " 'dreamquest': 728,\n",
       " 'migraines': 729,\n",
       " 'choose': 730,\n",
       " 'diamond': 731,\n",
       " 'co-producer': 732,\n",
       " 'crash': 733,\n",
       " 'sides': 734,\n",
       " 'bond': 735,\n",
       " 'beautiful': 736,\n",
       " 'makers': 737,\n",
       " 'jordan': 738,\n",
       " 'costner': 739,\n",
       " 'breasts': 740,\n",
       " 'witnessed': 741,\n",
       " 'nominee': 742,\n",
       " 'genre': 743,\n",
       " 'legendary': 744,\n",
       " 'establish': 745,\n",
       " 'tammy': 746,\n",
       " 'assumed': 747,\n",
       " 'accustomed': 748,\n",
       " 'artistry': 749,\n",
       " 'lines': 750,\n",
       " 'awe': 751,\n",
       " 'freaked-out': 752,\n",
       " 'joseph': 753,\n",
       " 'fatigue': 754,\n",
       " 'viewed': 755,\n",
       " 'abandoned': 756,\n",
       " 'supporting': 757,\n",
       " 'script!': 758,\n",
       " 'ensemble': 759,\n",
       " 'rescued': 760,\n",
       " 'portraying': 761,\n",
       " 'macchesney': 762,\n",
       " 'tongue-in-cheek': 763,\n",
       " 'edge': 764,\n",
       " 'grace': 765,\n",
       " 'insight': 766,\n",
       " 'males': 767,\n",
       " 'lack': 768,\n",
       " 'adamantly': 769,\n",
       " 'seeming': 770,\n",
       " 'visitors': 771,\n",
       " 'celestes': 772,\n",
       " 'dramatic': 773,\n",
       " 'persuades': 774,\n",
       " 'protracted': 775,\n",
       " 'dalmations': 776,\n",
       " 'baseslike': 777,\n",
       " 'behave': 778,\n",
       " 'lets': 779,\n",
       " 'opera': 780,\n",
       " 'piece': 781,\n",
       " 'body': 782,\n",
       " 'williamson': 783,\n",
       " 'awfully': 784,\n",
       " 'kumar': 785,\n",
       " 'drum': 786,\n",
       " 'dim': 787,\n",
       " 'ambition': 788,\n",
       " 'frolic': 789,\n",
       " 'principle': 790,\n",
       " 'mainstream': 791,\n",
       " 'babe': 792,\n",
       " 'carts': 793,\n",
       " 'cynical': 794,\n",
       " 'elliotts': 795,\n",
       " 'trousers': 796,\n",
       " 'exact': 797,\n",
       " 'hardly': 798,\n",
       " 'performance': 799,\n",
       " 'west': 800,\n",
       " 'depresses': 801,\n",
       " 'minute': 802,\n",
       " 'nothing': 803,\n",
       " 'possibly': 804,\n",
       " 'friday': 805,\n",
       " 'breath': 806,\n",
       " 'kiddy': 807,\n",
       " 'additive': 808,\n",
       " 'cw': 809,\n",
       " 'well-contained': 810,\n",
       " 'cast': 811,\n",
       " 'cedric': 812,\n",
       " 'misanthropic': 813,\n",
       " 'surpasses': 814,\n",
       " '`dramatize': 815,\n",
       " 'soundtrackthose': 816,\n",
       " 'cap': 817,\n",
       " 'lifestyle': 818,\n",
       " 'absence': 819,\n",
       " 'roadhouse': 820,\n",
       " 'written': 821,\n",
       " 'clumsy': 822,\n",
       " 'lose': 823,\n",
       " 'crisp': 824,\n",
       " 'simple': 825,\n",
       " 'possible': 826,\n",
       " 'canadianfrench': 827,\n",
       " 'spite': 828,\n",
       " 'bottom': 829,\n",
       " 'bakery': 830,\n",
       " 'admit': 831,\n",
       " 'speechless': 832,\n",
       " 'daring': 833,\n",
       " 'hammering': 834,\n",
       " 'junk': 835,\n",
       " 'andthats': 836,\n",
       " 'retreat': 837,\n",
       " 'comparisons': 838,\n",
       " 'candlelight': 839,\n",
       " 'playwright': 840,\n",
       " 'newcomers': 841,\n",
       " 'recreate': 842,\n",
       " '`star': 843,\n",
       " 'lover': 844,\n",
       " 'teacher': 845,\n",
       " 'purposes': 846,\n",
       " 'riotand': 847,\n",
       " 'despite': 848,\n",
       " 'blatant': 849,\n",
       " 'bourgeois': 850,\n",
       " 'buried': 851,\n",
       " 'bomb': 852,\n",
       " 'vengeance;': 853,\n",
       " 'masterpiece': 854,\n",
       " 'jobsons': 855,\n",
       " 'potential': 856,\n",
       " 'representation': 857,\n",
       " 'costuming': 858,\n",
       " 'class': 859,\n",
       " 'intrigued': 860,\n",
       " 'explanation': 861,\n",
       " 'butts': 862,\n",
       " 'debut': 863,\n",
       " 'eastwood': 864,\n",
       " 'stark': 865,\n",
       " 'featuring': 866,\n",
       " 'bathroom': 867,\n",
       " 'humansgerard': 868,\n",
       " '$75': 869,\n",
       " 'everett': 870,\n",
       " 'ten': 871,\n",
       " 'asian': 872,\n",
       " 'wagner': 873,\n",
       " 'judge': 874,\n",
       " 'playfully': 875,\n",
       " 'conductor': 876,\n",
       " '75': 877,\n",
       " 'uglying': 878,\n",
       " 'individual': 879,\n",
       " 'knowing': 880,\n",
       " 'bill': 881,\n",
       " 'playful': 882,\n",
       " 'jitterbug': 883,\n",
       " 'glenn': 884,\n",
       " 'endings': 885,\n",
       " 'excluding': 886,\n",
       " 'rhys': 887,\n",
       " 'north': 888,\n",
       " 'command': 889,\n",
       " '2007': 890,\n",
       " 'actually': 891,\n",
       " 'couch': 892,\n",
       " 'eitherfor': 893,\n",
       " 'stronger': 894,\n",
       " 'indie': 895,\n",
       " 'pancakes!!!': 896,\n",
       " 'tawnee': 897,\n",
       " 'disneys': 898,\n",
       " 'logic': 899,\n",
       " 'toilet': 900,\n",
       " 'disney': 901,\n",
       " 'doors': 902,\n",
       " 'coverage': 903,\n",
       " 'combines': 904,\n",
       " 'underdone': 905,\n",
       " 'genrefinal': 906,\n",
       " 'comes': 907,\n",
       " 'unkind': 908,\n",
       " 'nielsen': 909,\n",
       " 'bridges': 910,\n",
       " 'leaving': 911,\n",
       " 'laughed': 912,\n",
       " 'younger': 913,\n",
       " 'plentiful': 914,\n",
       " 'cunningham': 915,\n",
       " 'parrot': 916,\n",
       " 'awful': 917,\n",
       " 'oh': 918,\n",
       " 'fuzzy': 919,\n",
       " 'masks': 920,\n",
       " 'somewhat': 921,\n",
       " 'works': 922,\n",
       " 'experiencethe': 923,\n",
       " 'view': 924,\n",
       " 'anyones': 925,\n",
       " 'stephane': 926,\n",
       " 'extreme': 927,\n",
       " 'brains': 928,\n",
       " 'ritter': 929,\n",
       " 'men': 930,\n",
       " 'attain': 931,\n",
       " 'go!': 932,\n",
       " 'finally': 933,\n",
       " 'represented': 934,\n",
       " 'took': 935,\n",
       " 'utilized': 936,\n",
       " 'settings': 937,\n",
       " 'get': 938,\n",
       " 'shes': 939,\n",
       " 'science': 940,\n",
       " 'wrong:': 941,\n",
       " 'felecia': 942,\n",
       " 'machine': 943,\n",
       " 'whole': 944,\n",
       " 'premise': 945,\n",
       " 'charlize': 946,\n",
       " 'freely': 947,\n",
       " 'shaky': 948,\n",
       " 'scientific': 949,\n",
       " 'berry': 950,\n",
       " 'plodding': 951,\n",
       " 'lovethe': 952,\n",
       " 'criticize': 953,\n",
       " 'sgt': 954,\n",
       " 'mentioned': 955,\n",
       " 'rachel': 956,\n",
       " 'promptly': 957,\n",
       " 'realism': 958,\n",
       " 'race': 959,\n",
       " 'relying': 960,\n",
       " 'errors': 961,\n",
       " 'columbine': 962,\n",
       " 'moynahan': 963,\n",
       " 'enliven': 964,\n",
       " 'fun--possibly': 965,\n",
       " 'id': 966,\n",
       " 'wont': 967,\n",
       " 'filmmaking:': 968,\n",
       " 'fellow': 969,\n",
       " 'sxsw': 970,\n",
       " 'current': 971,\n",
       " 'relation': 972,\n",
       " 'audience': 973,\n",
       " 'half-expected': 974,\n",
       " 'fiction': 975,\n",
       " 'jay': 976,\n",
       " 'stuff': 977,\n",
       " 'burton': 978,\n",
       " 'telling': 979,\n",
       " 'superficiality': 980,\n",
       " 'image': 981,\n",
       " 'elements': 982,\n",
       " 'guilty': 983,\n",
       " 'lacey': 984,\n",
       " 'frances': 985,\n",
       " 'could': 986,\n",
       " 'celia': 987,\n",
       " 'average': 988,\n",
       " 'helps': 989,\n",
       " 'makes': 990,\n",
       " 'tourists': 991,\n",
       " 'honeymoon': 992,\n",
       " 'killed!!!': 993,\n",
       " 'efficient': 994,\n",
       " 'nonexistent': 995,\n",
       " 'sandlers': 996,\n",
       " 'gazzara': 997,\n",
       " 'browsing': 998,\n",
       " 'divorcing': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the bi-gram model\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class n_gram(nn.Module):\n",
    "    def __init__(self, vocab_size, n_dim=EMBEDDING_DIM):\n",
    "        super(n_gram, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, n_dim)\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(n_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        voc_embed = self.embed(x)\n",
    "        voc_embed2 = voc_embed.view(1, -1)\n",
    "        out = self.classify(voc_embed2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = n_gram(len(word_to_idx))\n",
    "net = net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:02<00:00, 172.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:26<00:00, 124.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 114.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:35<00:00, 112.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:34<00:00, 112.82it/s]\n",
      "  0%|                                                                              | 12/10702 [00:00<01:36, 110.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, Loss: 7.534471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:34<00:00, 112.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:37<00:00, 110.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:35<00:00, 112.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:35<00:00, 111.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:35<00:00, 112.44it/s]\n",
      "  0%|                                                                              | 12/10702 [00:00<01:30, 118.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, Loss: 6.805270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:36<00:00, 110.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 114.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 114.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:34<00:00, 112.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:34<00:00, 112.65it/s]\n",
      "  0%|▏                                                                             | 24/10702 [00:00<01:30, 118.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, Loss: 6.540151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:32<00:00, 115.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:34<00:00, 112.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:36<00:00, 111.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:41<00:00, 105.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:39<00:00, 107.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, Loss: 6.488505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for word, label in tqdm(bigram):\n",
    "        word = Variable(torch.LongTensor([word_to_idx[word]])).cuda()\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]])).cuda()\n",
    "   \n",
    "        out = net(word)\n",
    "        loss = criterion(out, label)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (e + 1) % 5 == 0:\n",
    "        print('epoch: {}, Loss: {:.6f}'.format(e + 1, train_loss / len(bigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: documentary\n",
      "label: watched\n",
      "\n",
      "\n",
      "real word is watched, predicted word is style\n"
     ]
    }
   ],
   "source": [
    "word, label = bigram[10]\n",
    "print('input: {}'.format(word))\n",
    "print('label: {}'.format(label))\n",
    "print('\\n')\n",
    "\n",
    "word = Variable(torch.LongTensor([word_to_idx[word]])).cuda()\n",
    "out = net(word)\n",
    "pred_label_idx = out.max(1)[1].item()\n",
    "predict_word = idx_to_word[pred_label_idx]\n",
    "print('real word is {}, predicted word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0423,  0.0522, -0.0436,  0.0518,  0.0422,  0.0138, -0.0937, -0.0321,\n",
       "          0.0788,  0.0339, -0.0821,  0.0825,  0.0375, -0.0528,  0.0366,  0.0319,\n",
       "         -0.0503, -0.0442, -0.0662,  0.0073, -0.0746, -0.0539,  0.0503,  0.0590,\n",
       "          0.0476, -0.0331,  0.0760,  0.0718,  0.0796,  0.0286,  0.0114,  0.0614,\n",
       "          0.0066, -0.0403, -0.0022, -0.0534,  0.0388,  0.0362,  0.0146,  0.1061,\n",
       "         -0.1104, -0.0167, -0.0653, -0.0241,  0.1121, -0.0115, -0.0447,  0.0528,\n",
       "          0.0389,  0.0418,  0.0164,  0.0319, -0.0150, -0.0541, -0.0345, -0.0407,\n",
       "         -0.0325,  0.0569, -0.0245,  0.0286, -0.0717, -0.0570, -0.0874, -0.0314]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embed(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we using the embedding layer to build the dataset we need\n",
    "\n",
    "train = []\n",
    "for s in train_remove_sw:\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w not in vocb:\n",
    "            word = Variable(torch.LongTensor([word_to_idx['unk']])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "        else:\n",
    "            word = Variable(torch.LongTensor([word_to_idx[w]])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "    train.append(np.array(tmp).sum())\n",
    "    \n",
    "train_vec = torch.cat(train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for s in test_remove_sw:\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w not in vocb:\n",
    "            word = Variable(torch.LongTensor([word_to_idx['unk']])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "        else:\n",
    "            word = Variable(torch.LongTensor([word_to_idx[w]])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "    test.append(np.array(tmp).sum())\n",
    "    \n",
    "test_vec = torch.cat(test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68        28\n",
      "           1       0.59      0.59      0.59        22\n",
      "\n",
      "    accuracy                           0.64        50\n",
      "   macro avg       0.63      0.63      0.63        50\n",
      "weighted avg       0.64      0.64      0.64        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(train_vec.data.cpu().numpy(), y_train)\n",
    "predictions=clf.predict(test_vec.data.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "\n",
    "raw_text = []\n",
    "for x in train_remove_sw:\n",
    "    raw_text.extend(x.split(' '))\n",
    "raw_text = [x for x in raw_text if x != '']\n",
    "\n",
    "vocab = set(raw_text)\n",
    "vocab.add('unk')\n",
    "vocab_size = len(vocab)\n",
    "freqs = Counter(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['stuff', 'going', 'mj', 'ive'], 'moment'), (['going', 'moment', 'ive', 'started'], 'mj'), (['moment', 'mj', 'started', 'listening'], 'ive'), (['mj', 'ive', 'listening', 'music'], 'started'), (['ive', 'started', 'music', 'watching'], 'listening')]\n"
     ]
    }
   ],
   "source": [
    "#Build data\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(len(inputs), -1)\n",
    "        out = self.linear1(embeds) #F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "batch_size = 10\n",
    "device = torch.device('cuda:0')\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embedding_dim=200,\n",
    "             context_size=CONTEXT_SIZE*2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:19<00:00, 55.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 8.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:15<00:00, 68.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 5.6650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:15<00:00, 69.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 loss 1.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████▍        | 963/1080 [00:14<00:01, 96.77it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b16b880f42b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    num = 0\n",
    "    for context, target in tqdm(data_iter):\n",
    "        context_ids = []\n",
    "        num += 1\n",
    "        for i in range(len(context[0])):\n",
    "            context_ids.append(make_context_vector([context[j][i] for j in range(len(context))], word_to_ix))\n",
    "        context_ids = torch.stack(context_ids)\n",
    "        context_ids = context_ids.to(device)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_ids)\n",
    "        label = make_context_vector(target, word_to_ix)\n",
    "        label = label.to(device)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('epoch %d loss %.4f' %(epoch, total_loss / num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faseterrrr- negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = []\n",
    "for x in train_remove_sw:\n",
    "    raw_text.extend(x.split(' '))\n",
    "raw_text = [x for x in raw_text if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab.add('unk')\n",
    "vocab_size = len(vocab)\n",
    "freqs = Counter(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['stuff', 'going', 'mj', 'ive'], 'moment'), (['going', 'moment', 'ive', 'started'], 'mj'), (['moment', 'mj', 'started', 'listening'], 'ive'), (['mj', 'ive', 'listening', 'music'], 'started'), (['ive', 'started', 'music', 'watching'], 'listening')]\n"
     ]
    }
   ],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_pow = torch.Tensor([freqs[ix_to_word[i]] for i in range(vocab_size)]).pow(0.75)\n",
    "dist = freqs_pow / freqs_pow.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(num_samples, positives=[]):\n",
    "    w = np.random.choice(len(dist), (len(positives), num_samples), p=dist.numpy())\n",
    "    if positives.is_cuda:\n",
    "        return torch.tensor(w).to(device)\n",
    "    else:\n",
    "        return torch.tensor(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-0.5 / vocab_size, 0.5 / vocab_size)\n",
    "    def forward(self, inputs, label):\n",
    "        negs = neg_sample(5, label)\n",
    "        u_embeds = self.embeddings(label)\n",
    "        v_embeds_pos = self.embeddings(inputs)\n",
    "        v_embeds_neg = self.embeddings(negs)\n",
    "        log_pos = torch.bmm(v_embeds_pos, u_embeds.unsqueeze(2)).squeeze(2)\n",
    "        log_neg = torch.bmm(v_embeds_neg, -u_embeds.unsqueeze(2)).squeeze(2)\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)\n",
    "       \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "device = torch.device('cuda:0')\n",
    "losses = []\n",
    "model = CBOW(vocab_size, embedding_dim=200)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1080 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 1/1080 [00:02<53:03,  2.95s/it]\n",
      "  1%|▊                                                                               | 11/1080 [00:03<36:51,  2.07s/it]\n",
      "  2%|█▊                                                                              | 24/1080 [00:03<25:31,  1.45s/it]\n",
      "  4%|██▉                                                                             | 39/1080 [00:03<17:39,  1.02s/it]\n",
      "  5%|████                                                                            | 55/1080 [00:03<12:11,  1.40it/s]\n",
      "  6%|█████▏                                                                          | 70/1080 [00:03<08:26,  1.99it/s]\n",
      "  8%|██████▎                                                                         | 86/1080 [00:03<05:51,  2.83it/s]\n",
      "  9%|███████▍                                                                       | 101/1080 [00:03<04:04,  4.01it/s]\n",
      " 11%|████████▌                                                                      | 117/1080 [00:03<02:50,  5.66it/s]\n",
      " 12%|█████████▋                                                                     | 132/1080 [00:03<01:59,  7.96it/s]\n",
      " 14%|██████████▊                                                                    | 147/1080 [00:03<01:24, 11.11it/s]\n",
      " 15%|███████████▊                                                                   | 162/1080 [00:04<00:59, 15.36it/s]\n",
      " 16%|████████████▉                                                                  | 177/1080 [00:04<00:43, 20.91it/s]\n",
      " 18%|██████████████                                                                 | 192/1080 [00:04<00:31, 28.13it/s]\n",
      " 19%|███████████████▏                                                               | 208/1080 [00:04<00:23, 37.27it/s]\n",
      " 21%|████████████████▎                                                              | 223/1080 [00:04<00:17, 47.93it/s]\n",
      " 22%|█████████████████▍                                                             | 239/1080 [00:04<00:13, 60.42it/s]\n",
      " 24%|██████████████████▋                                                            | 255/1080 [00:04<00:11, 73.88it/s]\n",
      " 25%|███████████████████▊                                                           | 270/1080 [00:04<00:09, 86.56it/s]\n",
      " 26%|████████████████████▊                                                          | 285/1080 [00:04<00:08, 98.37it/s]\n",
      " 28%|█████████████████████▋                                                        | 300/1080 [00:05<00:07, 108.76it/s]\n",
      " 29%|██████████████████████▊                                                       | 315/1080 [00:05<00:06, 117.44it/s]\n",
      " 31%|███████████████████████▊                                                      | 330/1080 [00:05<00:06, 124.39it/s]\n",
      " 32%|████████████████████████▉                                                     | 345/1080 [00:05<00:05, 129.77it/s]\n",
      " 33%|██████████████████████████                                                    | 360/1080 [00:05<00:06, 119.51it/s]\n",
      " 35%|███████████████████████████                                                   | 374/1080 [00:05<00:06, 111.84it/s]\n",
      " 36%|███████████████████████████▉                                                  | 387/1080 [00:05<00:06, 106.44it/s]\n",
      " 37%|████████████████████████████▊                                                 | 399/1080 [00:05<00:06, 100.26it/s]\n",
      " 38%|█████████████████████████████▉                                                 | 410/1080 [00:06<00:06, 97.54it/s]\n",
      " 39%|██████████████████████████████▊                                                | 421/1080 [00:06<00:06, 97.76it/s]\n",
      " 40%|███████████████████████████████▌                                               | 432/1080 [00:06<00:06, 95.87it/s]\n",
      " 41%|████████████████████████████████▎                                              | 442/1080 [00:06<00:06, 93.81it/s]\n",
      " 42%|█████████████████████████████████▏                                             | 453/1080 [00:06<00:06, 97.12it/s]\n",
      " 43%|█████████████████████████████████▊                                             | 463/1080 [00:06<00:06, 94.65it/s]\n",
      " 44%|██████████████████████████████████▌                                            | 473/1080 [00:06<00:06, 93.00it/s]\n",
      " 45%|███████████████████████████████████▎                                           | 483/1080 [00:06<00:06, 93.94it/s]\n",
      " 46%|████████████████████████████████████                                           | 493/1080 [00:06<00:06, 94.60it/s]\n",
      " 47%|████████████████████████████████████▊                                          | 503/1080 [00:07<00:06, 95.09it/s]\n",
      " 48%|█████████████████████████████████████▌                                         | 513/1080 [00:07<00:05, 95.43it/s]\n",
      " 48%|██████████████████████████████████████▎                                        | 523/1080 [00:07<00:05, 95.66it/s]\n",
      " 49%|██████████████████████████████████████▉                                        | 533/1080 [00:07<00:05, 95.83it/s]\n",
      " 50%|███████████████████████████████████████▋                                       | 543/1080 [00:07<00:05, 95.94it/s]\n",
      " 51%|████████████████████████████████████████▍                                      | 553/1080 [00:07<00:05, 96.02it/s]\n",
      " 52%|█████████████████████████████████████████▏                                     | 563/1080 [00:07<00:05, 95.04it/s]\n",
      " 53%|█████████████████████████████████████████▉                                     | 573/1080 [00:07<00:05, 91.20it/s]\n",
      " 54%|██████████████████████████████████████████▋                                    | 583/1080 [00:07<00:05, 91.54it/s]\n",
      " 55%|███████████████████████████████████████████▍                                   | 593/1080 [00:07<00:05, 88.93it/s]\n",
      " 56%|████████████████████████████████████████████                                   | 603/1080 [00:08<00:05, 89.05it/s]\n",
      " 57%|████████████████████████████████████████████▊                                  | 613/1080 [00:08<00:05, 91.09it/s]\n",
      " 58%|█████████████████████████████████████████████▌                                 | 623/1080 [00:08<00:04, 92.57it/s]\n",
      " 59%|██████████████████████████████████████████████▎                                | 633/1080 [00:08<00:04, 93.63it/s]\n",
      " 60%|███████████████████████████████████████████████                                | 643/1080 [00:08<00:04, 94.40it/s]\n",
      " 60%|███████████████████████████████████████████████▊                               | 653/1080 [00:08<00:04, 94.15it/s]\n",
      " 61%|████████████████████████████████████████████████▍                              | 663/1080 [00:08<00:04, 92.65it/s]\n",
      " 62%|█████████████████████████████████████████████████▏                             | 673/1080 [00:08<00:04, 93.70it/s]\n",
      " 63%|█████████████████████████████████████████████████▉                             | 683/1080 [00:08<00:04, 94.43it/s]\n",
      " 64%|██████████████████████████████████████████████████▋                            | 693/1080 [00:09<00:04, 94.77it/s]\n",
      " 65%|███████████████████████████████████████████████████▍                           | 703/1080 [00:09<00:03, 95.21it/s]\n",
      " 66%|████████████████████████████████████████████████████▏                          | 713/1080 [00:09<00:03, 95.51it/s]\n",
      " 67%|████████████████████████████████████████████████████▉                          | 723/1080 [00:09<00:03, 95.72it/s]\n",
      " 68%|█████████████████████████████████████████████████████▌                         | 733/1080 [00:09<00:03, 93.72it/s]\n",
      " 69%|██████████████████████████████████████████████████████▎                        | 743/1080 [00:09<00:03, 94.45it/s]\n",
      " 70%|███████████████████████████████████████████████████████                        | 753/1080 [00:09<00:03, 94.98it/s]\n",
      " 71%|███████████████████████████████████████████████████████▊                       | 763/1080 [00:09<00:03, 93.44it/s]\n",
      " 72%|████████████████████████████████████████████████████████▌                      | 773/1080 [00:09<00:03, 92.18it/s]\n",
      " 72%|█████████████████████████████████████████████████████████▎                     | 783/1080 [00:10<00:03, 93.35it/s]\n",
      " 73%|██████████████████████████████████████████████████████████                     | 793/1080 [00:10<00:03, 92.63it/s]\n",
      " 74%|██████████████████████████████████████████████████████████▋                    | 803/1080 [00:10<00:03, 90.66it/s]\n",
      " 75%|███████████████████████████████████████████████████████████▍                   | 813/1080 [00:10<00:02, 90.26it/s]\n",
      " 76%|████████████████████████████████████████████████████████████▏                  | 823/1080 [00:10<00:02, 91.97it/s]\n",
      " 77%|████████████████████████████████████████████████████████████▉                  | 833/1080 [00:10<00:02, 91.17it/s]\n",
      " 78%|█████████████████████████████████████████████████████████████▋                 | 843/1080 [00:10<00:02, 92.63it/s]\n",
      " 79%|██████████████████████████████████████████████████████████████▍                | 853/1080 [00:10<00:02, 92.24it/s]\n",
      " 80%|███████████████████████████████████████████████████████████████▏               | 863/1080 [00:10<00:02, 93.40it/s]\n",
      " 81%|███████████████████████████████████████████████████████████████▊               | 873/1080 [00:10<00:02, 92.14it/s]\n",
      " 82%|████████████████████████████████████████████████████████████████▌              | 883/1080 [00:11<00:02, 93.33it/s]\n",
      " 83%|█████████████████████████████████████████████████████████████████▎             | 893/1080 [00:11<00:02, 92.10it/s]\n",
      " 84%|██████████████████████████████████████████████████████████████████             | 903/1080 [00:11<00:01, 91.25it/s]\n",
      " 85%|██████████████████████████████████████████████████████████████████▊            | 913/1080 [00:11<00:01, 92.69it/s]\n",
      " 85%|███████████████████████████████████████████████████████████████████▌           | 923/1080 [00:11<00:01, 91.66it/s]\n",
      " 86%|████████████████████████████████████████████████████████████████████▏          | 933/1080 [00:11<00:01, 90.95it/s]\n",
      " 87%|████████████████████████████████████████████████████████████████████▉          | 943/1080 [00:11<00:01, 90.46it/s]\n",
      " 88%|█████████████████████████████████████████████████████████████████████▋         | 953/1080 [00:11<00:01, 90.13it/s]\n",
      " 89%|██████████████████████████████████████████████████████████████████████▍        | 963/1080 [00:11<00:01, 89.89it/s]\n",
      " 90%|███████████████████████████████████████████████████████████████████████▏       | 973/1080 [00:12<00:01, 91.70it/s]\n",
      " 91%|███████████████████████████████████████████████████████████████████████▉       | 983/1080 [00:12<00:01, 93.01it/s]\n",
      " 92%|████████████████████████████████████████████████████████████████████████▋      | 993/1080 [00:12<00:00, 93.95it/s]\n",
      " 93%|████████████████████████████████████████████████████████████████████████▍     | 1003/1080 [00:12<00:00, 94.61it/s]\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 1013/1080 [00:12<00:00, 95.09it/s]\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 1023/1080 [00:12<00:00, 95.43it/s]\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 1033/1080 [00:12<00:00, 95.66it/s]\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▎  | 1043/1080 [00:12<00:00, 95.83it/s]\n",
      " 98%|████████████████████████████████████████████████████████████████████████████  | 1053/1080 [00:12<00:00, 95.95it/s]\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▊ | 1063/1080 [00:13<00:00, 96.03it/s]\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 1073/1080 [00:13<00:00, 96.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:13<00:00, 80.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 6.2334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1080 [00:00<?, ?it/s]\n",
      "  0%|                                                                               | 1/1080 [00:04<1:18:21,  4.36s/it]\n",
      "  1%|▊                                                                               | 11/1080 [00:04<54:23,  3.05s/it]\n",
      "  2%|█▍                                                                              | 20/1080 [00:04<37:49,  2.14s/it]\n",
      "  3%|██▏                                                                             | 30/1080 [00:04<26:16,  1.50s/it]\n",
      "  4%|██▉                                                                             | 40/1080 [00:04<18:16,  1.05s/it]\n",
      "  5%|███▋                                                                            | 50/1080 [00:04<12:43,  1.35it/s]\n",
      "  6%|████▍                                                                           | 60/1080 [00:04<08:52,  1.92it/s]\n",
      "  6%|█████▏                                                                          | 70/1080 [00:05<06:12,  2.71it/s]\n",
      "  7%|█████▊                                                                          | 79/1080 [00:05<04:21,  3.83it/s]\n",
      "  8%|██████▌                                                                         | 89/1080 [00:05<03:04,  5.37it/s]\n",
      "  9%|███████▎                                                                        | 99/1080 [00:05<02:10,  7.50it/s]\n",
      " 10%|███████▉                                                                       | 109/1080 [00:05<01:33, 10.37it/s]\n",
      " 11%|████████▋                                                                      | 119/1080 [00:05<01:07, 14.15it/s]\n",
      " 12%|█████████▍                                                                     | 129/1080 [00:05<00:49, 19.02it/s]\n",
      " 13%|██████████▏                                                                    | 140/1080 [00:05<00:37, 25.23it/s]\n",
      " 14%|██████████▉                                                                    | 150/1080 [00:05<00:28, 32.40it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8d746ddd0cf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch %d loss %.4f'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    num = 0\n",
    "    for context, target in tqdm(data_iter):\n",
    "        num += 1\n",
    "        context_ids = []\n",
    "        for i in range(len(context[0])):\n",
    "            context_ids.append(make_context_vector([context[j][i] for j in range(len(context))], word_to_ix))\n",
    "        context_ids = torch.stack(context_ids)\n",
    "        context_ids = context_ids.to(device)\n",
    "        model.zero_grad()\n",
    "        label = make_context_vector(target, word_to_ix)\n",
    "        label = label.to(device)\n",
    "        loss = model(context_ids, label).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('epoch %d loss %.4f' %(epoch+1, total_loss / num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:45<00:00, 24.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for s in tqdm(train_remove_sw):\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w in vocab:\n",
    "            word = torch.LongTensor([word_to_ix[w]]).cuda()\n",
    "            emb = model.embeddings(word)\n",
    "            tmp.append(emb)\n",
    "    train.append(np.array(tmp).sum())\n",
    "    \n",
    "train_vec = torch.cat(train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:08<00:00, 29.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for s in tqdm(test_remove_sw):\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w in vocab:\n",
    "            word = torch.LongTensor([word_to_ix[w]]).cuda()\n",
    "            emb = model.embeddings(word)\n",
    "            tmp.append(emb)\n",
    "    test.append(np.array(tmp).sum())\n",
    "    \n",
    "test_vec = torch.cat(test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       978\n",
      "           1       0.72      0.69      0.70      1022\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.70      0.70      0.70      2000\n",
      "weighted avg       0.70      0.70      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(train_vec.data.cpu().numpy(), np.array(train_target))\n",
    "predictions = clf.predict(test_vec.data.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.array(test_target), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More FasterRRRRRRRRRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./IMDB_data/for_wv_ho.txt','w',encoding='utf8') as f:\n",
    "    for i in train_remove_sw:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 16:31:44,899 : INFO : collecting all words and their counts\n",
      "2020-07-13 16:31:44,915 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-07-13 16:31:44,915 : INFO : collected 4703 word types from a corpus of 10802 raw words and 100 sentences\n",
      "2020-07-13 16:31:44,923 : INFO : Loading a fresh vocabulary\n",
      "2020-07-13 16:31:44,931 : INFO : effective_min_count=1 retains 4703 unique words (100% of original 4703, drops 0)\n",
      "2020-07-13 16:31:44,931 : INFO : effective_min_count=1 leaves 10802 word corpus (100% of original 10802, drops 0)\n",
      "2020-07-13 16:31:44,955 : INFO : deleting the raw counts dictionary of 4703 items\n",
      "2020-07-13 16:31:44,963 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2020-07-13 16:31:44,963 : INFO : downsampling leaves estimated 10426 word corpus (96.5% of prior 10802)\n",
      "2020-07-13 16:31:44,987 : INFO : estimated required memory for 4703 words and 200 dimensions: 9876300 bytes\n",
      "2020-07-13 16:31:44,987 : INFO : resetting layer weights\n",
      "2020-07-13 16:31:45,075 : INFO : training model with 3 workers on 4703 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2020-07-13 16:31:45,083 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,115 : INFO : EPOCH - 1 : training on 10802 raw words (10441 effective words) took 0.0s, 325613 effective words/s\n",
      "2020-07-13 16:31:45,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,154 : INFO : EPOCH - 2 : training on 10802 raw words (10419 effective words) took 0.0s, 263951 effective words/s\n",
      "2020-07-13 16:31:45,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,202 : INFO : EPOCH - 3 : training on 10802 raw words (10428 effective words) took 0.0s, 264825 effective words/s\n",
      "2020-07-13 16:31:45,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,242 : INFO : EPOCH - 4 : training on 10802 raw words (10445 effective words) took 0.0s, 304416 effective words/s\n",
      "2020-07-13 16:31:45,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,258 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,282 : INFO : EPOCH - 5 : training on 10802 raw words (10449 effective words) took 0.0s, 271971 effective words/s\n",
      "2020-07-13 16:31:45,298 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,330 : INFO : EPOCH - 6 : training on 10802 raw words (10411 effective words) took 0.0s, 260650 effective words/s\n",
      "2020-07-13 16:31:45,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,377 : INFO : EPOCH - 7 : training on 10802 raw words (10431 effective words) took 0.0s, 266444 effective words/s\n",
      "2020-07-13 16:31:45,393 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,417 : INFO : EPOCH - 8 : training on 10802 raw words (10438 effective words) took 0.0s, 292809 effective words/s\n",
      "2020-07-13 16:31:45,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,457 : INFO : EPOCH - 9 : training on 10802 raw words (10406 effective words) took 0.0s, 281950 effective words/s\n",
      "2020-07-13 16:31:45,473 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,481 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,497 : INFO : EPOCH - 10 : training on 10802 raw words (10421 effective words) took 0.0s, 314689 effective words/s\n",
      "2020-07-13 16:31:45,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,545 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,545 : INFO : EPOCH - 11 : training on 10802 raw words (10388 effective words) took 0.0s, 265737 effective words/s\n",
      "2020-07-13 16:31:45,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,585 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,585 : INFO : EPOCH - 12 : training on 10802 raw words (10442 effective words) took 0.0s, 303428 effective words/s\n",
      "2020-07-13 16:31:45,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,625 : INFO : EPOCH - 13 : training on 10802 raw words (10434 effective words) took 0.0s, 308811 effective words/s\n",
      "2020-07-13 16:31:45,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,673 : INFO : EPOCH - 14 : training on 10802 raw words (10428 effective words) took 0.0s, 288337 effective words/s\n",
      "2020-07-13 16:31:45,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,713 : INFO : EPOCH - 15 : training on 10802 raw words (10411 effective words) took 0.0s, 292342 effective words/s\n",
      "2020-07-13 16:31:45,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,753 : INFO : EPOCH - 16 : training on 10802 raw words (10440 effective words) took 0.0s, 273973 effective words/s\n",
      "2020-07-13 16:31:45,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,801 : INFO : EPOCH - 17 : training on 10802 raw words (10427 effective words) took 0.0s, 273750 effective words/s\n",
      "2020-07-13 16:31:45,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,841 : INFO : EPOCH - 18 : training on 10802 raw words (10436 effective words) took 0.0s, 299369 effective words/s\n",
      "2020-07-13 16:31:45,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,857 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,886 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,886 : INFO : EPOCH - 19 : training on 10802 raw words (10421 effective words) took 0.0s, 265157 effective words/s\n",
      "2020-07-13 16:31:45,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,903 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,935 : INFO : EPOCH - 20 : training on 10802 raw words (10423 effective words) took 0.0s, 264203 effective words/s\n",
      "2020-07-13 16:31:45,935 : INFO : training on a 216040 raw words (208539 effective words) took 0.9s, 243350 effective words/s\n",
      "2020-07-13 16:31:45,935 : INFO : storing 4703x200 projection weights into ./models/for_ho_word2Vec.bin\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "\n",
    "sentences = word2vec.LineSentence('./IMDB_data/for_wv_ho.txt')\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, size = 200, sg = 0, iter = 20, window = 2, min_count = 1, hs = 0, negative = 5, ns_exponent = 0.75)  \n",
    "model.wv.save_word2vec_format(\"./models/for_ho_word2Vec\" + \".bin\", binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "for i in train_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in model.wv.vocab:\n",
    "            tmp.append(model.wv.get_vector(j))\n",
    "    train_set.append(np.array(tmp).sum(0).reshape(1,-1))\n",
    "    \n",
    "train = torch.cat(train_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for i in test_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in model.wv.vocab:\n",
    "            tmp.append(model.wv.get_vector(j))\n",
    "    test_set.append(np.array(tmp).sum(0).reshape(1,-1))\n",
    "    \n",
    "test = torch.cat(test_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       978\n",
      "           1       0.86      0.86      0.86      1022\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(train, np.array(train_target))\n",
    "predictions = clf.predict(test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.array(test_target), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "## Download GloVe weights:https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 12:03:25,865 : INFO : converting 400000 vectors from ./IMDB_data/glove.6b/glove.6B.100d.txt to ./IMDB_data/glove.6B.100d.word2vec.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 \n",
      " 100\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = './IMDB_data/glove.6b/glove.6B.100d.txt'\n",
    "word2vec_output_file = './IMDB_data/glove.6B.100d.word2vec.txt'\n",
    "(count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "print(count, '\\n', dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 12:03:27,866 : INFO : loading projection weights from ./IMDB_data/glove.6B.100d.word2vec.txt\n",
      "2020-07-03 12:04:08,960 : INFO : loaded (400000, 100) matrix from ./IMDB_data/glove.6B.100d.word2vec.txt\n",
      "2020-07-03 12:04:08,962 : INFO : storing 400000x100 projection weights into ./IMDB_data/word2vec.6B.100d.bin.gz\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "glove_model.save_word2vec_format('./IMDB_data/word2vec.6B.100d.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 12:04:19,458 : INFO : loading projection weights from ./IMDB_data/word2vec.6B.100d.bin.gz\n",
      "2020-07-03 12:04:32,274 : INFO : loaded (400000, 100) matrix from ./IMDB_data/word2vec.6B.100d.bin.gz\n"
     ]
    }
   ],
   "source": [
    "g_wordVec = KeyedVectors.load_word2vec_format(\"./IMDB_data/word2vec.6B.100d.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_wordVec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "g_train_set = []\n",
    "for i in train_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in g_wordVec.wv.vocab:\n",
    "            tmp.append(g_wordVec.wv.get_vector(j))\n",
    "    g_train_set.append(tmp)\n",
    "    \n",
    "g_test_set = []\n",
    "for i in test_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in g_wordVec.wv.vocab:\n",
    "            tmp.append(g_wordVec.wv.get_vector(j))\n",
    "    g_test_set.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train = None\n",
    "for i in range(len(g_train_set)):\n",
    "    if i == 0:\n",
    "        g_train = np.array(g_train_set[i]).mean(0).reshape(1,-1)\n",
    "    else:\n",
    "        g_train = np.concatenate((g_train, np.array(g_train_set[i]).mean(0).reshape(1,-1)), 0)\n",
    "        \n",
    "g_test = None\n",
    "for i in range(len(g_test_set)):\n",
    "    if i == 0:\n",
    "        g_test = np.array(g_test_set[i]).mean(0).reshape(1,-1)\n",
    "    else:\n",
    "        g_test = np.concatenate((g_test, np.array(g_test_set[i]).mean(0).reshape(1,-1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80      2566\n",
      "           1       0.78      0.79      0.79      2434\n",
      "\n",
      "    accuracy                           0.79      5000\n",
      "   macro avg       0.79      0.79      0.79      5000\n",
      "weighted avg       0.79      0.79      0.79      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(g_train, np.array(train_target))\n",
    "predictions=clf.predict(g_test)\n",
    "\n",
    "print(classification_report(np.array(test_target), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
