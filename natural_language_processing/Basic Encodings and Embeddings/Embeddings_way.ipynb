{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "## Data download:\n",
    "https://drive.google.com/file/d/1zvc4_mKBpEhFWVju91KRIM0uQc6kR-S0/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv('./IMDB_data/labeledTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data['review'].tolist()[:100]\n",
    "train_target=data['sentiment'].tolist()[:100]\n",
    "\n",
    "test_data=data['review'].tolist()[100:150]\n",
    "test_target=data['sentiment'].tolist()[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "stopWordDict = dict(zip(sw, list(range(len(sw)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 12456.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_remove_sw=[]\n",
    "for review in tqdm(train_data):\n",
    "    temp=''\n",
    "    for word in review.split():\n",
    "        if word not in stopWordDict:\n",
    "            temp+=word+' '\n",
    "    train_remove_sw.append(temp.strip())\n",
    "    \n",
    "test_remove_sw=[]\n",
    "for review in tqdm(test_data):\n",
    "    temp=''\n",
    "    for word in review.split():\n",
    "        if word not in stopWordDict:\n",
    "            temp+=word+' '\n",
    "    test_remove_sw.append(temp.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n",
      "100it [00:00, 12427.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_index={}\n",
    "\n",
    "for sample in tqdm(train_remove_sw):\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word]=len(token_index)+ 1\n",
    "\n",
    "max_length=0\n",
    "for s in train_remove_sw:\n",
    "    if len(s)>max_length:\n",
    "        max_length=len(s)\n",
    "\n",
    "train_results=np.zeros((len(train_remove_sw),max_length,max(token_index.values())+1))\n",
    "for i,smaple in tqdm(enumerate(train_remove_sw)):\n",
    "    for j,word in list(enumerate(sample.split())):\n",
    "        train_index=token_index.get(word)\n",
    "        train_results[i,j,train_index]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3938, 4704)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "test_results=np.zeros((len(test_remove_sw),max_length,max(token_index.values())+1))\n",
    "for i,smaple in tqdm(enumerate(test_remove_sw)):\n",
    "    for j,word in list(enumerate(sample.split())):\n",
    "        test_index=token_index.get(word)\n",
    "        test_results[i,j,test_index]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3938, 4704)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_results.mean(1)\n",
    "y_train=train_target\n",
    "\n",
    "X_test=test_results.mean(1)\n",
    "y_test=test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        28\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.56        50\n",
      "   macro avg       0.28      0.50      0.36        50\n",
      "weighted avg       0.31      0.56      0.40        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train=tfidf_vectorizer.fit_transform(train_remove_sw)\n",
    "X_test=tfidf_vectorizer.transform(test_remove_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4550)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.96      0.73        28\n",
      "           1       0.75      0.14      0.23        22\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.67      0.55      0.48        50\n",
      "weighted avg       0.66      0.60      0.51        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 1\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram=[]\n",
    "for sentence in train_remove_sw:\n",
    "    tmp=sentence.split()\n",
    "    for i in range(len(tmp)-1):\n",
    "        bigram.append((tmp[i],tmp[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('going', 'moment')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for s in train_remove_sw:\n",
    "    tmp=s.split()\n",
    "    for w in tmp:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocb = set(words)\n",
    "vocb.add('unk')\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crazy': 0,\n",
       " 'disneys': 1,\n",
       " 'jealousy': 2,\n",
       " 'atrocious;': 3,\n",
       " 'location': 4,\n",
       " 'merchant': 5,\n",
       " 'couldve': 6,\n",
       " 'paranoid!': 7,\n",
       " 'weekend': 8,\n",
       " 'said': 9,\n",
       " '1980': 10,\n",
       " 'veronica': 11,\n",
       " 'position': 12,\n",
       " 'berserk': 13,\n",
       " 'earhole': 14,\n",
       " 'annuls': 15,\n",
       " 'water': 16,\n",
       " 'eats': 17,\n",
       " 'showed': 18,\n",
       " 'fare': 19,\n",
       " 'starting': 20,\n",
       " 'question:': 21,\n",
       " '--': 22,\n",
       " 'sad': 23,\n",
       " 'using': 24,\n",
       " 'post-production': 25,\n",
       " 'overly': 26,\n",
       " 'sing': 27,\n",
       " 'middle-aged': 28,\n",
       " 'hated': 29,\n",
       " 'props': 30,\n",
       " 'join': 31,\n",
       " 'shake': 32,\n",
       " 'intentionally': 33,\n",
       " 'rule': 34,\n",
       " 'crisp': 35,\n",
       " 'dracula': 36,\n",
       " 'drags': 37,\n",
       " 'sun': 38,\n",
       " 'pants': 39,\n",
       " 'rudyard': 40,\n",
       " 'coat': 41,\n",
       " 'lacklustre': 42,\n",
       " 'boss': 43,\n",
       " 'horrible': 44,\n",
       " 'wait': 45,\n",
       " 'fight': 46,\n",
       " 'police': 47,\n",
       " 'theres': 48,\n",
       " 'transforms': 49,\n",
       " 'satisfying': 50,\n",
       " 'waiting': 51,\n",
       " 'splat': 52,\n",
       " 'centers': 53,\n",
       " 'low-budget': 54,\n",
       " 'skip': 55,\n",
       " 'cliff': 56,\n",
       " '1930s': 57,\n",
       " 'pace': 58,\n",
       " 'viewed': 59,\n",
       " 'historical': 60,\n",
       " 'reference': 61,\n",
       " 'decide': 62,\n",
       " 'yorker': 63,\n",
       " 'meantime': 64,\n",
       " 'principle': 65,\n",
       " 'awards': 66,\n",
       " 'human': 67,\n",
       " 'gorgeous': 68,\n",
       " 'motherking': 69,\n",
       " 'halle': 70,\n",
       " 'thor': 71,\n",
       " 'memorable': 72,\n",
       " ':': 73,\n",
       " '-it': 74,\n",
       " 'utter': 75,\n",
       " 'create': 76,\n",
       " 'seriousness': 77,\n",
       " 'rediscoveries:': 78,\n",
       " 'stare': 79,\n",
       " 'put': 80,\n",
       " 'kinky': 81,\n",
       " 'marcus': 82,\n",
       " 'itthird': 83,\n",
       " 'hayle': 84,\n",
       " 'keeping': 85,\n",
       " 'must': 86,\n",
       " 'de': 87,\n",
       " 'viewers': 88,\n",
       " 'delicious': 89,\n",
       " 'purposes': 90,\n",
       " 'delightful': 91,\n",
       " 'block': 92,\n",
       " 'ranks': 93,\n",
       " 'disappears--making': 94,\n",
       " 'laurie': 95,\n",
       " 'ivory': 96,\n",
       " 'indian': 97,\n",
       " 'nudity': 98,\n",
       " 'behavior': 99,\n",
       " 'clintons': 100,\n",
       " 'eccentric': 101,\n",
       " 'card': 102,\n",
       " 'flow': 103,\n",
       " 'bring': 104,\n",
       " 'ers': 105,\n",
       " 'reminds': 106,\n",
       " 'kornbluth': 107,\n",
       " 'basically': 108,\n",
       " 'hears': 109,\n",
       " 'others': 110,\n",
       " 'standard': 111,\n",
       " 'jeannot': 112,\n",
       " 'possibilities': 113,\n",
       " 'broken': 114,\n",
       " 'reginal': 115,\n",
       " 'videos': 116,\n",
       " 'chutzpah': 117,\n",
       " 'spalls': 118,\n",
       " 'acting': 119,\n",
       " 'deali': 120,\n",
       " 'heart-breaking': 121,\n",
       " 'singers': 122,\n",
       " 'due': 123,\n",
       " 'afterthought': 124,\n",
       " 'imperfections': 125,\n",
       " 'males': 126,\n",
       " 'concernedalthough': 127,\n",
       " 'confess': 128,\n",
       " 'curtis': 129,\n",
       " 'job': 130,\n",
       " 'newcomer': 131,\n",
       " 'wells': 132,\n",
       " 'watchingwell': 133,\n",
       " 'oscar-winner': 134,\n",
       " 'dog': 135,\n",
       " 'pride': 136,\n",
       " 'tug': 137,\n",
       " 'paul': 138,\n",
       " 'toxic': 139,\n",
       " 'fantastic': 140,\n",
       " 'mad': 141,\n",
       " 'grind': 142,\n",
       " 'match': 143,\n",
       " 'screen710': 144,\n",
       " 'ebb': 145,\n",
       " 'forward-movement': 146,\n",
       " 'kind': 147,\n",
       " 'mostly': 148,\n",
       " 'hastings': 149,\n",
       " 'segal': 150,\n",
       " 'lynn': 151,\n",
       " 'simon': 152,\n",
       " 'women': 153,\n",
       " 'grown': 154,\n",
       " 'watchthe': 155,\n",
       " 'asia': 156,\n",
       " 'insert': 157,\n",
       " 'imagine': 158,\n",
       " 'lifts': 159,\n",
       " 'harvey': 160,\n",
       " 'perceive': 161,\n",
       " 'reveals': 162,\n",
       " 'wondrously': 163,\n",
       " '1600': 164,\n",
       " 'blank!the': 165,\n",
       " 'hyping': 166,\n",
       " 'steady': 167,\n",
       " 'sxsw': 168,\n",
       " 'scott': 169,\n",
       " 'disastrous': 170,\n",
       " 'ackland': 171,\n",
       " 'dh': 172,\n",
       " 'involved': 173,\n",
       " 'masks': 174,\n",
       " 'international': 175,\n",
       " 'believing': 176,\n",
       " 'satire': 177,\n",
       " 'park¨': 178,\n",
       " 'dogs': 179,\n",
       " 'lamas': 180,\n",
       " 'report': 181,\n",
       " 'shell': 182,\n",
       " 'buellers': 183,\n",
       " 'strongest': 184,\n",
       " 'art': 185,\n",
       " 'dreck': 186,\n",
       " 'successful': 187,\n",
       " 'injected': 188,\n",
       " 'killed': 189,\n",
       " 'sure': 190,\n",
       " '*how*': 191,\n",
       " 'apart': 192,\n",
       " 'documentary': 193,\n",
       " 'designerlike': 194,\n",
       " 'scientists': 195,\n",
       " 'celebrity': 196,\n",
       " '87': 197,\n",
       " 'died': 198,\n",
       " 'shoe': 199,\n",
       " 'memorial': 200,\n",
       " 'exhibits': 201,\n",
       " 'notion': 202,\n",
       " 'dialog;': 203,\n",
       " 'coincidentally': 204,\n",
       " 'christopher': 205,\n",
       " 'junior': 206,\n",
       " 'find': 207,\n",
       " 'persona': 208,\n",
       " 'hines': 209,\n",
       " 'gore': 210,\n",
       " 'quality': 211,\n",
       " 'witty': 212,\n",
       " 'mambo': 213,\n",
       " 'waterworld': 214,\n",
       " 'script!': 215,\n",
       " 'rain': 216,\n",
       " 'andor': 217,\n",
       " 'segment': 218,\n",
       " 'stills;': 219,\n",
       " 'law': 220,\n",
       " 'wholesome': 221,\n",
       " 'titillation': 222,\n",
       " 'astounding': 223,\n",
       " 'cruel': 224,\n",
       " 'awfully': 225,\n",
       " 'mistreated': 226,\n",
       " 'anywhere': 227,\n",
       " 'vhs': 228,\n",
       " 'rid': 229,\n",
       " 'coupling': 230,\n",
       " 'started': 231,\n",
       " 'future': 232,\n",
       " 'alive': 233,\n",
       " 'temperamental': 234,\n",
       " 'wrings': 235,\n",
       " 'cushing': 236,\n",
       " 'grossly': 237,\n",
       " 'old': 238,\n",
       " 'beyond': 239,\n",
       " 'discussing': 240,\n",
       " 'tidal': 241,\n",
       " 'well-crafted': 242,\n",
       " 'roles': 243,\n",
       " 'morgan': 244,\n",
       " 'nose': 245,\n",
       " 'integration': 246,\n",
       " 'pliers': 247,\n",
       " 'smart': 248,\n",
       " 'playing': 249,\n",
       " 'unbelievable': 250,\n",
       " 'carradine': 251,\n",
       " 'figures': 252,\n",
       " 'bimbo': 253,\n",
       " 'ten': 254,\n",
       " 'talented': 255,\n",
       " 'mundane': 256,\n",
       " 'happened': 257,\n",
       " 'intended': 258,\n",
       " 'swimfan': 259,\n",
       " 'talents': 260,\n",
       " 'commentating': 261,\n",
       " 'primal': 262,\n",
       " 'witnessed': 263,\n",
       " 'blatant': 264,\n",
       " 'dont': 265,\n",
       " 'moving': 266,\n",
       " 'neuron': 267,\n",
       " 'relieve': 268,\n",
       " 'portrayals': 269,\n",
       " 'daring': 270,\n",
       " 'pranksso': 271,\n",
       " 'uniforms': 272,\n",
       " 'moriarty': 273,\n",
       " 'highlight': 274,\n",
       " 'poorness': 275,\n",
       " 'yaayyyyy': 276,\n",
       " 'embarrassment': 277,\n",
       " 'viewings': 278,\n",
       " 'moynahan': 279,\n",
       " 'deserve': 280,\n",
       " 'teenager': 281,\n",
       " 'publicitywill': 282,\n",
       " 'frustration': 283,\n",
       " 'importance': 284,\n",
       " 'full-frontal': 285,\n",
       " 'anthology': 286,\n",
       " 'romance': 287,\n",
       " 'decided': 288,\n",
       " 'misses': 289,\n",
       " 'short-sighted': 290,\n",
       " 'never': 291,\n",
       " 'gang': 292,\n",
       " '¨sabretooth2002¨by': 293,\n",
       " 'nasty': 294,\n",
       " 'voerhovens': 295,\n",
       " 'dunst': 296,\n",
       " 'happily': 297,\n",
       " 'beautiful': 298,\n",
       " 'pursuingmy': 299,\n",
       " 'speaking': 300,\n",
       " 'cary': 301,\n",
       " 'instance': 302,\n",
       " 'aplomb': 303,\n",
       " 'single': 304,\n",
       " 'circus': 305,\n",
       " 'riled': 306,\n",
       " 'lifetime': 307,\n",
       " 'umits': 308,\n",
       " 'rugged': 309,\n",
       " 'realizes': 310,\n",
       " 'presented': 311,\n",
       " 'particular': 312,\n",
       " 'hope': 313,\n",
       " 'flicker': 314,\n",
       " 'carpenter': 315,\n",
       " 'methe': 316,\n",
       " 'pasture': 317,\n",
       " 'lousy': 318,\n",
       " 'learning': 319,\n",
       " 'nauseum': 320,\n",
       " 'back': 321,\n",
       " 'tunnel': 322,\n",
       " 'met': 323,\n",
       " 'lead': 324,\n",
       " 'imitation': 325,\n",
       " 'screw': 326,\n",
       " 'angry': 327,\n",
       " '2000': 328,\n",
       " 'humansgerard': 329,\n",
       " 'rough': 330,\n",
       " 'unusual': 331,\n",
       " 'scope': 332,\n",
       " 'necks': 333,\n",
       " 'wetbacks': 334,\n",
       " 'seeit': 335,\n",
       " 'abandoned': 336,\n",
       " 'japanese': 337,\n",
       " 'playwright': 338,\n",
       " 'smith': 339,\n",
       " 'ballantine': 340,\n",
       " 'willem': 341,\n",
       " 'hugo': 342,\n",
       " 'fearsome': 343,\n",
       " '30s': 344,\n",
       " 'twinkle': 345,\n",
       " 'reasons': 346,\n",
       " 'robbing': 347,\n",
       " '10x10': 348,\n",
       " 'filmthe': 349,\n",
       " 'storyi': 350,\n",
       " 'scarcity': 351,\n",
       " 'tension': 352,\n",
       " 'behind': 353,\n",
       " 'actswere': 354,\n",
       " 'directors': 355,\n",
       " 'dream': 356,\n",
       " 'awesome': 357,\n",
       " 'sandleri': 358,\n",
       " 'superiors': 359,\n",
       " 'mary': 360,\n",
       " 'implausible': 361,\n",
       " 'advise': 362,\n",
       " 'musical': 363,\n",
       " 'supervisors': 364,\n",
       " 'speechless': 365,\n",
       " 'judge': 366,\n",
       " 'eddy': 367,\n",
       " 'suspect': 368,\n",
       " 'predict': 369,\n",
       " 'wars': 370,\n",
       " 'fire': 371,\n",
       " 'designed': 372,\n",
       " 'q': 373,\n",
       " 'hair-ball': 374,\n",
       " 'sex': 375,\n",
       " 'priest': 376,\n",
       " 'character!': 377,\n",
       " 'maine': 378,\n",
       " 'plenty': 379,\n",
       " 'ham': 380,\n",
       " 'wolf': 381,\n",
       " 'wannabe': 382,\n",
       " '`so': 383,\n",
       " 'network:': 384,\n",
       " 'fictional': 385,\n",
       " 'classes': 386,\n",
       " 'suggest': 387,\n",
       " 'rightbelow': 388,\n",
       " 'wastes': 389,\n",
       " 'downbeat': 390,\n",
       " 'behalf': 391,\n",
       " 'odyssey': 392,\n",
       " 'james': 393,\n",
       " 'fancy': 394,\n",
       " 'festival': 395,\n",
       " 'nut': 396,\n",
       " 'field': 397,\n",
       " 'thanksgiving': 398,\n",
       " 'bomber': 399,\n",
       " '5': 400,\n",
       " 'freaked': 401,\n",
       " 'tawnee': 402,\n",
       " 'bound': 403,\n",
       " 'treasured': 404,\n",
       " 'betsy': 405,\n",
       " 'colonialist': 406,\n",
       " 'befriends': 407,\n",
       " 'rounds': 408,\n",
       " 'poetic': 409,\n",
       " 'jon': 410,\n",
       " 'couples': 411,\n",
       " 'nonstop': 412,\n",
       " '1968': 413,\n",
       " 'listen': 414,\n",
       " 'yummy': 415,\n",
       " 'itching': 416,\n",
       " 'material': 417,\n",
       " 'scattering': 418,\n",
       " '810': 419,\n",
       " 'weep': 420,\n",
       " 'science': 421,\n",
       " 'chase': 422,\n",
       " 'careers': 423,\n",
       " 'miller': 424,\n",
       " 'intrusive': 425,\n",
       " 'sympathetic': 426,\n",
       " 'eye': 427,\n",
       " 'thirty': 428,\n",
       " 'beer': 429,\n",
       " 'cow': 430,\n",
       " 'smith-': 431,\n",
       " 'fall': 432,\n",
       " 'believability': 433,\n",
       " 'explains': 434,\n",
       " 'infamous': 435,\n",
       " 'revenge': 436,\n",
       " 'thorntons': 437,\n",
       " 'violent': 438,\n",
       " 'adam': 439,\n",
       " 'tolerance': 440,\n",
       " 'bukowski': 441,\n",
       " 'haiku': 442,\n",
       " 'succeed': 443,\n",
       " 'inspire': 444,\n",
       " 'portraying': 445,\n",
       " 'matched': 446,\n",
       " 'cunningham': 447,\n",
       " 'ridiculous': 448,\n",
       " 'expense': 449,\n",
       " 'line': 450,\n",
       " 'now!': 451,\n",
       " 'trashy': 452,\n",
       " 'view': 453,\n",
       " 'drained': 454,\n",
       " 'plentiful': 455,\n",
       " 'breezy': 456,\n",
       " 'teleprompter': 457,\n",
       " 'mentioned': 458,\n",
       " 'sickperhaps': 459,\n",
       " 'hills': 460,\n",
       " '70s': 461,\n",
       " 'faraway': 462,\n",
       " 'addition': 463,\n",
       " 'emperor': 464,\n",
       " 'easier': 465,\n",
       " 'attack': 466,\n",
       " 'army': 467,\n",
       " 'sleepwalks': 468,\n",
       " 'lifeacting---there': 469,\n",
       " 'must-see': 470,\n",
       " 'well-known': 471,\n",
       " 'whereas': 472,\n",
       " 'forever': 473,\n",
       " 'debut': 474,\n",
       " 'entranced': 475,\n",
       " 'adult': 476,\n",
       " '9': 477,\n",
       " 'right': 478,\n",
       " 'poisonreally': 479,\n",
       " 'appear': 480,\n",
       " 'abyss': 481,\n",
       " 'voicethe': 482,\n",
       " 'explain': 483,\n",
       " 'passing': 484,\n",
       " 'krige': 485,\n",
       " 'force': 486,\n",
       " 'accomplice': 487,\n",
       " 'murder': 488,\n",
       " 'macbeth': 489,\n",
       " 'remove': 490,\n",
       " 'sergeant': 491,\n",
       " 'loosely': 492,\n",
       " 'trash!': 493,\n",
       " 'jared': 494,\n",
       " 'apocalyptic': 495,\n",
       " 'fists': 496,\n",
       " 'marriage': 497,\n",
       " 'claim': 498,\n",
       " 'period': 499,\n",
       " 'suddenly': 500,\n",
       " 'soundtrack': 501,\n",
       " 'youit': 502,\n",
       " 'singular': 503,\n",
       " 'way:': 504,\n",
       " 'unusually': 505,\n",
       " 'towelheads': 506,\n",
       " 'things:': 507,\n",
       " 'local': 508,\n",
       " 'din': 509,\n",
       " 'reads': 510,\n",
       " 'rural': 511,\n",
       " 'settle': 512,\n",
       " 'related': 513,\n",
       " 'problem': 514,\n",
       " 'dare': 515,\n",
       " 'jr': 516,\n",
       " 'wandering': 517,\n",
       " 'reeds': 518,\n",
       " 'one': 519,\n",
       " 'jobs': 520,\n",
       " 'command': 521,\n",
       " 'closest': 522,\n",
       " 'planet': 523,\n",
       " 'first': 524,\n",
       " 'flock': 525,\n",
       " 'examples': 526,\n",
       " 'goods': 527,\n",
       " 'depresses': 528,\n",
       " 'follows': 529,\n",
       " 'bazillion': 530,\n",
       " 'vigorously': 531,\n",
       " 'wordsthe': 532,\n",
       " 'cops': 533,\n",
       " 'usually': 534,\n",
       " 'fred': 535,\n",
       " 'presence!': 536,\n",
       " 'doubts:': 537,\n",
       " 'hes': 538,\n",
       " 'arrest': 539,\n",
       " 'nero': 540,\n",
       " 'walken': 541,\n",
       " 'twist': 542,\n",
       " 'runs': 543,\n",
       " 'charlize': 544,\n",
       " 'helen': 545,\n",
       " 'expel': 546,\n",
       " 'overheard': 547,\n",
       " 'mountain': 548,\n",
       " 'sink': 549,\n",
       " 'packing': 550,\n",
       " 'elizabeth': 551,\n",
       " 'jerry': 552,\n",
       " 'particularly': 553,\n",
       " 'racial': 554,\n",
       " 'materialized': 555,\n",
       " 'disappointment': 556,\n",
       " 'batonzilla': 557,\n",
       " 'capricorn': 558,\n",
       " 'spell': 559,\n",
       " 'true': 560,\n",
       " 'teeth': 561,\n",
       " 'scottish': 562,\n",
       " 'hit-man': 563,\n",
       " 'neuroses': 564,\n",
       " 'tammy': 565,\n",
       " 'penchant': 566,\n",
       " 'smallest': 567,\n",
       " 'ultimate': 568,\n",
       " 'recently': 569,\n",
       " 'ranked': 570,\n",
       " 'gotten': 571,\n",
       " 'operatic': 572,\n",
       " 'make': 573,\n",
       " 'sequence': 574,\n",
       " 'small': 575,\n",
       " 'public': 576,\n",
       " 'lacey': 577,\n",
       " 'employer': 578,\n",
       " 'treated': 579,\n",
       " 'janine': 580,\n",
       " 'twig': 581,\n",
       " 'cat': 582,\n",
       " 'sincerity': 583,\n",
       " 'several': 584,\n",
       " 'underscored': 585,\n",
       " 'sitting': 586,\n",
       " 'letdownchristopher': 587,\n",
       " 'canvas': 588,\n",
       " 'may': 589,\n",
       " 'atmosphere': 590,\n",
       " 'referred': 591,\n",
       " 'starring': 592,\n",
       " 'devil': 593,\n",
       " 'encountered': 594,\n",
       " 'press': 595,\n",
       " 'mate': 596,\n",
       " 'mean': 597,\n",
       " 'likely': 598,\n",
       " 'business': 599,\n",
       " 'hodet': 600,\n",
       " 'club': 601,\n",
       " 'cake': 602,\n",
       " 'illogical': 603,\n",
       " 'homophobici': 604,\n",
       " 'buffs': 605,\n",
       " 'proves': 606,\n",
       " 'academy': 607,\n",
       " 'extreme': 608,\n",
       " 'everywhere:': 609,\n",
       " 'commitment-phobia': 610,\n",
       " 'martial': 611,\n",
       " 'expert': 612,\n",
       " 'track': 613,\n",
       " 'styled': 614,\n",
       " 'trivia': 615,\n",
       " 'blazing': 616,\n",
       " 'choo-chooceleste': 617,\n",
       " 'todd': 618,\n",
       " 'stereo': 619,\n",
       " 'it!': 620,\n",
       " 'understanding': 621,\n",
       " 'expected': 622,\n",
       " '3000': 623,\n",
       " 'move': 624,\n",
       " 'key': 625,\n",
       " 'handful': 626,\n",
       " '`dramatize': 627,\n",
       " 'itcons:': 628,\n",
       " 'hoffman': 629,\n",
       " 'anthony': 630,\n",
       " 'popular': 631,\n",
       " 'spliced': 632,\n",
       " 'raunchy': 633,\n",
       " 'slightest': 634,\n",
       " 'competitive': 635,\n",
       " 'grunts': 636,\n",
       " 'classic': 637,\n",
       " 'ha!': 638,\n",
       " 'bestest': 639,\n",
       " 'suggestion': 640,\n",
       " 'explodes': 641,\n",
       " 'tycoon': 642,\n",
       " 'distraction': 643,\n",
       " 'however': 644,\n",
       " 'ingenue;': 645,\n",
       " 'somethings': 646,\n",
       " 'river': 647,\n",
       " 'promised': 648,\n",
       " 'performs': 649,\n",
       " 'new': 650,\n",
       " 'pits': 651,\n",
       " 'looked': 652,\n",
       " 'elliott': 653,\n",
       " 'deaths': 654,\n",
       " 'killer': 655,\n",
       " 'plane': 656,\n",
       " 'faithfully': 657,\n",
       " 'rockstar': 658,\n",
       " 'adds': 659,\n",
       " 'ministerfollowed': 660,\n",
       " 'portray': 661,\n",
       " 'claus': 662,\n",
       " 'salisbury': 663,\n",
       " 'overwhelmed': 664,\n",
       " 'palette': 665,\n",
       " 'fortune': 666,\n",
       " 'actuality': 667,\n",
       " 'regrets': 668,\n",
       " 'via': 669,\n",
       " 'overuse': 670,\n",
       " 'effect': 671,\n",
       " 'usual': 672,\n",
       " 'feat': 673,\n",
       " 'annoyed': 674,\n",
       " 'culkins': 675,\n",
       " 'perv': 676,\n",
       " 'wagerfoster': 677,\n",
       " 'riggs': 678,\n",
       " 'depressed': 679,\n",
       " 'demeanor': 680,\n",
       " 'bogdanovich': 681,\n",
       " 'intellectual': 682,\n",
       " 'serves': 683,\n",
       " 'enthusiasm': 684,\n",
       " 'faithful': 685,\n",
       " 'absence': 686,\n",
       " 'twenty': 687,\n",
       " 'couple': 688,\n",
       " 'absurd': 689,\n",
       " 'turkey': 690,\n",
       " 'baseslike': 691,\n",
       " 'visit': 692,\n",
       " 'charles': 693,\n",
       " 'saying': 694,\n",
       " 'please': 695,\n",
       " 'fatal': 696,\n",
       " 'parental': 697,\n",
       " 'southwest': 698,\n",
       " 'pat': 699,\n",
       " 'gale': 700,\n",
       " 'douglas': 701,\n",
       " 'released': 702,\n",
       " 'false': 703,\n",
       " 'wing': 704,\n",
       " 'advertised': 705,\n",
       " 'tour': 706,\n",
       " 'pascow': 707,\n",
       " 'ship': 708,\n",
       " 'writer': 709,\n",
       " 'hey': 710,\n",
       " 'excuse': 711,\n",
       " 'motivate': 712,\n",
       " 'glasses': 713,\n",
       " 'brilliant': 714,\n",
       " 'ed': 715,\n",
       " 'ii': 716,\n",
       " 'rapping': 717,\n",
       " 'definitive': 718,\n",
       " 'appeal': 719,\n",
       " 'paced': 720,\n",
       " 'detail': 721,\n",
       " 'scola': 722,\n",
       " 'dalmation': 723,\n",
       " 'hg': 724,\n",
       " 'eruptions': 725,\n",
       " 'steele': 726,\n",
       " 'deserves': 727,\n",
       " 'rocks': 728,\n",
       " 'french': 729,\n",
       " 'urge': 730,\n",
       " 'soap': 731,\n",
       " 'foodactors': 732,\n",
       " 'call': 733,\n",
       " 'wiz': 734,\n",
       " 'premise': 735,\n",
       " 'allusion': 736,\n",
       " 'ratings': 737,\n",
       " 'incarnate': 738,\n",
       " 'lee': 739,\n",
       " 'dangling': 740,\n",
       " 'uncanny': 741,\n",
       " 'holocaust': 742,\n",
       " 'outpost': 743,\n",
       " 'vogue': 744,\n",
       " 'cant': 745,\n",
       " 'fatigue': 746,\n",
       " 'consideration': 747,\n",
       " 'micmac': 748,\n",
       " 'eminent': 749,\n",
       " 'eliza': 750,\n",
       " 'alter-ego': 751,\n",
       " 'annoying': 752,\n",
       " 'cameo': 753,\n",
       " 'logic': 754,\n",
       " 'richard': 755,\n",
       " 'source': 756,\n",
       " 'carter': 757,\n",
       " 'heroes': 758,\n",
       " '`reloaded': 759,\n",
       " 'shreds!!!!!': 760,\n",
       " 'reloaded': 761,\n",
       " 'beginning': 762,\n",
       " 'pesci': 763,\n",
       " 'biting': 764,\n",
       " 'individually': 765,\n",
       " 'impatience': 766,\n",
       " 'journey': 767,\n",
       " 'chiller': 768,\n",
       " 'spoilers': 769,\n",
       " 'deservedly': 770,\n",
       " 'thumbs': 771,\n",
       " 'insults': 772,\n",
       " 'actually': 773,\n",
       " 'critics': 774,\n",
       " 'audrey': 775,\n",
       " 'clampett': 776,\n",
       " 'macabre': 777,\n",
       " 'bottom': 778,\n",
       " 'dire': 779,\n",
       " 'presentation': 780,\n",
       " 'werching': 781,\n",
       " 'catchy!': 782,\n",
       " 'definitely': 783,\n",
       " 'trainspotting': 784,\n",
       " 'bonkers': 785,\n",
       " 'schlock': 786,\n",
       " 'deadly': 787,\n",
       " 'soldiers': 788,\n",
       " 'soul': 789,\n",
       " 'resolution': 790,\n",
       " 'case': 791,\n",
       " 'strait': 792,\n",
       " 'transitions': 793,\n",
       " 'establish': 794,\n",
       " 'kumari': 795,\n",
       " 'asked': 796,\n",
       " 'believes': 797,\n",
       " 'simple': 798,\n",
       " 'fourth': 799,\n",
       " 'pillars': 800,\n",
       " '40-or-so': 801,\n",
       " 'profound': 802,\n",
       " 'now-a-days': 803,\n",
       " 'dna': 804,\n",
       " 'danny': 805,\n",
       " 'depicted': 806,\n",
       " 'midkiffs': 807,\n",
       " 'play': 808,\n",
       " 'childless': 809,\n",
       " 'spoiler*the': 810,\n",
       " 'funniest': 811,\n",
       " 'lesbian': 812,\n",
       " 'chorus': 813,\n",
       " 'bloody': 814,\n",
       " 'extremely-large': 815,\n",
       " 'farcical': 816,\n",
       " 'sabretooths': 817,\n",
       " 'guns': 818,\n",
       " 'hype': 819,\n",
       " 'joe': 820,\n",
       " 'certainty': 821,\n",
       " 'viewer': 822,\n",
       " 'filthy': 823,\n",
       " 'writers': 824,\n",
       " 'logical': 825,\n",
       " 'baker': 826,\n",
       " 'pressures': 827,\n",
       " 'qualifies': 828,\n",
       " 'fair': 829,\n",
       " 'hedy': 830,\n",
       " 'episode': 831,\n",
       " 'sub-genre': 832,\n",
       " 'old-fashioned': 833,\n",
       " 'birds': 834,\n",
       " 'comedy': 835,\n",
       " 'filled': 836,\n",
       " 'punished': 837,\n",
       " 'orphaned': 838,\n",
       " 'giant': 839,\n",
       " '1974': 840,\n",
       " 'talbert': 841,\n",
       " 'white': 842,\n",
       " 'mj': 843,\n",
       " 'spoil': 844,\n",
       " 'what!!!!': 845,\n",
       " '£1': 846,\n",
       " 'important': 847,\n",
       " 'suggests': 848,\n",
       " 'production': 849,\n",
       " 'felt': 850,\n",
       " 'b': 851,\n",
       " 'inspiring': 852,\n",
       " 'corehowever': 853,\n",
       " 'sorry': 854,\n",
       " 'replaced': 855,\n",
       " 'timothy': 856,\n",
       " 'filters': 857,\n",
       " 'heterosexual': 858,\n",
       " 'slowly': 859,\n",
       " 'kingsley': 860,\n",
       " 'frenzy': 861,\n",
       " 'offensive': 862,\n",
       " 'altogether': 863,\n",
       " 'boogey': 864,\n",
       " 'candle': 865,\n",
       " 'spirit--and': 866,\n",
       " 'whole': 867,\n",
       " 'boot;': 868,\n",
       " 'battles': 869,\n",
       " 'complex': 870,\n",
       " 'students': 871,\n",
       " 'involves': 872,\n",
       " 'comparisons': 873,\n",
       " 'recreate': 874,\n",
       " 'mustache': 875,\n",
       " 'software': 876,\n",
       " 'puke': 877,\n",
       " 'likable': 878,\n",
       " 'dafoe': 879,\n",
       " 'restricted': 880,\n",
       " 'take': 881,\n",
       " 'depressing': 882,\n",
       " 'mutated': 883,\n",
       " 'elisabeth': 884,\n",
       " 'poor': 885,\n",
       " 'blockbuster': 886,\n",
       " 'blew': 887,\n",
       " 'four': 888,\n",
       " 'spectacular': 889,\n",
       " 'car!!!': 890,\n",
       " 'somewhere': 891,\n",
       " 'urban': 892,\n",
       " 'nikah': 893,\n",
       " 'remakes': 894,\n",
       " 'daunting': 895,\n",
       " 'married': 896,\n",
       " 'edward': 897,\n",
       " 'lynns': 898,\n",
       " 'epic': 899,\n",
       " 'piece': 900,\n",
       " 'mona': 901,\n",
       " 'uniquely': 902,\n",
       " 'sufferthe': 903,\n",
       " 'concept': 904,\n",
       " 'b&w': 905,\n",
       " 'bc2006¨': 906,\n",
       " 'store': 907,\n",
       " 'abc': 908,\n",
       " 'lurid': 909,\n",
       " 'part!': 910,\n",
       " 'single-minded': 911,\n",
       " 'enjoyed': 912,\n",
       " 'sinister': 913,\n",
       " '-well': 914,\n",
       " 'keith': 915,\n",
       " 'synthetic': 916,\n",
       " 'aged': 917,\n",
       " 'little': 918,\n",
       " 'cemetery': 919,\n",
       " 'uses': 920,\n",
       " 'fun': 921,\n",
       " 'far': 922,\n",
       " 'stebbins': 923,\n",
       " 'antonio': 924,\n",
       " 'editing': 925,\n",
       " 'otherworldly': 926,\n",
       " 'thoughts:': 927,\n",
       " 'equally': 928,\n",
       " 'warning:': 929,\n",
       " 'nerves': 930,\n",
       " 'cecil': 931,\n",
       " 'seesure': 932,\n",
       " 'south': 933,\n",
       " 'day': 934,\n",
       " 'remembered': 935,\n",
       " 'thalmus': 936,\n",
       " 'bbc': 937,\n",
       " 'attain': 938,\n",
       " 'vein': 939,\n",
       " 'roll': 940,\n",
       " 'cvs': 941,\n",
       " 'creepiness': 942,\n",
       " 'rube': 943,\n",
       " 'stops': 944,\n",
       " 'overboard': 945,\n",
       " 'importantly': 946,\n",
       " 'lacks': 947,\n",
       " 'across': 948,\n",
       " 'genetically': 949,\n",
       " 'supporter': 950,\n",
       " 'worst': 951,\n",
       " 'sympathy': 952,\n",
       " 'absolutely': 953,\n",
       " 'cover;': 954,\n",
       " 'liked': 955,\n",
       " 'car': 956,\n",
       " '`zion': 957,\n",
       " 'thought-provoking': 958,\n",
       " 'looks': 959,\n",
       " 'citethe': 960,\n",
       " 'loses': 961,\n",
       " 'heads': 962,\n",
       " 'plenty!': 963,\n",
       " 'blind': 964,\n",
       " 'proving': 965,\n",
       " 'carried': 966,\n",
       " 'creature': 967,\n",
       " '$20': 968,\n",
       " 'immediately': 969,\n",
       " 'ask': 970,\n",
       " 'routines': 971,\n",
       " 'heroine': 972,\n",
       " 'three': 973,\n",
       " 'ideas': 974,\n",
       " 'linguistic': 975,\n",
       " 'jamie': 976,\n",
       " 'plunge': 977,\n",
       " 'link': 978,\n",
       " 'stark': 979,\n",
       " 'minded': 980,\n",
       " 'horrors': 981,\n",
       " 'limits': 982,\n",
       " 'flickill': 983,\n",
       " 'entertainingthe': 984,\n",
       " 'dale': 985,\n",
       " 'david': 986,\n",
       " 'guess': 987,\n",
       " 'lack': 988,\n",
       " 'male': 989,\n",
       " 'cutter': 990,\n",
       " 'gains': 991,\n",
       " 'dozen': 992,\n",
       " 'revoltingi': 993,\n",
       " 'text': 994,\n",
       " 'asians': 995,\n",
       " 'direction': 996,\n",
       " 'gwynne': 997,\n",
       " 'tops': 998,\n",
       " 'later': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class n_gram(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size=CONTEXT_SIZE, n_dim=EMBEDDING_DIM):\n",
    "        super(n_gram, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, n_dim)\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(context_size * n_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        voc_embed = self.embed(x)\n",
    "        voc_embed2 = voc_embed.view(1, -1)\n",
    "        out = self.classify(voc_embed2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = n_gram(len(word_to_idx))\n",
    "net=net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:13<00:00, 144.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:36<00:00, 103.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 10702/10702 [01:50<00:00, 97.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:29<00:00, 119.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:30<00:00, 113.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, Loss: 7.523582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:28<00:00, 120.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:30<00:00, 118.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 114.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 114.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:36<00:00, 111.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, Loss: 6.790649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:32<00:00, 115.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 115.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:31<00:00, 116.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:33<00:00, 113.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:32<00:00, 116.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, Loss: 6.561274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:30<00:00, 117.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:31<00:00, 117.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:32<00:00, 116.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:31<00:00, 116.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10702/10702 [01:31<00:00, 117.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, Loss: 6.521470\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for word, label in tqdm(bigram):\n",
    "        word = Variable(torch.LongTensor([word_to_idx[word]])).cuda()\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]])).cuda()\n",
    "   \n",
    "        out = net(word)\n",
    "        loss = criterion(out, label)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (e + 1) % 5 == 0:\n",
    "        print('epoch: {}, Loss: {:.6f}'.format(e + 1, train_loss / len(bigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: documentary\n",
      "label: watched\n",
      "\n",
      "real word is watched, predicted word is style\n"
     ]
    }
   ],
   "source": [
    "word, label = bigram[10]\n",
    "print('input: {}'.format(word))\n",
    "print('label: {}'.format(label))\n",
    "print()\n",
    "word = Variable(torch.LongTensor([word_to_idx[word]])).cuda()\n",
    "out = net(word)\n",
    "pred_label_idx = out.max(1)[1].item()\n",
    "predict_word = idx_to_word[pred_label_idx]\n",
    "print('real word is {}, predicted word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0526, -0.0787,  0.0455, -0.0671,  0.0538,  0.0146, -0.0098,  0.0116,\n",
       "          0.0437,  0.0463, -0.0784, -0.0542, -0.0388, -0.0337, -0.0103,  0.0394,\n",
       "         -0.0650, -0.0013,  0.0034, -0.0095, -0.0908,  0.0336, -0.0818, -0.0900,\n",
       "         -0.0786,  0.0189,  0.0776,  0.0805,  0.0629,  0.0735,  0.0325, -0.0300,\n",
       "         -0.0589, -0.0146,  0.0438, -0.0028,  0.0426,  0.0182,  0.0225,  0.0079,\n",
       "         -0.0603, -0.0398,  0.0456,  0.0440, -0.0154, -0.0675, -0.0255, -0.0791,\n",
       "         -0.0368, -0.0046, -0.0411, -0.0281,  0.0576,  0.0551,  0.1262, -0.0151,\n",
       "          0.0035, -0.0096, -0.0756, -0.0364,  0.0558,  0.0564,  0.0625,  0.0292]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embed(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for s in train_remove_sw:\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w not in vocb:\n",
    "            word = Variable(torch.LongTensor([word_to_idx['unk']])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "        else:\n",
    "            word = Variable(torch.LongTensor([word_to_idx[w]])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "    train.append(np.array(tmp).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = None\n",
    "for i in range(len(train)):\n",
    "    if i == 0:\n",
    "        train_vec = train[i]\n",
    "    else:\n",
    "        train_vec = torch.cat((train_vec, train[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for s in test_remove_sw:\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w not in vocb:\n",
    "            word = Variable(torch.LongTensor([word_to_idx['unk']])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "        else:\n",
    "            word = Variable(torch.LongTensor([word_to_idx[w]])).cuda()\n",
    "            emb = net.embed(word)\n",
    "            tmp.append(emb)\n",
    "    test.append(np.array(tmp).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = None\n",
    "for i in range(len(test)):\n",
    "    if i == 0:\n",
    "        test_vec = test[i]\n",
    "    else:\n",
    "        test_vec = torch.cat((test_vec, test[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.46      0.58        28\n",
      "           1       0.55      0.82      0.65        22\n",
      "\n",
      "    accuracy                           0.62        50\n",
      "   macro avg       0.66      0.64      0.62        50\n",
      "weighted avg       0.67      0.62      0.61        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(train_vec.data.cpu().numpy(), y_train)\n",
    "predictions=clf.predict(test_vec.data.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = []\n",
    "for x in train_remove_sw:\n",
    "    raw_text.extend(x.split(' '))\n",
    "raw_text = [x for x in raw_text if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab.add('unk')\n",
    "vocab_size = len(vocab)\n",
    "freqs = Counter(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['stuff', 'going', 'mj', 'ive'], 'moment'), (['going', 'moment', 'ive', 'started'], 'mj'), (['moment', 'mj', 'started', 'listening'], 'ive'), (['mj', 'ive', 'listening', 'music'], 'started'), (['ive', 'started', 'music', 'watching'], 'listening')]\n"
     ]
    }
   ],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(len(inputs), -1)\n",
    "        out = self.linear1(embeds) #F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "batch_size = 10\n",
    "device = torch.device('cuda:0')\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embedding_dim=200,\n",
    "             context_size=CONTEXT_SIZE*2)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:19<00:00, 55.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 8.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:15<00:00, 68.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 5.6650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:15<00:00, 69.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 loss 1.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████▍        | 963/1080 [00:14<00:01, 96.77it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b16b880f42b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    num = 0\n",
    "    for context, target in tqdm(data_iter):\n",
    "        context_ids = []\n",
    "        num += 1\n",
    "        for i in range(len(context[0])):\n",
    "            context_ids.append(make_context_vector([context[j][i] for j in range(len(context))], word_to_ix))\n",
    "        context_ids = torch.stack(context_ids)\n",
    "        context_ids = context_ids.to(device)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_ids)\n",
    "        label = make_context_vector(target, word_to_ix)\n",
    "        label = label.to(device)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('epoch %d loss %.4f' %(epoch, total_loss / num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faseterrrr- negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = []\n",
    "for x in train_remove_sw:\n",
    "    raw_text.extend(x.split(' '))\n",
    "raw_text = [x for x in raw_text if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab.add('unk')\n",
    "vocab_size = len(vocab)\n",
    "freqs = Counter(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['stuff', 'going', 'mj', 'ive'], 'moment'), (['going', 'moment', 'ive', 'started'], 'mj'), (['moment', 'mj', 'started', 'listening'], 'ive'), (['mj', 'ive', 'listening', 'music'], 'started'), (['ive', 'started', 'music', 'watching'], 'listening')]\n"
     ]
    }
   ],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_pow = torch.Tensor([freqs[ix_to_word[i]] for i in range(vocab_size)]).pow(0.75)\n",
    "dist = freqs_pow / freqs_pow.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(num_samples, positives=[]):\n",
    "    w = np.random.choice(len(dist), (len(positives), num_samples), p=dist.numpy())\n",
    "    if positives.is_cuda:\n",
    "        return torch.tensor(w).to(device)\n",
    "    else:\n",
    "        return torch.tensor(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-0.5 / vocab_size, 0.5 / vocab_size)\n",
    "    def forward(self, inputs, label):\n",
    "        negs = neg_sample(5, label)\n",
    "        u_embeds = self.embeddings(label)\n",
    "        v_embeds_pos = self.embeddings(inputs)\n",
    "        v_embeds_neg = self.embeddings(negs)\n",
    "        log_pos = torch.bmm(v_embeds_pos, u_embeds.unsqueeze(2)).squeeze(2)\n",
    "        log_neg = torch.bmm(v_embeds_neg, -u_embeds.unsqueeze(2)).squeeze(2)\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)\n",
    "       \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "device = torch.device('cuda:0')\n",
    "losses = []\n",
    "model = CBOW(vocab_size, embedding_dim=200)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1080 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 1/1080 [00:02<53:03,  2.95s/it]\n",
      "  1%|▊                                                                               | 11/1080 [00:03<36:51,  2.07s/it]\n",
      "  2%|█▊                                                                              | 24/1080 [00:03<25:31,  1.45s/it]\n",
      "  4%|██▉                                                                             | 39/1080 [00:03<17:39,  1.02s/it]\n",
      "  5%|████                                                                            | 55/1080 [00:03<12:11,  1.40it/s]\n",
      "  6%|█████▏                                                                          | 70/1080 [00:03<08:26,  1.99it/s]\n",
      "  8%|██████▎                                                                         | 86/1080 [00:03<05:51,  2.83it/s]\n",
      "  9%|███████▍                                                                       | 101/1080 [00:03<04:04,  4.01it/s]\n",
      " 11%|████████▌                                                                      | 117/1080 [00:03<02:50,  5.66it/s]\n",
      " 12%|█████████▋                                                                     | 132/1080 [00:03<01:59,  7.96it/s]\n",
      " 14%|██████████▊                                                                    | 147/1080 [00:03<01:24, 11.11it/s]\n",
      " 15%|███████████▊                                                                   | 162/1080 [00:04<00:59, 15.36it/s]\n",
      " 16%|████████████▉                                                                  | 177/1080 [00:04<00:43, 20.91it/s]\n",
      " 18%|██████████████                                                                 | 192/1080 [00:04<00:31, 28.13it/s]\n",
      " 19%|███████████████▏                                                               | 208/1080 [00:04<00:23, 37.27it/s]\n",
      " 21%|████████████████▎                                                              | 223/1080 [00:04<00:17, 47.93it/s]\n",
      " 22%|█████████████████▍                                                             | 239/1080 [00:04<00:13, 60.42it/s]\n",
      " 24%|██████████████████▋                                                            | 255/1080 [00:04<00:11, 73.88it/s]\n",
      " 25%|███████████████████▊                                                           | 270/1080 [00:04<00:09, 86.56it/s]\n",
      " 26%|████████████████████▊                                                          | 285/1080 [00:04<00:08, 98.37it/s]\n",
      " 28%|█████████████████████▋                                                        | 300/1080 [00:05<00:07, 108.76it/s]\n",
      " 29%|██████████████████████▊                                                       | 315/1080 [00:05<00:06, 117.44it/s]\n",
      " 31%|███████████████████████▊                                                      | 330/1080 [00:05<00:06, 124.39it/s]\n",
      " 32%|████████████████████████▉                                                     | 345/1080 [00:05<00:05, 129.77it/s]\n",
      " 33%|██████████████████████████                                                    | 360/1080 [00:05<00:06, 119.51it/s]\n",
      " 35%|███████████████████████████                                                   | 374/1080 [00:05<00:06, 111.84it/s]\n",
      " 36%|███████████████████████████▉                                                  | 387/1080 [00:05<00:06, 106.44it/s]\n",
      " 37%|████████████████████████████▊                                                 | 399/1080 [00:05<00:06, 100.26it/s]\n",
      " 38%|█████████████████████████████▉                                                 | 410/1080 [00:06<00:06, 97.54it/s]\n",
      " 39%|██████████████████████████████▊                                                | 421/1080 [00:06<00:06, 97.76it/s]\n",
      " 40%|███████████████████████████████▌                                               | 432/1080 [00:06<00:06, 95.87it/s]\n",
      " 41%|████████████████████████████████▎                                              | 442/1080 [00:06<00:06, 93.81it/s]\n",
      " 42%|█████████████████████████████████▏                                             | 453/1080 [00:06<00:06, 97.12it/s]\n",
      " 43%|█████████████████████████████████▊                                             | 463/1080 [00:06<00:06, 94.65it/s]\n",
      " 44%|██████████████████████████████████▌                                            | 473/1080 [00:06<00:06, 93.00it/s]\n",
      " 45%|███████████████████████████████████▎                                           | 483/1080 [00:06<00:06, 93.94it/s]\n",
      " 46%|████████████████████████████████████                                           | 493/1080 [00:06<00:06, 94.60it/s]\n",
      " 47%|████████████████████████████████████▊                                          | 503/1080 [00:07<00:06, 95.09it/s]\n",
      " 48%|█████████████████████████████████████▌                                         | 513/1080 [00:07<00:05, 95.43it/s]\n",
      " 48%|██████████████████████████████████████▎                                        | 523/1080 [00:07<00:05, 95.66it/s]\n",
      " 49%|██████████████████████████████████████▉                                        | 533/1080 [00:07<00:05, 95.83it/s]\n",
      " 50%|███████████████████████████████████████▋                                       | 543/1080 [00:07<00:05, 95.94it/s]\n",
      " 51%|████████████████████████████████████████▍                                      | 553/1080 [00:07<00:05, 96.02it/s]\n",
      " 52%|█████████████████████████████████████████▏                                     | 563/1080 [00:07<00:05, 95.04it/s]\n",
      " 53%|█████████████████████████████████████████▉                                     | 573/1080 [00:07<00:05, 91.20it/s]\n",
      " 54%|██████████████████████████████████████████▋                                    | 583/1080 [00:07<00:05, 91.54it/s]\n",
      " 55%|███████████████████████████████████████████▍                                   | 593/1080 [00:07<00:05, 88.93it/s]\n",
      " 56%|████████████████████████████████████████████                                   | 603/1080 [00:08<00:05, 89.05it/s]\n",
      " 57%|████████████████████████████████████████████▊                                  | 613/1080 [00:08<00:05, 91.09it/s]\n",
      " 58%|█████████████████████████████████████████████▌                                 | 623/1080 [00:08<00:04, 92.57it/s]\n",
      " 59%|██████████████████████████████████████████████▎                                | 633/1080 [00:08<00:04, 93.63it/s]\n",
      " 60%|███████████████████████████████████████████████                                | 643/1080 [00:08<00:04, 94.40it/s]\n",
      " 60%|███████████████████████████████████████████████▊                               | 653/1080 [00:08<00:04, 94.15it/s]\n",
      " 61%|████████████████████████████████████████████████▍                              | 663/1080 [00:08<00:04, 92.65it/s]\n",
      " 62%|█████████████████████████████████████████████████▏                             | 673/1080 [00:08<00:04, 93.70it/s]\n",
      " 63%|█████████████████████████████████████████████████▉                             | 683/1080 [00:08<00:04, 94.43it/s]\n",
      " 64%|██████████████████████████████████████████████████▋                            | 693/1080 [00:09<00:04, 94.77it/s]\n",
      " 65%|███████████████████████████████████████████████████▍                           | 703/1080 [00:09<00:03, 95.21it/s]\n",
      " 66%|████████████████████████████████████████████████████▏                          | 713/1080 [00:09<00:03, 95.51it/s]\n",
      " 67%|████████████████████████████████████████████████████▉                          | 723/1080 [00:09<00:03, 95.72it/s]\n",
      " 68%|█████████████████████████████████████████████████████▌                         | 733/1080 [00:09<00:03, 93.72it/s]\n",
      " 69%|██████████████████████████████████████████████████████▎                        | 743/1080 [00:09<00:03, 94.45it/s]\n",
      " 70%|███████████████████████████████████████████████████████                        | 753/1080 [00:09<00:03, 94.98it/s]\n",
      " 71%|███████████████████████████████████████████████████████▊                       | 763/1080 [00:09<00:03, 93.44it/s]\n",
      " 72%|████████████████████████████████████████████████████████▌                      | 773/1080 [00:09<00:03, 92.18it/s]\n",
      " 72%|█████████████████████████████████████████████████████████▎                     | 783/1080 [00:10<00:03, 93.35it/s]\n",
      " 73%|██████████████████████████████████████████████████████████                     | 793/1080 [00:10<00:03, 92.63it/s]\n",
      " 74%|██████████████████████████████████████████████████████████▋                    | 803/1080 [00:10<00:03, 90.66it/s]\n",
      " 75%|███████████████████████████████████████████████████████████▍                   | 813/1080 [00:10<00:02, 90.26it/s]\n",
      " 76%|████████████████████████████████████████████████████████████▏                  | 823/1080 [00:10<00:02, 91.97it/s]\n",
      " 77%|████████████████████████████████████████████████████████████▉                  | 833/1080 [00:10<00:02, 91.17it/s]\n",
      " 78%|█████████████████████████████████████████████████████████████▋                 | 843/1080 [00:10<00:02, 92.63it/s]\n",
      " 79%|██████████████████████████████████████████████████████████████▍                | 853/1080 [00:10<00:02, 92.24it/s]\n",
      " 80%|███████████████████████████████████████████████████████████████▏               | 863/1080 [00:10<00:02, 93.40it/s]\n",
      " 81%|███████████████████████████████████████████████████████████████▊               | 873/1080 [00:10<00:02, 92.14it/s]\n",
      " 82%|████████████████████████████████████████████████████████████████▌              | 883/1080 [00:11<00:02, 93.33it/s]\n",
      " 83%|█████████████████████████████████████████████████████████████████▎             | 893/1080 [00:11<00:02, 92.10it/s]\n",
      " 84%|██████████████████████████████████████████████████████████████████             | 903/1080 [00:11<00:01, 91.25it/s]\n",
      " 85%|██████████████████████████████████████████████████████████████████▊            | 913/1080 [00:11<00:01, 92.69it/s]\n",
      " 85%|███████████████████████████████████████████████████████████████████▌           | 923/1080 [00:11<00:01, 91.66it/s]\n",
      " 86%|████████████████████████████████████████████████████████████████████▏          | 933/1080 [00:11<00:01, 90.95it/s]\n",
      " 87%|████████████████████████████████████████████████████████████████████▉          | 943/1080 [00:11<00:01, 90.46it/s]\n",
      " 88%|█████████████████████████████████████████████████████████████████████▋         | 953/1080 [00:11<00:01, 90.13it/s]\n",
      " 89%|██████████████████████████████████████████████████████████████████████▍        | 963/1080 [00:11<00:01, 89.89it/s]\n",
      " 90%|███████████████████████████████████████████████████████████████████████▏       | 973/1080 [00:12<00:01, 91.70it/s]\n",
      " 91%|███████████████████████████████████████████████████████████████████████▉       | 983/1080 [00:12<00:01, 93.01it/s]\n",
      " 92%|████████████████████████████████████████████████████████████████████████▋      | 993/1080 [00:12<00:00, 93.95it/s]\n",
      " 93%|████████████████████████████████████████████████████████████████████████▍     | 1003/1080 [00:12<00:00, 94.61it/s]\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 1013/1080 [00:12<00:00, 95.09it/s]\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 1023/1080 [00:12<00:00, 95.43it/s]\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 1033/1080 [00:12<00:00, 95.66it/s]\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▎  | 1043/1080 [00:12<00:00, 95.83it/s]\n",
      " 98%|████████████████████████████████████████████████████████████████████████████  | 1053/1080 [00:12<00:00, 95.95it/s]\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▊ | 1063/1080 [00:13<00:00, 96.03it/s]\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 1073/1080 [00:13<00:00, 96.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [00:13<00:00, 80.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 6.2334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1080 [00:00<?, ?it/s]\n",
      "  0%|                                                                               | 1/1080 [00:04<1:18:21,  4.36s/it]\n",
      "  1%|▊                                                                               | 11/1080 [00:04<54:23,  3.05s/it]\n",
      "  2%|█▍                                                                              | 20/1080 [00:04<37:49,  2.14s/it]\n",
      "  3%|██▏                                                                             | 30/1080 [00:04<26:16,  1.50s/it]\n",
      "  4%|██▉                                                                             | 40/1080 [00:04<18:16,  1.05s/it]\n",
      "  5%|███▋                                                                            | 50/1080 [00:04<12:43,  1.35it/s]\n",
      "  6%|████▍                                                                           | 60/1080 [00:04<08:52,  1.92it/s]\n",
      "  6%|█████▏                                                                          | 70/1080 [00:05<06:12,  2.71it/s]\n",
      "  7%|█████▊                                                                          | 79/1080 [00:05<04:21,  3.83it/s]\n",
      "  8%|██████▌                                                                         | 89/1080 [00:05<03:04,  5.37it/s]\n",
      "  9%|███████▎                                                                        | 99/1080 [00:05<02:10,  7.50it/s]\n",
      " 10%|███████▉                                                                       | 109/1080 [00:05<01:33, 10.37it/s]\n",
      " 11%|████████▋                                                                      | 119/1080 [00:05<01:07, 14.15it/s]\n",
      " 12%|█████████▍                                                                     | 129/1080 [00:05<00:49, 19.02it/s]\n",
      " 13%|██████████▏                                                                    | 140/1080 [00:05<00:37, 25.23it/s]\n",
      " 14%|██████████▉                                                                    | 150/1080 [00:05<00:28, 32.40it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8d746ddd0cf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch %d loss %.4f'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    num = 0\n",
    "    for context, target in tqdm(data_iter):\n",
    "        num += 1\n",
    "        context_ids = []\n",
    "        for i in range(len(context[0])):\n",
    "            context_ids.append(make_context_vector([context[j][i] for j in range(len(context))], word_to_ix))\n",
    "        context_ids = torch.stack(context_ids)\n",
    "        context_ids = context_ids.to(device)\n",
    "        model.zero_grad()\n",
    "        label = make_context_vector(target, word_to_ix)\n",
    "        label = label.to(device)\n",
    "        loss = model(context_ids, label).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('epoch %d loss %.4f' %(epoch+1, total_loss / num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:45<00:00, 24.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for s in tqdm(train_remove_sw):\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w in vocab:\n",
    "            word = torch.LongTensor([word_to_ix[w]]).cuda()\n",
    "            emb = model.embeddings(word)\n",
    "            tmp.append(emb)\n",
    "    train.append(np.array(tmp).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 3075.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vec = None\n",
    "for i in tqdm(range(len(train))):\n",
    "    if i == 0:\n",
    "        train_vec = train[i]\n",
    "    else:\n",
    "        train_vec = torch.cat((train_vec, train[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:08<00:00, 29.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for s in tqdm(test_remove_sw):\n",
    "    tmp = []\n",
    "    for w in s.split():\n",
    "        if w in vocab:\n",
    "            word = torch.LongTensor([word_to_ix[w]]).cuda()\n",
    "            emb = model.embeddings(word)\n",
    "            tmp.append(emb)\n",
    "    test.append(np.array(tmp).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4799.10it/s]\n"
     ]
    }
   ],
   "source": [
    "test_vec = None\n",
    "for i in tqdm(range(len(test))):\n",
    "    if i == 0:\n",
    "        test_vec = test[i]\n",
    "    else:\n",
    "        test_vec = torch.cat((test_vec, test[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       978\n",
      "           1       0.72      0.69      0.70      1022\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.70      0.70      0.70      2000\n",
      "weighted avg       0.70      0.70      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(train_vec.data.cpu().numpy(), np.array(train_target))\n",
    "predictions=clf.predict(test_vec.data.cpu().numpy())\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(test_target), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More FasterRRRRRRRRRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./IMDB_data/for_wv_ho.txt','w',encoding='utf8') as f:\n",
    "    for i in train_remove_sw:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 16:31:44,899 : INFO : collecting all words and their counts\n",
      "2020-07-13 16:31:44,915 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-07-13 16:31:44,915 : INFO : collected 4703 word types from a corpus of 10802 raw words and 100 sentences\n",
      "2020-07-13 16:31:44,923 : INFO : Loading a fresh vocabulary\n",
      "2020-07-13 16:31:44,931 : INFO : effective_min_count=1 retains 4703 unique words (100% of original 4703, drops 0)\n",
      "2020-07-13 16:31:44,931 : INFO : effective_min_count=1 leaves 10802 word corpus (100% of original 10802, drops 0)\n",
      "2020-07-13 16:31:44,955 : INFO : deleting the raw counts dictionary of 4703 items\n",
      "2020-07-13 16:31:44,963 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2020-07-13 16:31:44,963 : INFO : downsampling leaves estimated 10426 word corpus (96.5% of prior 10802)\n",
      "2020-07-13 16:31:44,987 : INFO : estimated required memory for 4703 words and 200 dimensions: 9876300 bytes\n",
      "2020-07-13 16:31:44,987 : INFO : resetting layer weights\n",
      "2020-07-13 16:31:45,075 : INFO : training model with 3 workers on 4703 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2020-07-13 16:31:45,083 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,115 : INFO : EPOCH - 1 : training on 10802 raw words (10441 effective words) took 0.0s, 325613 effective words/s\n",
      "2020-07-13 16:31:45,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,154 : INFO : EPOCH - 2 : training on 10802 raw words (10419 effective words) took 0.0s, 263951 effective words/s\n",
      "2020-07-13 16:31:45,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,202 : INFO : EPOCH - 3 : training on 10802 raw words (10428 effective words) took 0.0s, 264825 effective words/s\n",
      "2020-07-13 16:31:45,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,242 : INFO : EPOCH - 4 : training on 10802 raw words (10445 effective words) took 0.0s, 304416 effective words/s\n",
      "2020-07-13 16:31:45,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,258 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,282 : INFO : EPOCH - 5 : training on 10802 raw words (10449 effective words) took 0.0s, 271971 effective words/s\n",
      "2020-07-13 16:31:45,298 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,330 : INFO : EPOCH - 6 : training on 10802 raw words (10411 effective words) took 0.0s, 260650 effective words/s\n",
      "2020-07-13 16:31:45,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,377 : INFO : EPOCH - 7 : training on 10802 raw words (10431 effective words) took 0.0s, 266444 effective words/s\n",
      "2020-07-13 16:31:45,393 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,417 : INFO : EPOCH - 8 : training on 10802 raw words (10438 effective words) took 0.0s, 292809 effective words/s\n",
      "2020-07-13 16:31:45,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,457 : INFO : EPOCH - 9 : training on 10802 raw words (10406 effective words) took 0.0s, 281950 effective words/s\n",
      "2020-07-13 16:31:45,473 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,481 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,497 : INFO : EPOCH - 10 : training on 10802 raw words (10421 effective words) took 0.0s, 314689 effective words/s\n",
      "2020-07-13 16:31:45,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,545 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,545 : INFO : EPOCH - 11 : training on 10802 raw words (10388 effective words) took 0.0s, 265737 effective words/s\n",
      "2020-07-13 16:31:45,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,585 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,585 : INFO : EPOCH - 12 : training on 10802 raw words (10442 effective words) took 0.0s, 303428 effective words/s\n",
      "2020-07-13 16:31:45,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,625 : INFO : EPOCH - 13 : training on 10802 raw words (10434 effective words) took 0.0s, 308811 effective words/s\n",
      "2020-07-13 16:31:45,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,673 : INFO : EPOCH - 14 : training on 10802 raw words (10428 effective words) took 0.0s, 288337 effective words/s\n",
      "2020-07-13 16:31:45,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,713 : INFO : EPOCH - 15 : training on 10802 raw words (10411 effective words) took 0.0s, 292342 effective words/s\n",
      "2020-07-13 16:31:45,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,753 : INFO : EPOCH - 16 : training on 10802 raw words (10440 effective words) took 0.0s, 273973 effective words/s\n",
      "2020-07-13 16:31:45,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,801 : INFO : EPOCH - 17 : training on 10802 raw words (10427 effective words) took 0.0s, 273750 effective words/s\n",
      "2020-07-13 16:31:45,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,841 : INFO : EPOCH - 18 : training on 10802 raw words (10436 effective words) took 0.0s, 299369 effective words/s\n",
      "2020-07-13 16:31:45,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,857 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,886 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,886 : INFO : EPOCH - 19 : training on 10802 raw words (10421 effective words) took 0.0s, 265157 effective words/s\n",
      "2020-07-13 16:31:45,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-13 16:31:45,903 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-13 16:31:45,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-13 16:31:45,935 : INFO : EPOCH - 20 : training on 10802 raw words (10423 effective words) took 0.0s, 264203 effective words/s\n",
      "2020-07-13 16:31:45,935 : INFO : training on a 216040 raw words (208539 effective words) took 0.9s, 243350 effective words/s\n",
      "2020-07-13 16:31:45,935 : INFO : storing 4703x200 projection weights into ./models/for_ho_word2Vec.bin\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "\n",
    "sentences = word2vec.LineSentence('./IMDB_data/for_wv_ho.txt')\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, size = 200, sg = 0, iter = 20, window = 2, min_count = 1, hs = 0, negative = 5, ns_exponent = 0.75)  \n",
    "model.wv.save_word2vec_format(\"./models/for_ho_word2Vec\" + \".bin\", binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "for i in train_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in model.wv.vocab:\n",
    "            tmp.append(model.wv.get_vector(j))\n",
    "    train_set.append(np.array(tmp).sum(0).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for i in test_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in model.wv.vocab:\n",
    "            tmp.append(model.wv.get_vector(j))\n",
    "    test_set.append(np.array(tmp).sum(0).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None\n",
    "for i in range(len(train_set)):\n",
    "    if i == 0:\n",
    "        train = train_set[i]\n",
    "    else:\n",
    "        train = np.concatenate((train, train_set[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = None\n",
    "for i in range(len(test_set)):\n",
    "    if i == 0:\n",
    "        test = test_set[i]\n",
    "    else:\n",
    "        test = np.concatenate((test, test_set[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\envs\\allennlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       978\n",
      "           1       0.86      0.86      0.86      1022\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(train, np.array(train_target))\n",
    "predictions=clf.predict(test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(test_target), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "## Download GloVe weights:https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 12:03:25,865 : INFO : converting 400000 vectors from ./IMDB_data/glove.6b/glove.6B.100d.txt to ./IMDB_data/glove.6B.100d.word2vec.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 \n",
      " 100\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = './IMDB_data/glove.6b/glove.6B.100d.txt'\n",
    "word2vec_output_file = './IMDB_data/glove.6B.100d.word2vec.txt'\n",
    "(count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "print(count, '\\n', dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 12:03:27,866 : INFO : loading projection weights from ./IMDB_data/glove.6B.100d.word2vec.txt\n",
      "2020-07-03 12:04:08,960 : INFO : loaded (400000, 100) matrix from ./IMDB_data/glove.6B.100d.word2vec.txt\n",
      "2020-07-03 12:04:08,962 : INFO : storing 400000x100 projection weights into ./IMDB_data/word2vec.6B.100d.bin.gz\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "glove_model.save_word2vec_format('./IMDB_data/word2vec.6B.100d.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 12:04:19,458 : INFO : loading projection weights from ./IMDB_data/word2vec.6B.100d.bin.gz\n",
      "2020-07-03 12:04:32,274 : INFO : loaded (400000, 100) matrix from ./IMDB_data/word2vec.6B.100d.bin.gz\n"
     ]
    }
   ],
   "source": [
    "g_wordVec = KeyedVectors.load_word2vec_format(\"./IMDB_data/word2vec.6B.100d.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_wordVec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "g_train_set = []\n",
    "for i in train_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in g_wordVec.wv.vocab:\n",
    "            tmp.append(g_wordVec.wv.get_vector(j))\n",
    "    g_train_set.append(tmp)\n",
    "    \n",
    "g_test_set = []\n",
    "for i in test_remove_sw:\n",
    "    tmp = []\n",
    "    for j in i.split():\n",
    "        if j in g_wordVec.wv.vocab:\n",
    "            tmp.append(g_wordVec.wv.get_vector(j))\n",
    "    g_test_set.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train = None\n",
    "for i in range(len(g_train_set)):\n",
    "    if i == 0:\n",
    "        g_train = np.array(g_train_set[i]).mean(0).reshape(1,-1)\n",
    "    else:\n",
    "        g_train = np.concatenate((g_train, np.array(g_train_set[i]).mean(0).reshape(1,-1)), 0)\n",
    "        \n",
    "g_test = None\n",
    "for i in range(len(g_test_set)):\n",
    "    if i == 0:\n",
    "        g_test = np.array(g_test_set[i]).mean(0).reshape(1,-1)\n",
    "    else:\n",
    "        g_test = np.concatenate((g_test, np.array(g_test_set[i]).mean(0).reshape(1,-1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80      2566\n",
      "           1       0.78      0.79      0.79      2434\n",
      "\n",
      "    accuracy                           0.79      5000\n",
      "   macro avg       0.79      0.79      0.79      5000\n",
      "weighted avg       0.79      0.79      0.79      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(g_train, np.array(train_target))\n",
    "predictions=clf.predict(g_test)\n",
    "\n",
    "print(classification_report(np.array(test_target), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
